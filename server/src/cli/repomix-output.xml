This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
data-cleaner.ts
db-stats-simple.ts
db-stats.ts
queue-analyzer.ts
queue-manager.ts
README_ENHANCED.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="data-cleaner.ts">
#!/usr/bin/env npx tsx

/**
 * Data Cleaner CLI
 *
 * Consolidates data cleanup functionality:
 * - remove-all-duplicates.ts
 * - Future: filter-* and remove-* scripts
 */

import { Command } from 'commander';
import { scraperQueue } from '../queues/scraper.queue';
import { prisma } from '../lib/prisma';
import { removeDuplicatesFromQueue } from '../utils/deduplication';
import logger from '../lib/logger';

const program = new Command();

program
  .name('data-cleaner')
  .description('Clean and optimize database and queue data')
  .version('1.0.0');

// ============================================================================
// REMOVE DUPLICATES FROM QUEUE COMMAND
// ============================================================================
program
  .command('queue-duplicates')
  .description('Remove duplicate search terms from the queue')
  .option('--dry-run', 'Show what would be removed without actually removing', false)
  .option('--quiet', 'Suppress verbose output', false)
  .action(async (options) => {
    logger.info('üßπ Removing Duplicate Search Terms from Queue\n');
    logger.info('='.repeat(60));

    if (options.dryRun) {
      logger.info('üîç Dry run mode - no changes will be made\n');

      // Analyze duplicates without removing
      const waitingJobs = await scraperQueue.getWaiting();
      const seenTerms = new Set<string>();
      const duplicates: any[] = [];

      waitingJobs.forEach(job => {
        const term = job.data.searchTerm;
        if (seenTerms.has(term)) {
          duplicates.push(job);
        } else {
          seenTerms.add(term);
        }
      });

      logger.info(`\nüìä Analysis Results:`);
      logger.info(`   - Total waiting jobs: ${waitingJobs.length}`);
      logger.info(`   - Unique terms: ${seenTerms.size}`);
      logger.info(`   - Duplicate jobs: ${duplicates.length}`);

      if (duplicates.length > 0) {
        logger.info('\nüìã Sample duplicates (would be removed):');
        duplicates.slice(0, 10).forEach((job, idx) => {
          logger.info(`   ${idx + 1}. ${job.data.searchTerm} (job #${job.id})`);
        });
        logger.info('\nRun without --dry-run to actually remove duplicates.');
      }
    } else {
      // Use shared deduplication utility
      await removeDuplicatesFromQueue({
        verbose: !options.quiet,
        showProgress: !options.quiet
      });
    }

    // Get updated queue stats
    const [waiting, active, delayed, completed, failedCount] = await Promise.all([
      scraperQueue.getWaitingCount(),
      scraperQueue.getActiveCount(),
      scraperQueue.getDelayedCount(),
      scraperQueue.getCompletedCount(),
      scraperQueue.getFailedCount(),
    ]);

    logger.info(`\nüìä ${options.dryRun ? 'Current' : 'Final'} Queue Status:`);
    logger.info(`   - Waiting: ${waiting}`);
    logger.info(`   - Active: ${active}`);
    logger.info(`   - Delayed: ${delayed}`);
    logger.info(`   - Completed: ${completed}`);
    logger.info(`   - Failed: ${failedCount}`);

    if (!options.dryRun) {
      logger.info('\nüéâ All duplicates removed! Queue fully optimized.');
    }

    await cleanup();
  });

// ============================================================================
// REMOVE PROPERTY DUPLICATES COMMAND
// ============================================================================
program
  .command('property-duplicates')
  .description('Identify and optionally remove duplicate properties in database')
  .option('--dry-run', 'Show duplicates without removing', true)
  .option('--remove', 'Actually remove duplicates (use with caution)', false)
  .action(async (options) => {
    logger.info('üîç Finding Duplicate Properties in Database\n');
    logger.info('='.repeat(60));

    // Find properties with duplicate property_id
    const duplicates = await prisma.$queryRaw<Array<{ property_id: string; count: bigint }>>`
      SELECT property_id, COUNT(*) as count
      FROM properties
      GROUP BY property_id
      HAVING COUNT(*) > 1
      ORDER BY count DESC
    `;

    logger.info(`   Found ${duplicates.length} duplicate property_ids`);

    const totalDuplicates = duplicates.reduce((sum, d) => sum + Number(d.count) - 1, 0);
    logger.info(`   Total duplicate records to remove: ${totalDuplicates}`);

    if (options.dryRun) {
      logger.info('\nüìã DRY RUN - Sample duplicates (would be removed):');
      duplicates.slice(0, 10).forEach((dup, idx) => {
        logger.info(`   ${idx + 1}. Property ID: ${dup.property_id} (${Number(dup.count)} copies)`);
      });
      logger.info('\nRun without --dry-run to actually remove duplicates.');
      await cleanup();
      return;
    }

    // Remove duplicates - keep the oldest record
    logger.info('\nüóëÔ∏è  Removing duplicates (keeping oldest record for each property_id)...');

    let removed = 0;
    for (const dup of duplicates) {
      // Get all records for this property_id
      const records = await prisma.property.findMany({
        where: { propertyId: dup.property_id },
        orderBy: { createdAt: 'asc' }
      });

      // Delete all but the first (oldest)
      const toDelete = records.slice(1).map(r => r.id);

      if (toDelete.length > 0) {
        await prisma.property.deleteMany({
          where: { id: { in: toDelete } }
        });
        removed += toDelete.length;

        if (removed % 100 === 0) {
          process.stdout.write(`\r   Progress: ${removed}/${totalDuplicates} removed`);
        }
      }
    }

    logger.info(`\n\n‚úÖ Removed ${removed} duplicate properties!`);

    // Show final stats
    const totalProperties = await prisma.property.count();
    logger.info(`\nüìä Final property count: ${totalProperties.toLocaleString()}`);

    await cleanup();
  });

// ============================================================================
// REMOVE INEFFICIENT SEARCH TERMS COMMAND
// ============================================================================
program
  .command('inefficient-terms')
  .description('Remove search terms with consistently low results')
  .option('--threshold <n>', 'Max average results to be considered inefficient', '5')
  .option('--min-attempts <n>', 'Minimum attempts before considering term inefficient', '2')
  .option('--dry-run', 'Show what would be removed without removing')
  .action(async (options: any) => {
    logger.info('üßπ Removing Inefficient Search Terms\n');
    logger.info('='.repeat(70));

    const threshold = parseInt(options.threshold);
    const minAttempts = parseInt(options.minAttempts);

    logger.info(`\nüìä Criteria:`);
    logger.info(`   - Average results <= ${threshold} properties`);
    logger.info(`   - Minimum ${minAttempts} scrape attempts`);

    // Find terms and their average results
    const termStats = await prisma.scrapeJob.groupBy({
      by: ['searchTerm'],
      where: {
        status: 'completed'
      },
      _count: true,
      _avg: {
        resultCount: true
      }
    });

    const inefficientTerms = termStats.filter(stat =>
      stat._count >= minAttempts &&
      (stat._avg.resultCount || 0) <= threshold
    );

    logger.info(`\nüìä Analyzed ${termStats.length} search terms`);
    logger.info(`üìä Inefficient terms found: ${inefficientTerms.length}`);

    if (inefficientTerms.length === 0) {
      logger.info('\n‚úÖ No inefficient terms found!');
      await cleanup();
      return;
    }

    if (options.dryRun) {
      logger.info('\nüìã DRY RUN - Sample inefficient terms (would be removed):');
      inefficientTerms.slice(0, 20).forEach((stat, idx) => {
        logger.info(`   ${idx + 1}. "${stat.searchTerm}" - avg: ${(stat._avg.resultCount || 0).toFixed(1)} properties (${stat._count} attempts)`);
      });
      logger.info('\nRun without --dry-run to actually remove these terms.');
      await cleanup();
      return;
    }

    // Remove from database
    const inefficientTermsList = inefficientTerms.map(t => t.searchTerm);
    logger.info(`\nüóëÔ∏è  Removing ${inefficientTermsList.length} inefficient terms from database...`);

    await prisma.scrapeJob.deleteMany({
      where: {
        searchTerm: { in: inefficientTermsList }
      }
    });

    logger.info(`‚úÖ Removed from database!`);

    // Remove from queue
    const waitingJobs = await scraperQueue.getWaiting();
    const inefficientQueueJobs = waitingJobs.filter(job =>
      inefficientTermsList.includes(job.data.searchTerm)
    );

    if (inefficientQueueJobs.length > 0) {
      logger.info(`\nüîç Found ${inefficientQueueJobs.length} inefficient terms in queue, removing...`);

      let removed = 0;
      for (const job of inefficientQueueJobs) {
        try {
          await job.remove();
          removed++;
          if (removed % 50 === 0) {
            process.stdout.write(`\r   Progress: ${removed}/${inefficientQueueJobs.length}`);
          }
        } catch (error) {
          // Ignore errors
        }
      }

      logger.info(`\n   ‚úÖ Removed ${removed} from queue!`);
    }

    await cleanup();
  });

// ============================================================================
// CLEAN SEARCH TERMS COMMAND
// ============================================================================
program
  .command('search-terms')
  .description('Remove problematic search terms from database')
  .option('--short', 'Remove terms shorter than 4 characters', false)
  .option('--numbers', 'Remove numeric-only terms (ZIP codes)', false)
  .option('--compounds', 'Remove compound names (first + last)', false)
  .option('--cities', 'Remove city names', false)
  .option('--dry-run', 'Show what would be removed without removing', false)
  .action(async (options) => {
    logger.info('üßπ Cleaning Problematic Search Terms\n');
    logger.info('='.repeat(60));

    const filters: string[] = [];
    const allTerms = await prisma.scrapeJob.findMany({
      select: { searchTerm: true },
      distinct: ['searchTerm']
    });

    const termsToRemove = new Set<string>();

    // Apply filters
    if (options.short) {
      filters.push('short terms (< 4 chars)');
      allTerms.forEach(t => {
        if (t.searchTerm.length < 4) {
          termsToRemove.add(t.searchTerm);
        }
      });
    }

    if (options.numbers) {
      filters.push('numeric terms (ZIP codes)');
      allTerms.forEach(t => {
        if (/^\d+$/.test(t.searchTerm)) {
          termsToRemove.add(t.searchTerm);
        }
      });
    }

    if (options.compounds) {
      filters.push('compound names');
      allTerms.forEach(t => {
        const words = t.searchTerm.split(/\s+/);
        if (words.length >= 2 && /^[A-Z][a-z]+(\s+[A-Z][a-z]+)+$/.test(t.searchTerm)) {
          termsToRemove.add(t.searchTerm);
        }
      });
    }

    if (options.cities) {
      filters.push('city names');
      const cityNames = ['Austin', 'Lakeway', 'Manor', 'Pflugerville', 'Cedar Park', 'Round Rock', 'Georgetown', 'Leander', 'Kyle', 'Buda'];
      allTerms.forEach(t => {
        if (cityNames.includes(t.searchTerm)) {
          termsToRemove.add(t.searchTerm);
        }
      });
    }

    logger.info(`\nüéØ Filters applied: ${filters.join(', ')}`);
    logger.info(`üìä Terms to remove: ${termsToRemove.size}\n`);

    if (termsToRemove.size === 0) {
      logger.info('‚úÖ No terms match the selected filters');
      await cleanup();
      return;
    }

    if (options.dryRun) {
      logger.info('\nüìã DRY RUN - Sample terms (would be removed):');
      Array.from(termsToRemove).slice(0, 20).forEach((term, idx) => {
        logger.info(`   ${idx + 1}. "${term}"`);
      });
      logger.info('\nRun without --dry-run to actually remove these terms.');
      await cleanup();
      return;
    }

    // Convert Set to Array for database operations
    const termsArray = Array.from(termsToRemove);

    // Remove from database
    logger.info(`\nüóëÔ∏è  Removing ${termsArray.length} terms from database...`);

    await prisma.scrapeJob.deleteMany({
      where: {
        searchTerm: { in: termsArray }
      }
    });

    logger.info(`‚úÖ Removed from database!`);

    // Remove from queue
    const waitingJobs = await scraperQueue.getWaiting();
    const queueJobsToRemove = waitingJobs.filter(job =>
      termsToRemove.has(job.data.searchTerm)
    );

    if (queueJobsToRemove.length > 0) {
      logger.info(`\nüîç Found ${queueJobsToRemove.length} matching terms in queue, removing...`);

      let removed = 0;
      for (const job of queueJobsToRemove) {
        try {
          await job.remove();
          removed++;
          if (removed % 50 === 0) {
            process.stdout.write(`\r   Progress: ${removed}/${queueJobsToRemove.length}`);
          }
        } catch (error) {
          // Ignore errors
        }
      }

      logger.info(`\n   ‚úÖ Removed ${removed} from queue!`);
    }

    await cleanup();
  });

/**
 * Comprehensive cleanup - all filters
 */
program
  .command('all')
  .description('Run all cleanup operations (short, numeric, duplicates, inefficient)')
  .option('--dry-run', 'Show what would be done without actually doing it')
  .action(async (options: any) => {
    logger.info('üßπ COMPREHENSIVE DATA CLEANUP\n');
    logger.info('='.repeat(70));

    if (options.dryRun) {
      logger.info('\n‚ö†Ô∏è  DRY RUN MODE - No data will be modified\n');
    }

    logger.info('\n1Ô∏è‚É£  Removing short terms...');
    // Run short-terms command
    await program.parseAsync(['node', 'data-cleaner', 'short-terms', ...(options.dryRun ? ['--dry-run'] : [])]);

    logger.info('\n2Ô∏è‚É£  Removing numeric terms...');
    // Run numeric-terms command
    await program.parseAsync(['node', 'data-cleaner', 'numeric-terms', ...(options.dryRun ? ['--dry-run'] : [])]);

    logger.info('\n3Ô∏è‚É£  Removing queue duplicates...');
    // Run queue-duplicates command (no dry-run support)
    if (!options.dryRun) {
      await program.parseAsync(['node', 'data-cleaner', 'queue-duplicates']);
    }

    logger.info('\n4Ô∏è‚É£  Removing property duplicates...');
    // Run properties-duplicates command
    await program.parseAsync(['node', 'data-cleaner', 'properties-duplicates', ...(options.dryRun ? ['--dry-run'] : [])]);

    logger.info('\n‚úÖ Comprehensive cleanup complete!');

    await cleanup();
  });

/**
 * Helper function to cleanup connections
 */
async function cleanup() {
  await scraperQueue.close();
  await prisma.$disconnect();
}

// Handle errors and cleanup
process.on('SIGINT', async () => {
  logger.info('\n\nüëã Interrupted. Cleaning up...');
  await cleanup();
  process.exit(0);
});

process.on('unhandledRejection', async (error: any) => {
  logger.error('\n‚ùå Unhandled error:', error.message);
  await cleanup();
  process.exit(1);
});

// Parse arguments
program.parse();
</file>

<file path="db-stats-simple.ts">
#!/usr/bin/env npx tsx

import { prisma } from './src/lib/prisma';
import logger from '../lib/logger';

async function checkDatabaseStats() {
  logger.info('üìä Database Statistics\n');
  logger.info('=' .repeat(60));

  // Count total properties
  const totalProperties = await prisma.property.count();
  logger.info(`\nüè† Total Properties: ${totalProperties.toLocaleString()}`);

  // Count scrape jobs by status
  const jobStats = await prisma.scrapeJob.groupBy({
    by: ['status'],
    _count: {
      _all: true
    },
    _sum: {
      resultCount: true
    }
  });

  logger.info('\nüìã Scrape Jobs:');
  let totalJobs = 0;
  let totalScraped = 0;

  jobStats.forEach(stat => {
    totalJobs += stat._count._all;
    totalScraped += stat._sum.resultCount || 0;
    logger.info(`  ${stat.status}: ${stat._count._all} jobs (${(stat._sum.resultCount || 0).toLocaleString()} properties)`);
  });

  logger.info(`  ---`);
  logger.info(`  Total Jobs: ${totalJobs}`);
  logger.info(`  Total Properties Scraped: ${totalScraped.toLocaleString()}`);

  // Properties by city
  const propertiesByCity = await prisma.property.groupBy({
    by: ['city'],
    _count: {
      _all: true
    },
    orderBy: {
      _count: {
        _all: 'desc'
      }
    },
    take: 10
  });

  logger.info('\nüèôÔ∏è  Top 10 Cities:');
  propertiesByCity.forEach((city, idx) => {
    logger.info(`  ${idx + 1}. ${city.city || 'Unknown'}: ${city._count._all.toLocaleString()} properties`);
  });

  // Most recent scrapes
  const recentJobs = await prisma.scrapeJob.findMany({
    where: { status: 'completed' },
    orderBy: { id: 'desc' },
    take: 5,
    select: {
      searchTerm: true,
      resultCount: true,
      completedAt: true
    }
  });

  logger.info('\nüìÖ Recent Completed Scrapes:');
  recentJobs.forEach((job, idx) => {
    const time = job.completedAt ? new Date(job.completedAt).toLocaleString() : 'N/A';
    logger.info(`  ${idx + 1}. "${job.searchTerm}": ${job.resultCount} properties (${time})`);
  });

  // Average properties per successful scrape
  const avgStats = await prisma.scrapeJob.aggregate({
    where: {
      status: 'completed',
      resultCount: { gt: 0 }
    },
    _avg: {
      resultCount: true
    },
    _max: {
      resultCount: true
    },
    _min: {
      resultCount: true
    }
  });

  logger.info('\nüìà Scrape Performance:');
  logger.info(`  Average properties per scrape: ${avgStats._avg.resultCount?.toFixed(0) || 0}`);
  logger.info(`  Max properties in single scrape: ${avgStats._max.resultCount || 0}`);
  logger.info(`  Min properties in single scrape: ${avgStats._min.resultCount || 0}`);

  logger.info('\n' + '=' .repeat(60));

  await prisma.$disconnect();
}

checkDatabaseStats()
  .then(() => {
    process.exit(0);
  })
  .catch(async (error) => {
    logger.error('‚ùå Error:', error);
    await prisma.$disconnect();
    process.exit(1);
  });
</file>

<file path="db-stats.ts">
#!/usr/bin/env npx tsx

import { Command } from 'commander';
import { prisma } from '../lib/prisma';
import { scraperQueue } from '../queues/scraper.queue';
import logger from '../lib/logger';

const program = new Command();

program
  .name('db-stats')
  .description('Display database statistics and metrics')
  .version('1.0.0');

/**
 * Show comprehensive summary statistics
 */
program
  .command('summary')
  .description('Show comprehensive database statistics summary')
  .action(async () => {
    logger.info('üìä Database Statistics Summary\n');
    logger.info('='.repeat(70));

    // Count total properties
    const totalProperties = await prisma.property.count();
    logger.info(`\nüè† Total Properties: ${totalProperties.toLocaleString()}`);

    // Count scrape jobs by status
    const jobStats = await prisma.scrapeJob.groupBy({
      by: ['status'],
      _count: {
        _all: true
      },
      _sum: {
        resultCount: true
      }
    });

    logger.info('\nüìã Scrape Jobs:');
    let totalJobs = 0;
    let totalScraped = 0;

    jobStats.forEach(stat => {
      totalJobs += stat._count._all;
      totalScraped += stat._sum.resultCount || 0;
      logger.info(`   ${stat.status}: ${stat._count._all} jobs (${(stat._sum.resultCount || 0).toLocaleString()} properties)`);
    });

    logger.info(`   ---`);
    logger.info(`   Total Jobs: ${totalJobs}`);
    logger.info(`   Total Properties Scraped: ${totalScraped.toLocaleString()}`);

    // Average properties per successful scrape
    const avgStats = await prisma.scrapeJob.aggregate({
      where: {
        status: 'completed',
        resultCount: { gt: 0 }
      },
      _avg: {
        resultCount: true
      },
      _max: {
        resultCount: true
      },
      _min: {
        resultCount: true
      }
    });

    logger.info('\nüìà Scrape Performance:');
    logger.info(`   Average properties per scrape: ${avgStats._avg.resultCount?.toFixed(0) || 0}`);
    logger.info(`   Max properties in single scrape: ${avgStats._max.resultCount || 0}`);
    logger.info(`   Min properties in single scrape: ${avgStats._min.resultCount || 0}`);

    // Most recent scrapes
    const recentJobs = await prisma.scrapeJob.findMany({
      where: { status: 'completed' },
      orderBy: { id: 'desc' },
      take: 5,
      select: {
        searchTerm: true,
        resultCount: true,
        completedAt: true
      }
    });

    logger.info('\nüìÖ Recent Completed Scrapes:');
    recentJobs.forEach((job, idx) => {
      const time = job.completedAt ? new Date(job.completedAt).toLocaleString() : 'N/A';
      logger.info(`   ${idx + 1}. "${job.searchTerm}": ${job.resultCount} properties (${time})`);
    });

    logger.info('\n' + '='.repeat(70));

    await cleanup();
  });

/**
 * Show property statistics
 */
program
  .command('properties')
  .description('Show detailed property statistics')
  .option('--by-city', 'Show properties grouped by city')
  .option('--by-type', 'Show properties grouped by type')
  .option('--top <n>', 'Show top N results', '10')
  .action(async (options: any) => {
    logger.info('üè† Property Statistics\n');
    logger.info('='.repeat(70));

    const totalProperties = await prisma.property.count();
    logger.info(`\nüìä Total Properties: ${totalProperties.toLocaleString()}`);

    // By city
    if (options.byCity) {
      const propertiesByCity = await prisma.property.groupBy({
        by: ['city'],
        _count: {
          id: true
        },
        orderBy: {
          _count: {
            id: 'desc'
          }
        },
        take: parseInt(options.top)
      });

      logger.info(`\nüèôÔ∏è  Top ${options.top} Cities:`);
      propertiesByCity.forEach((city, idx) => {
        const count = city._count.id;
        const pct = ((count / totalProperties) * 100).toFixed(1);
        logger.info(`   ${idx + 1}. ${city.city || 'Unknown'}: ${count.toLocaleString()} (${pct}%)`);
      });
    }

    // By property type
    if (options.byType) {
      const propertiesByType = await prisma.property.groupBy({
        by: ['propType'],
        _count: {
          id: true
        },
        orderBy: {
          _count: {
            id: 'desc'
          }
        },
        take: parseInt(options.top)
      });

      logger.info(`\nüèóÔ∏è  Top ${options.top} Property Types:`);
      propertiesByType.forEach((type, idx) => {
        const count = type._count.id;
        const pct = ((count / totalProperties) * 100).toFixed(1);
        logger.info(`   ${idx + 1}. ${type.propType}: ${count.toLocaleString()} (${pct}%)`);
      });
    }

    // Value statistics
    const valueStats = await prisma.property.aggregate({
      _avg: {
        appraisedValue: true,
        assessedValue: true
      },
      _max: {
        appraisedValue: true,
        assessedValue: true
      },
      _min: {
        appraisedValue: true,
        assessedValue: true
      }
    });

    logger.info('\nüí∞ Property Values:');
    logger.info(`   Average Appraised: $${(valueStats._avg.appraisedValue || 0).toLocaleString()}`);
    logger.info(`   Max Appraised: $${(valueStats._max.appraisedValue || 0).toLocaleString()}`);
    logger.info(`   Min Appraised: $${(valueStats._min.appraisedValue || 0).toLocaleString()}`);
    logger.info(`   Average Assessed: $${(valueStats._avg.assessedValue || 0).toLocaleString()}`);

    logger.info('\n' + '='.repeat(70));

    await cleanup();
  });

/**
 * Show scraping rate and progress
 */
program
  .command('rate')
  .description('Show scraping rate and time-based statistics')
  .option('--days <n>', 'Analyze last N days', '7')
  .action(async (options: any) => {
    logger.info('‚ö° Scraping Rate Analysis\n');
    logger.info('='.repeat(70));

    const days = parseInt(options.days);
    const since = new Date();
    since.setDate(since.getDate() - days);

    // Get jobs completed in timeframe
    const recentJobs = await prisma.scrapeJob.findMany({
      where: {
        status: 'completed',
        completedAt: { gte: since }
      },
      select: {
        resultCount: true,
        completedAt: true,
        startedAt: true
      }
    });

    logger.info(`\nüìÖ Last ${days} days:`);
    logger.info(`   Jobs completed: ${recentJobs.length}`);

    const totalProperties = recentJobs.reduce((sum, j) => sum + (j.resultCount || 0), 0);
    logger.info(`   Properties scraped: ${totalProperties.toLocaleString()}`);

    const jobsPerDay = recentJobs.length / days;
    const propertiesPerDay = totalProperties / days;

    logger.info(`\nüìà Rates:`);
    logger.info(`   Jobs per day: ${jobsPerDay.toFixed(1)}`);
    logger.info(`   Properties per day: ${propertiesPerDay.toFixed(0)}`);
    logger.info(`   Avg properties per job: ${(totalProperties / recentJobs.length || 0).toFixed(1)}`);

    // Calculate processing time
    const processingTimes = recentJobs
      .filter(j => j.startedAt && j.completedAt)
      .map(j => {
        const created = new Date(j.startedAt!).getTime();
        const completed = new Date(j.completedAt!).getTime();
        return (completed - created) / 1000; // seconds
      });

    if (processingTimes.length > 0) {
      const avgTime = processingTimes.reduce((sum, t) => sum + t, 0) / processingTimes.length;

      logger.info(`\n‚è±Ô∏è  Average Processing Time:`);
      logger.info(`   ${(avgTime / 60).toFixed(1)} minutes per job`);
    }

    // Get queue status
    const [waiting, active] = await Promise.all([
      scraperQueue.getWaitingCount(),
      scraperQueue.getActiveCount()
    ]);

    logger.info(`\nüìä Current Queue:`);
    logger.info(`   Waiting: ${waiting}`);
    logger.info(`   Active: ${active}`);

    // Estimate completion time
    if (waiting > 0 && jobsPerDay > 0) {
      const daysToComplete = waiting / jobsPerDay;
      logger.info(`\n‚è≥ Estimated time to clear queue:`);
      logger.info(`   ${daysToComplete.toFixed(1)} days at current rate`);
    }

    logger.info('\n' + '='.repeat(70));

    await cleanup();
  });

/**
 * Show search term statistics
 */
program
  .command('search-terms')
  .description('Show statistics about search terms')
  .option('--top <n>', 'Show top N most productive terms', '10')
  .action(async (options: any) => {
    logger.info('üîç Search Term Statistics\n');
    logger.info('='.repeat(70));

    // Count distinct search terms
    const distinctTerms = await prisma.scrapeJob.findMany({
      select: { searchTerm: true },
      distinct: ['searchTerm']
    });

    logger.info(`\nüìä Total distinct search terms: ${distinctTerms.length.toLocaleString()}`);

    // Terms by status
    const termsByStatus = await prisma.scrapeJob.groupBy({
      by: ['status'],
      _count: {
        searchTerm: true
      }
    });

    logger.info('\nüìã Search Terms by Status:');
    termsByStatus.forEach(stat => {
      logger.info(`   ${stat.status}: ${stat._count.searchTerm} terms`);
    });

    // Most productive terms
    const topTerms = await prisma.scrapeJob.findMany({
      where: {
        status: 'completed',
        resultCount: { gt: 0 }
      },
      orderBy: {
        resultCount: 'desc'
      },
      take: parseInt(options.top),
      select: {
        searchTerm: true,
        resultCount: true
      }
    });

    logger.info(`\nüèÜ Top ${options.top} Most Productive Terms:`);
    topTerms.forEach((term, idx) => {
      logger.info(`   ${idx + 1}. "${term.searchTerm}": ${(term.resultCount || 0).toLocaleString()} properties`);
    });

    // Zero result terms
    const zeroResultCount = await prisma.scrapeJob.count({
      where: {
        status: 'completed',
        resultCount: 0
      }
    });

    logger.info(`\n‚ùå Zero-result scrapes: ${zeroResultCount}`);

    // Average results per term
    const avgResults = await prisma.scrapeJob.aggregate({
      where: {
        status: 'completed',
        resultCount: { gt: 0 }
      },
      _avg: {
        resultCount: true
      }
    });

    logger.info(`üìä Average results per successful term: ${(avgResults._avg.resultCount || 0).toFixed(1)}`);

    logger.info('\n' + '='.repeat(70));

    await cleanup();
  });

/**
 * Show priority job results
 */
program
  .command('priority')
  .description('Show statistics for priority jobs')
  .option('--recent <n>', 'Show last N priority jobs', '10')
  .action(async (options: any) => {
    logger.info('‚≠ê Priority Job Statistics\n');
    logger.info('='.repeat(70));

    // This assumes priority jobs were marked with specific search terms
    // Adjust the query based on how priority jobs are identified
    const priorityJobs = await prisma.scrapeJob.findMany({
      where: {
        searchTerm: { in: ['Estate', 'Family', 'Trust'] }
      },
      select: {
        searchTerm: true,
        status: true,
        resultCount: true,
        completedAt: true,
        startedAt: true
      },
      orderBy: {
        startedAt: 'desc'
      },
      take: parseInt(options.recent)
    });

    logger.info(`\nüìä Priority jobs found: ${priorityJobs.length}`);

    if (priorityJobs.length === 0) {
      logger.info('\nNo priority jobs found in database.');
      await cleanup();
      return;
    }

    // Group by status
    const statusCounts: Record<string, number> = {};
    let totalResults = 0;

    priorityJobs.forEach(job => {
      statusCounts[job.status] = (statusCounts[job.status] || 0) + 1;
      totalResults += job.resultCount || 0;
    });

    logger.info('\nüìã By Status:');
    Object.entries(statusCounts).forEach(([status, count]) => {
      logger.info(`   ${status}: ${count}`);
    });

    logger.info(`\nüìà Total properties from priority jobs: ${totalResults.toLocaleString()}`);
    logger.info(`üìà Average per priority job: ${(totalResults / priorityJobs.length).toFixed(1)}`);

    logger.info(`\nüìÖ Recent Priority Jobs:`);
    priorityJobs.forEach((job, idx) => {
      const status = job.status === 'completed' ? '‚úÖ' : job.status === 'failed' ? '‚ùå' : '‚è≥';
      const results = job.status === 'completed' ? ` (${job.resultCount} props)` : '';
      logger.info(`   ${idx + 1}. ${status} "${job.searchTerm}"${results}`);
    });

    logger.info('\n' + '='.repeat(70));

    await cleanup();
  });

/**
 * Show all statistics - comprehensive view
 */
program
  .command('all')
  .description('Show all available statistics (comprehensive report)')
  .action(async () => {
    logger.info('üìä COMPREHENSIVE DATABASE REPORT\n');
    logger.info('='.repeat(70));

    // Run all stat commands
    logger.info('\n1Ô∏è‚É£  SUMMARY\n');
    await program.parseAsync(['node', 'db-stats', 'summary']);

    logger.info('\n2Ô∏è‚É£  PROPERTIES\n');
    await program.parseAsync(['node', 'db-stats', 'properties', '--by-city', '--by-type']);

    logger.info('\n3Ô∏è‚É£  SCRAPING RATE\n');
    await program.parseAsync(['node', 'db-stats', 'rate']);

    logger.info('\n4Ô∏è‚É£  SEARCH TERMS\n');
    await program.parseAsync(['node', 'db-stats', 'search-terms']);

    logger.info('\n‚úÖ Comprehensive report complete!\n');
    logger.info('='.repeat(70));

    await cleanup();
  });

/**
 * Helper function to cleanup connections
 */
async function cleanup() {
  await scraperQueue.close();
  await prisma.$disconnect();
}

// Handle errors and cleanup
process.on('SIGINT', async () => {
  logger.info('\n\nüëã Interrupted. Cleaning up...');
  await cleanup();
  process.exit(0);
});

process.on('unhandledRejection', async (error: any) => {
  logger.error('\n‚ùå Unhandled error:', error.message);
  await cleanup();
  process.exit(1);
});

// Parse arguments
program.parse();
</file>

<file path="queue-analyzer.ts">
#!/usr/bin/env npx tsx

import { Command } from 'commander';
import { scraperQueue } from '../queues/scraper.queue';
import { prisma } from '../lib/prisma';
import logger from '../lib/logger';

const program = new Command();

program
  .name('queue-analyzer')
  .description('Analyze queue performance and search term effectiveness')
  .version('1.0.0');

/**
 * Analyze successful search terms by category
 */
program
  .command('success')
  .description('Analyze most successful search term patterns')
  .option('--top <n>', 'Show top N examples per category', '5')
  .action(async (options: any) => {
    logger.info('üîç Analyzing Most Successful Search Term Types\n');
    logger.info('='.repeat(70));

    // Get all successful jobs (with results)
    const successfulJobs = await prisma.scrapeJob.findMany({
      where: {
        status: 'completed',
        resultCount: { gt: 0 }
      },
      select: {
        searchTerm: true,
        resultCount: true
      },
      orderBy: {
        resultCount: 'desc'
      }
    });

    logger.info(`\nüìä Total successful scrapes: ${successfulJobs.length.toLocaleString()}`);

    const totalProperties = successfulJobs.reduce((sum, job) => sum + (job.resultCount || 0), 0);
    logger.info(`üìä Total properties found: ${totalProperties.toLocaleString()}`);
    logger.info(`üìä Average per successful search: ${(totalProperties / successfulJobs.length).toFixed(1)}`);

    // Categorize search terms
    const categories = {
      singleLastName: [] as typeof successfulJobs,
      fullName: [] as typeof successfulJobs,
      businessWithSuffix: [] as typeof successfulJobs,
      businessGeneric: [] as typeof successfulJobs,
      streetAddress: [] as typeof successfulJobs,
      streetName: [] as typeof successfulJobs,
      shortCode: [] as typeof successfulJobs,
      other: [] as typeof successfulJobs
    };

    const businessSuffixes = /\b(LLC|Inc|Corp|Company|Trust|Foundation|Partners|Group|Properties|Ventures|Capital|Holdings|Development|Estate|Assets|Portfolio|LTD|Enterprises|Management|Realty|Investment)\b/i;
    const streetSuffixes = /\b(Street|St|Avenue|Ave|Road|Rd|Drive|Dr|Lane|Ln|Boulevard|Blvd|Court|Ct|Circle|Cir|Way|Place|Pl|Parkway|Loop|Trail|Path|Highway|Hwy)\b/i;
    const commonStreets = /\b(Lamar|Congress|Riverside|Guadalupe|Airport|Burnet|Mopac|Anderson|MLK|Oltorf|Barton|Springs|Research|Metric|Wells Branch|Far West|Dean Keeton|Speedway|Red River|Manchaca|William Cannon|Slaughter|Parmer|Braker|Rundberg|Koenig|North Loop|South Congress|East Riverside|West Anderson|Capital of Texas|Loop 360|IH 35|Enfield|Exposition|Westlake|Windsor|Oak|Rainey|Cameron|Duval|San Jacinto|Shoal Creek|Cesar Chavez|Main|Howard|McNeil|Dessau|Jollyville|Spicewood|Bee Cave|Balcones|Mueller|Cherrywood|Sabine|Nueces|Trinity|Rio Grande|Manor|Springdale)\b/i;

    successfulJobs.forEach(job => {
      const term = job.searchTerm;
      const words = term.split(/\s+/);

      // Check for street address (starts with number + street name)
      if (/^\d+\s+/.test(term) && (streetSuffixes.test(term) || commonStreets.test(term))) {
        categories.streetAddress.push(job);
      }
      // Check for street name only
      else if (streetSuffixes.test(term) || commonStreets.test(term)) {
        categories.streetName.push(job);
      }
      // Check for business with suffix
      else if (businessSuffixes.test(term)) {
        categories.businessWithSuffix.push(job);
      }
      // Check for full name (2-3 words, mostly letters, capitalized)
      else if (words.length >= 2 && words.length <= 3 && /^[A-Z][a-z]+(\s+[A-Z][a-z]+)+$/.test(term)) {
        categories.fullName.push(job);
      }
      // Check for single last name (one word, mostly letters, capitalized)
      else if (words.length === 1 && /^[A-Z][a-z]+$/.test(term) && term.length > 3) {
        categories.singleLastName.push(job);
      }
      // Check for short codes (alphanumeric, short)
      else if (term.length <= 6 && /[A-Z0-9]/.test(term)) {
        categories.shortCode.push(job);
      }
      // Everything else
      else {
        categories.other.push(job);
      }
    });

    // Calculate statistics for each category
    const stats = Object.entries(categories).map(([name, jobs]) => {
      const total = jobs.reduce((sum, job) => sum + (job.resultCount || 0), 0);
      const avg = jobs.length > 0 ? total / jobs.length : 0;
      const max = jobs.length > 0 ? Math.max(...jobs.map(j => j.resultCount || 0)) : 0;
      return {
        name,
        count: jobs.length,
        totalProperties: total,
        avgProperties: avg,
        maxProperties: max,
        percentage: (jobs.length / successfulJobs.length) * 100
      };
    }).sort((a, b) => b.totalProperties - a.totalProperties);

    logger.info('\nüìã Search Term Categories (by total properties found):\n');
    stats.forEach((stat, idx) => {
      const categoryName = stat.name
        .replace(/([A-Z])/g, ' $1')
        .replace(/^./, str => str.toUpperCase())
        .trim();

      logger.info(`${idx + 1}. ${categoryName}`);
      logger.info(`   Searches: ${stat.count.toLocaleString()} (${stat.percentage.toFixed(1)}%)`);
      logger.info(`   Properties: ${stat.totalProperties.toLocaleString()}`);
      logger.info(`   Avg per search: ${stat.avgProperties.toFixed(1)}`);
      logger.info(`   Max in single search: ${stat.maxProperties}`);
      logger.info('');
    });

    // Show top examples from each category
    logger.info('='.repeat(70));
    logger.info('\nüèÜ TOP PERFORMERS BY CATEGORY:\n');

    const topN = parseInt(options.top);

    for (const [categoryName, jobs] of Object.entries(categories)) {
      if (jobs.length === 0) continue;

      const displayName = categoryName
        .replace(/([A-Z])/g, ' $1')
        .replace(/^./, str => str.toUpperCase())
        .trim();

      logger.info(`\n${displayName} (${jobs.length} total):`);

      const topJobs = jobs.sort((a, b) => (b.resultCount || 0) - (a.resultCount || 0)).slice(0, topN);

      topJobs.forEach((job, idx) => {
        logger.info(`  ${idx + 1}. "${job.searchTerm}" - ${(job.resultCount || 0).toLocaleString()} properties`);
      });
    }

    logger.info('\n' + '='.repeat(70));
    logger.info('\nüí° Insights:');
    logger.info('   - Focus on search term types with high avg properties per search');
    logger.info('   - Consider filtering out or deprioritizing "Short Code" and "Other" categories');
    logger.info('   - Business names with suffixes (LLC, Trust, etc.) are highly effective');

    await cleanup();
  });

/**
 * Analyze failed searches
 */
program
  .command('failures')
  .description('Analyze failed search patterns and zero-result terms')
  .option('--limit <n>', 'Limit results', '50')
  .action(async (options: any) => {
    logger.info('‚ùå Analyzing Failed and Zero-Result Searches\n');
    logger.info('='.repeat(70));

    // Get failed jobs
    const failedJobs = await prisma.scrapeJob.findMany({
      where: { status: 'failed' },
      select: {
        searchTerm: true,
        error: true,
      },
      take: parseInt(options.limit)
    });

    logger.info(`\nüìä Failed Jobs: ${failedJobs.length}`);

    if (failedJobs.length > 0) {
      logger.info('\nCommon Failure Patterns:');
      const errorCounts: Record<string, number> = {};

      failedJobs.forEach(job => {
        const errorType = job.error ? job.error.split(':')[0].trim() : 'Unknown';
        errorCounts[errorType] = (errorCounts[errorType] || 0) + 1;
      });

      Object.entries(errorCounts)
        .sort((a, b) => b[1] - a[1])
        .forEach(([error, count]) => {
          logger.info(`   - ${error}: ${count} occurrences`);
        });
    }

    // Get zero-result jobs
    const zeroResultJobs = await prisma.scrapeJob.findMany({
      where: {
        status: 'completed',
        resultCount: 0
      },
      select: {
        searchTerm: true,
      },
      take: parseInt(options.limit)
    });

    logger.info(`\nüìä Zero-Result Searches: ${zeroResultJobs.length}`);

    // Analyze zero-result patterns
    if (zeroResultJobs.length > 0) {
      const patterns = {
        tooShort: zeroResultJobs.filter(j => j.searchTerm.length <= 2),
        numbers: zeroResultJobs.filter(j => /^\d+$/.test(j.searchTerm)),
        specialChars: zeroResultJobs.filter(j => /[^a-zA-Z0-9\s]/.test(j.searchTerm)),
        singleLetter: zeroResultJobs.filter(j => /^[A-Z]$/.test(j.searchTerm)),
      };

      logger.info('\nZero-Result Patterns:');
      logger.info(`   - Too short (<= 2 chars): ${patterns.tooShort.length}`);
      logger.info(`   - Pure numbers: ${patterns.numbers.length}`);
      logger.info(`   - Contains special chars: ${patterns.specialChars.length}`);
      logger.info(`   - Single letter: ${patterns.singleLetter.length}`);

      logger.info('\nSample Zero-Result Terms:');
      zeroResultJobs.slice(0, 10).forEach((job, idx) => {
        logger.info(`   ${idx + 1}. "${job.searchTerm}"`);
      });
    }

    logger.info('\n' + '='.repeat(70));
    logger.info('\nüí° Recommendations:');
    logger.info('   - Filter out single letters and numbers before queuing');
    logger.info('   - Implement minimum length requirement (3+ characters)');
    logger.info('   - Consider removing terms with special characters');
    logger.info('   - Use "queue-manager cleanup --zero-results" to clean queue');

    await cleanup();
  });

/**
 * Analyze queue performance metrics
 */
program
  .command('performance')
  .description('Analyze queue performance metrics and throughput')
  .option('--days <n>', 'Analyze last N days', '7')
  .action(async (options: any) => {
    logger.info('‚ö° Queue Performance Analysis\n');
    logger.info('='.repeat(70));

    const days = parseInt(options.days);
    const since = new Date();
    since.setDate(since.getDate() - days);

    // Get completed jobs in timeframe
    const completed = await prisma.scrapeJob.findMany({
      where: {
        status: 'completed',
        completedAt: { gte: since }
      },
      select: {
        searchTerm: true,
        resultCount: true,
        completedAt: true,
        startedAt: true,
      }
    });

    logger.info(`\nüìä Jobs completed in last ${days} days: ${completed.length}`);

    if (completed.length === 0) {
      logger.info('\nNo completed jobs in this timeframe.');
      await cleanup();
      return;
    }

    // Calculate throughput
    const totalProperties = completed.reduce((sum, j) => sum + (j.resultCount || 0), 0);
    const avgPropertiesPerJob = totalProperties / completed.length;
    const jobsPerDay = completed.length / days;
    const propertiesPerDay = totalProperties / days;

    logger.info(`\nüìà Throughput:`);
    logger.info(`   - Jobs per day: ${jobsPerDay.toFixed(1)}`);
    logger.info(`   - Properties per day: ${propertiesPerDay.toFixed(0)}`);
    logger.info(`   - Avg properties per job: ${avgPropertiesPerJob.toFixed(1)}`);

    // Calculate processing time
    const processingTimes = completed
      .filter(j => j.startedAt && j.completedAt)
      .map(j => {
        const created = new Date(j.startedAt!).getTime();
        const completedTime = new Date(j.completedAt!).getTime();
        return (completedTime - created) / 1000; // seconds
      });

    if (processingTimes.length > 0) {
      const avgTime = processingTimes.reduce((sum, t) => sum + t, 0) / processingTimes.length;
      const minTime = Math.min(...processingTimes);
      const maxTime = Math.max(...processingTimes);

      logger.info(`\n‚è±Ô∏è  Processing Time:`);
      logger.info(`   - Average: ${(avgTime / 60).toFixed(1)} minutes`);
      logger.info(`   - Minimum: ${(minTime / 60).toFixed(1)} minutes`);
      logger.info(`   - Maximum: ${(maxTime / 60).toFixed(1)} minutes`);
    }

    // Get current queue metrics
    const [waiting, active, failed] = await Promise.all([
      scraperQueue.getWaitingCount(),
      scraperQueue.getActiveCount(),
      scraperQueue.getFailedCount(),
    ]);

    logger.info(`\nüìä Current Queue State:`);
    logger.info(`   - Waiting: ${waiting}`);
    logger.info(`   - Active: ${active}`);
    logger.info(`   - Failed: ${failed}`);

    // Estimate completion time
    if (waiting > 0 && jobsPerDay > 0) {
      const daysToComplete = waiting / jobsPerDay;
      logger.info(`\n‚è≥ Estimated time to clear queue: ${daysToComplete.toFixed(1)} days`);
    }

    // Success rate
    const totalJobsAttempted = completed.length + failed;
    const successRate = (completed.length / totalJobsAttempted) * 100;

    logger.info(`\n‚úÖ Success Rate: ${successRate.toFixed(1)}% (${completed.length}/${totalJobsAttempted})`);

    logger.info('\n' + '='.repeat(70));

    await cleanup();
  });

/**
 * Comprehensive queue overview
 */
program
  .command('overview')
  .description('Show comprehensive queue and performance overview')
  .action(async () => {
    logger.info('üìä Queue Overview\n');
    logger.info('='.repeat(70));

    // Get all counts
    const [waiting, active, delayed, completed, failed] = await Promise.all([
      scraperQueue.getWaitingCount(),
      scraperQueue.getActiveCount(),
      scraperQueue.getDelayedCount(),
      scraperQueue.getCompletedCount(),
      scraperQueue.getFailedCount(),
    ]);

    logger.info(`\nüî¢ Queue Counts:`);
    logger.info(`   - Waiting: ${waiting}`);
    logger.info(`   - Active: ${active}`);
    logger.info(`   - Delayed: ${delayed}`);
    logger.info(`   - Completed: ${completed}`);
    logger.info(`   - Failed: ${failed}`);

    // Get database stats
    const [totalProperties, totalJobs, distinctTerms] = await Promise.all([
      prisma.property.count(),
      prisma.scrapeJob.count(),
      prisma.scrapeJob.findMany({
        select: { searchTerm: true },
        distinct: ['searchTerm']
      }).then(results => results.length)
    ]);

    logger.info(`\nüìä Database Stats:`);
    logger.info(`   - Total properties: ${totalProperties.toLocaleString()}`);
    logger.info(`   - Total scrape jobs: ${totalJobs.toLocaleString()}`);
    logger.info(`   - Distinct search terms: ${distinctTerms.toLocaleString()}`);

    // Get success/failure breakdown
    const successfulJobs = await prisma.scrapeJob.count({
      where: { status: 'completed', resultCount: { gt: 0 } }
    });

    const zeroResultJobs = await prisma.scrapeJob.count({
      where: { status: 'completed', resultCount: 0 }
    });

    const successRate = totalJobs > 0 ? (successfulJobs / totalJobs) * 100 : 0;

    logger.info(`\n‚úÖ Success Metrics:`);
    logger.info(`   - Successful jobs: ${successfulJobs.toLocaleString()}`);
    logger.info(`   - Zero-result jobs: ${zeroResultJobs.toLocaleString()}`);
    logger.info(`   - Failed jobs: ${failed}`);
    logger.info(`   - Success rate: ${successRate.toFixed(1)}%`);

    logger.info('\n' + '='.repeat(70));

    await cleanup();
  });

/**
 * Helper function to cleanup connections
 */
async function cleanup() {
  await scraperQueue.close();
  await prisma.$disconnect();
}

// Handle errors and cleanup
process.on('SIGINT', async () => {
  logger.info('\n\nüëã Interrupted. Cleaning up...');
  await cleanup();
  process.exit(0);
});

process.on('unhandledRejection', async (error: any) => {
  logger.error('\n‚ùå Unhandled error:', error.message);
  await cleanup();
  process.exit(1);
});

// Parse arguments
program.parse();
</file>

<file path="queue-manager.ts">
#!/usr/bin/env npx tsx

import { Command } from 'commander';
import { scraperQueue } from '../queues/scraper.queue';
import { prisma } from '../lib/prisma';
import * as fs from 'fs';
import * as path from 'path';
import logger from '../lib/logger';

const program = new Command();

program
  .name('queue-manager')
  .description('Manage scraper job queue - consolidates queue management utilities')
  .version('1.0.0');

/**
 * Add search terms to queue from file
 */
program
  .command('add-terms')
  .description('Add search terms from file to queue')
  .argument('<file>', 'Path to file with one term per line')
  .option('-p, --priority', 'Add as priority jobs (highest priority)')
  .option('-d, --dedupe', 'Remove duplicates from database first')
  .option('--priority-level <level>', 'Set priority level (1-10, lower is higher priority)', '10')
  .action(async (file: string, options: any) => {
    logger.info('üìù Adding Search Terms to Queue\n');
    logger.info('='.repeat(60));

    const filePath = path.resolve(file);

    // Check if file exists
    if (!fs.existsSync(filePath)) {
      logger.error(`‚ùå Error: File not found: ${filePath}`);
      process.exit(1);
    }

    // Read terms from file
    const content = fs.readFileSync(filePath, 'utf-8');
    const terms = content
      .split('\n')
      .map(line => line.trim())
      .filter(line => line && !line.startsWith('#'));

    logger.info(`üìÑ File: ${path.basename(filePath)}`);
    logger.info(`üìä Found ${terms.length} search terms`);

    // Dedupe if requested
    if (options.dedupe) {
      logger.info('\nüîç Checking for existing search terms in database...');
      const existing = await prisma.scrapeJob.findMany({
        where: { searchTerm: { in: terms } },
        select: { searchTerm: true },
        distinct: ['searchTerm']
      });
      const existingSet = new Set(existing.map(j => j.searchTerm));
      const newTerms = terms.filter(t => !existingSet.has(t));
      logger.info(`   - Existing terms (skipped): ${existing.length}`);
      logger.info(`   - New terms to add: ${newTerms.length}`);

      if (newTerms.length === 0) {
        logger.info('\n‚úÖ No new terms to add (all already in database)');
        await cleanup();
        return;
      }
      // Use only new terms
      terms.length = 0;
      terms.push(...newTerms);
    }

    const priorityLevel = options.priority ? 1 : parseInt(options.priorityLevel);

    logger.info(`\nüöÄ Adding ${terms.length} jobs to queue...`);
    logger.info(`   Priority: ${priorityLevel} ${options.priority ? '(highest)' : ''}`);

    let added = 0;
    for (const term of terms) {
      try {
        await scraperQueue.add(
          'scrape-properties',
          {
            searchTerm: term,
            userId: 'cli-queue-manager',
            scheduled: false,
          },
          {
            priority: priorityLevel,
            attempts: 3,
            backoff: {
              type: 'exponential',
              delay: 2000,
            },
          }
        );
        added++;
        if (added % 10 === 0) {
          process.stdout.write(`\r   Progress: ${added}/${terms.length} (${((added/terms.length)*100).toFixed(1)}%)`);
        }
      } catch (error: any) {
        logger.error(`\n   ‚ùå Failed to add "${term}":`, error.message);
      }
    }

    logger.info(`\n\n‚úÖ Added ${added} jobs to queue!`);

    // Show queue status
    const [waiting, active] = await Promise.all([
      scraperQueue.getWaitingCount(),
      scraperQueue.getActiveCount(),
    ]);

    logger.info(`\nüìä Queue Status:`);
    logger.info(`   - Waiting: ${waiting}`);
    logger.info(`   - Active: ${active}`);

    await cleanup();
  });

/**
 * Stop all pending/waiting jobs
 */
program
  .command('stop')
  .description('Stop all pending jobs in queue (active jobs will complete)')
  .option('--force', 'Also attempt to fail active jobs')
  .action(async (options: any) => {
    logger.info('üõë Stopping All Jobs in Queue\n');
    logger.info('='.repeat(60));

    // Get current queue stats
    const [waiting, active, delayed] = await Promise.all([
      scraperQueue.getWaitingCount(),
      scraperQueue.getActiveCount(),
      scraperQueue.getDelayedCount(),
    ]);

    logger.info(`üìä Current Queue State:`);
    logger.info(`   - Waiting: ${waiting}`);
    logger.info(`   - Active: ${active}`);
    logger.info(`   - Delayed: ${delayed}`);
    logger.info(`   - Total to stop: ${waiting + delayed}\n`);

    if (waiting + delayed === 0 && !options.force) {
      logger.info('‚úÖ No pending jobs to stop (queue is empty)');

      if (active > 0) {
        logger.info(`\n‚ÑπÔ∏è  Note: ${active} jobs are currently active and cannot be stopped.`);
        logger.info('   They will finish processing. Use --force to attempt to fail them.');
      }
      await cleanup();
      return;
    }

    let removed = 0;
    let failed = 0;

    // Remove waiting jobs
    if (waiting > 0) {
      logger.info(`üìã Removing ${waiting} waiting jobs...`);
      const waitingJobs = await scraperQueue.getWaiting();

      for (const job of waitingJobs) {
        try {
          await job.remove();
          removed++;
          if (removed % 50 === 0) {
            process.stdout.write(`\r   Progress: ${removed}/${waiting + delayed} (${((removed/(waiting + delayed))*100).toFixed(1)}%)`);
          }
        } catch (error: any) {
          failed++;
          if (failed <= 3) {
            logger.error(`\n   ‚ùå Failed to remove job ${job.id}:`, error.message);
          }
        }
      }
    }

    // Remove delayed jobs
    if (delayed > 0) {
      logger.info(`\n‚è∞ Removing ${delayed} delayed jobs...`);
      const delayedJobs = await scraperQueue.getDelayed();

      for (const job of delayedJobs) {
        try {
          await job.remove();
          removed++;
          if (removed % 50 === 0) {
            process.stdout.write(`\r   Progress: ${removed}/${waiting + delayed} (${((removed/(waiting + delayed))*100).toFixed(1)}%)`);
          }
        } catch (error: any) {
          failed++;
          if (failed <= 3) {
            logger.error(`\n   ‚ùå Failed to remove job ${job.id}:`, error.message);
          }
        }
      }
    }

    // Force-fail active jobs if requested
    if (options.force && active > 0) {
      logger.info(`\n‚ö†Ô∏è  Force-failing ${active} active jobs...`);
      const activeJobs = await scraperQueue.getActive();

      for (const job of activeJobs) {
        try {
          await job.moveToFailed(new Error('Force-stopped by CLI'), false);
          removed++;
          if (removed % 10 === 0) {
            process.stdout.write(`\r   Progress: ${removed}/${waiting + delayed + active} jobs`);
          }
        } catch (error: any) {
          failed++;
          if (failed <= 3) {
            logger.error(`\n   ‚ùå Failed to stop job ${job.id}:`, error.message);
          }
        }
      }
    }

    logger.info(`\n\n‚úÖ Jobs stopped!`);
    logger.info(`   - Successfully stopped: ${removed}`);
    logger.info(`   - Failed to stop: ${failed}`);

    // Get final queue stats
    const [finalWaiting, finalActive, finalDelayed] = await Promise.all([
      scraperQueue.getWaitingCount(),
      scraperQueue.getActiveCount(),
      scraperQueue.getDelayedCount(),
    ]);

    logger.info(`\nüìä Final Queue Status:`);
    logger.info(`   - Waiting: ${finalWaiting}`);
    logger.info(`   - Active: ${finalActive}`);
    logger.info(`   - Delayed: ${finalDelayed}`);

    await cleanup();
  });

/**
 * Cleanup queue - remove failed/completed jobs
 */
program
  .command('cleanup')
  .description('Clean up queue by removing old jobs')
  .option('--aggressive', 'Remove all completed and failed jobs')
  .option('--older-than <days>', 'Remove jobs older than N days', '7')
  .option('--zero-results', 'Remove waiting jobs for terms that previously returned zero results')
  .action(async (options: any) => {
    logger.info('üßπ Queue Cleanup\n');
    logger.info('='.repeat(60));

    let totalRemoved = 0;
    let totalFailed = 0;

    // Aggressive cleanup - remove all completed/failed
    if (options.aggressive) {
      logger.info('\n‚ö†Ô∏è  AGGRESSIVE MODE: Removing ALL completed and failed jobs...');

      const [completedCount, failedCount] = await Promise.all([
        scraperQueue.getCompletedCount(),
        scraperQueue.getFailedCount(),
      ]);

      logger.info(`   - Completed jobs: ${completedCount}`);
      logger.info(`   - Failed jobs: ${failedCount}`);
      logger.info(`   - Total to remove: ${completedCount + failedCount}`);

      if (completedCount + failedCount === 0) {
        logger.info('\n‚úÖ No jobs to clean up!');
        await cleanup();
        return;
      }

      logger.info('\nüöÄ Removing jobs...');
      const [removedCompleted, removedFailed] = await Promise.all([
        scraperQueue.clean(0, 'completed'),
        scraperQueue.clean(0, 'failed'),
      ]);

      totalRemoved = removedCompleted.length + removedFailed.length;
      logger.info(`\n‚úÖ Removed ${totalRemoved} jobs (${removedCompleted.length} completed, ${removedFailed.length} failed)`);
    }

    // Remove jobs older than N days
    if (!options.aggressive && options.olderThan) {
      const days = parseInt(options.olderThan);
      const timestamp = Date.now() - (days * 24 * 60 * 60 * 1000);

      logger.info(`\nüìÖ Removing jobs older than ${days} days...`);

      const [removedCompleted, removedFailed] = await Promise.all([
        scraperQueue.clean(timestamp, 'completed'),
        scraperQueue.clean(timestamp, 'failed'),
      ]);

      totalRemoved = removedCompleted.length + removedFailed.length;
      logger.info(`   ‚úÖ Removed ${totalRemoved} old jobs`);
    }

    // Zero results cleanup
    if (options.zeroResults) {
      logger.info('\nüîç Finding terms with zero results...');

      const emptyJobs = await prisma.scrapeJob.findMany({
        where: {
          status: 'completed',
          resultCount: 0
        },
        select: { searchTerm: true },
        distinct: ['searchTerm']
      });

      const emptyTerms = new Set(emptyJobs.map(j => j.searchTerm));
      logger.info(`   Found ${emptyTerms.size} terms that returned zero results`);

      const waitingJobs = await scraperQueue.getWaiting();
      const jobsToRemove = waitingJobs.filter(job =>
        emptyTerms.has(job.data.searchTerm)
      );

      logger.info(`   Found ${jobsToRemove.length} waiting jobs to remove`);

      if (jobsToRemove.length > 0) {
        logger.info('\nüóëÔ∏è  Removing zero-result jobs...');
        let removed = 0;

        for (const job of jobsToRemove) {
          try {
            await job.remove();
            removed++;
            if (removed % 20 === 0) {
              process.stdout.write(`\r   Progress: ${removed}/${jobsToRemove.length} (${((removed/jobsToRemove.length)*100).toFixed(1)}%)`);
            }
          } catch (error: any) {
            totalFailed++;
          }
        }

        logger.info(`\n   ‚úÖ Removed ${removed} zero-result jobs`);
        totalRemoved += removed;
      }
    }

    // Show final stats
    const [waiting, active, completed, failed] = await Promise.all([
      scraperQueue.getWaitingCount(),
      scraperQueue.getActiveCount(),
      scraperQueue.getCompletedCount(),
      scraperQueue.getFailedCount(),
    ]);

    logger.info(`\nüìä Final Queue Status:`);
    logger.info(`   - Waiting: ${waiting}`);
    logger.info(`   - Active: ${active}`);
    logger.info(`   - Completed: ${completed}`);
    logger.info(`   - Failed: ${failed}`);
    logger.info(`\n‚ú® Cleanup complete! Removed ${totalRemoved} jobs.`);

    await cleanup();
  });

/**
 * Show queue status
 */
program
  .command('status')
  .description('Show current queue status with job counts')
  .option('--detailed', 'Show detailed information about recent jobs')
  .action(async (options: any) => {
    logger.info('üìä Queue Status\n');
    logger.info('='.repeat(60));

    const [waiting, active, delayed, completed, failed, paused] = await Promise.all([
      scraperQueue.getWaitingCount(),
      scraperQueue.getActiveCount(),
      scraperQueue.getDelayedCount(),
      scraperQueue.getCompletedCount(),
      scraperQueue.getFailedCount(),
      scraperQueue.isPaused(),
    ]);

    logger.info(`\nüî¢ Job Counts:`);
    logger.info(`   - Waiting: ${waiting}`);
    logger.info(`   - Active: ${active}`);
    logger.info(`   - Delayed: ${delayed}`);
    logger.info(`   - Completed: ${completed}`);
    logger.info(`   - Failed: ${failed}`);
    logger.info(`   - Paused: ${paused ? 'Yes' : 'No'}`);
    logger.info(`   - Total: ${waiting + active + delayed + completed + failed}`);

    if (options.detailed) {
      logger.info(`\nüìù Recent Jobs:`);

      // Show recent active jobs
      if (active > 0) {
        const activeJobs = await scraperQueue.getActive(0, 5);
        logger.info(`\n   Active Jobs (${Math.min(5, active)}):`);
        activeJobs.forEach((job, idx) => {
          logger.info(`   ${idx + 1}. "${job.data.searchTerm}" (ID: ${job.id})`);
        });
      }

      // Show recent waiting jobs
      if (waiting > 0) {
        const waitingJobs = await scraperQueue.getWaiting(0, 5);
        logger.info(`\n   Waiting Jobs (showing ${Math.min(5, waiting)}):`);
        waitingJobs.forEach((job, idx) => {
          logger.info(`   ${idx + 1}. "${job.data.searchTerm}" (Priority: ${job.opts.priority || 10})`);
        });
      }

      // Show recent failed jobs
      if (failed > 0) {
        const failedJobs = await scraperQueue.getFailed(0, 3);
        logger.info(`\n   Recent Failed Jobs (showing ${Math.min(3, failed)}):`);
        failedJobs.forEach((job, idx) => {
          logger.info(`   ${idx + 1}. "${job.data.searchTerm}" - ${job.failedReason || 'Unknown error'}`);
        });
      }
    }

    logger.info('');
    await cleanup();
  });

/**
 * Pause/resume queue processing
 */
program
  .command('pause')
  .description('Pause queue processing')
  .action(async () => {
    logger.info('‚è∏Ô∏è  Pausing queue...');
    await scraperQueue.pause();
    logger.info('‚úÖ Queue paused. Jobs will not be processed until resumed.');
    await cleanup();
  });

program
  .command('resume')
  .description('Resume queue processing')
  .action(async () => {
    logger.info('‚ñ∂Ô∏è  Resuming queue...');
    await scraperQueue.resume();
    logger.info('‚úÖ Queue resumed. Processing will continue.');
    await cleanup();
  });

/**
 * Helper function to cleanup connections
 */
async function cleanup() {
  await scraperQueue.close();
  await prisma.$disconnect();
}

// Handle errors and cleanup
process.on('SIGINT', async () => {
  logger.info('\n\nüëã Interrupted. Cleaning up...');
  await cleanup();
  process.exit(0);
});

process.on('unhandledRejection', async (error: any) => {
  logger.error('\n‚ùå Unhandled error:', error.message);
  await cleanup();
  process.exit(1);
});

// Parse arguments
program.parse();
</file>

<file path="README_ENHANCED.md">
# cli

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "SoftwareSourceCode",
  "name": "cli",
  "description": "Directory containing 5 code files with 0 classes and 5 functions",
  "programmingLanguage": [
    {
      "@type": "ComputerLanguage",
      "name": "Typescript"
    }
  ],
  "featureList": [
    "5 function definitions"
  ]
}
</script>

## Overview

This directory contains 5 code file(s) with extracted schemas.

## Files and Schemas

### `data-cleaner.ts` (typescript)

**Functions:**
- `async cleanup()` - Line 426

### `db-stats-simple.ts` (typescript)

**Functions:**
- `async checkDatabaseStats()` - Line 5

### `db-stats.ts` (typescript)

**Functions:**
- `async cleanup()` - Line 447

### `queue-analyzer.ts` (typescript)

**Functions:**
- `async cleanup()` - Line 405

### `queue-manager.ts` (typescript)

**Functions:**
- `async cleanup()` - Line 452

---
*Generated by Enhanced Schema Generator with schema.org markup*
</file>

</files>
