This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
batch-scrape-100.ts
batch-scrape-comprehensive.ts
batch-scrape.ts
check-column-ids.ts
check-grove-job.ts
check-queue-status.ts
continuous-batch-scraper.ts
debug-token-refresh.ts
enqueue-commercial-batch.ts
enqueue-construction-batch.ts
enqueue-corporation-batch.ts
enqueue-foundation-batch.ts
enqueue-grove.ts
enqueue-high-priority.ts
enqueue-high-value-batch.ts
enqueue-investment-batch.ts
enqueue-llc-batch.ts
enqueue-partnership-batch.ts
enqueue-priority-terms.ts
enqueue-property-type-batch.ts
enqueue-residential-batch.ts
enqueue-test-batch-20.ts
enqueue-trust-batch.ts
enqueue-ultra-high-priority.ts
migrate-to-logger.ts
queue-entity-searches-fresh.ts
queue-entity-searches.ts
README_ENHANCED.md
test-api-token-config.ts
test-queue-job-flow.ts
test-single-job.ts
test-token-refresh.ts
worker.ts
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="batch-scrape-100.ts">
import { scraperQueue } from '../queues/scraper.queue';
import winston from 'winston';

const logger = winston.createLogger({
  level: 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.simple()
  ),
  transports: [
    new winston.transports.Console(),
  ],
});

// 100 diverse search terms for maximum property coverage
const SEARCH_TERMS = [
  // More Austin streets (30)
  'South Lamar', 'East Riverside', 'West Anderson', 'South Congress',
  'East 6th', 'West 6th', 'Manchaca', 'Mopac', 'Red River',
  'Rainey', 'Cesar Chavez', 'MLK', 'Dean Keeton', 'Speedway',
  'Duval', 'Shoal Creek', 'Koenig', 'Far West', 'Research Blvd',
  'South First', 'East 7th', 'West 12th', 'Barton Springs',
  'Westlake', 'Exposition', 'Windsor', 'Enfield', 'Balcones',
  'Spicewood', 'Capital of Texas',

  // Common last names (30)
  'Smith', 'Johnson', 'Williams', 'Jones', 'Brown',
  'Davis', 'Miller', 'Wilson', 'Moore', 'Taylor',
  'Anderson', 'Thomas', 'Jackson', 'White', 'Harris',
  'Martin', 'Thompson', 'Garcia', 'Martinez', 'Robinson',
  'Clark', 'Rodriguez', 'Lewis', 'Lee', 'Walker',
  'Hall', 'Allen', 'Young', 'King', 'Wright',

  // Austin neighborhoods (20)
  'Hyde Park', 'Tarrytown', 'Clarksville', 'Bouldin Creek',
  'South Austin', 'North Austin', 'East Austin', 'West Austin',
  'Rosedale', 'Crestview', 'Mueller', 'Domain', 'Downtown',
  'Zilker', 'Barton Hills', 'Travis Heights', 'Allandale',
  'Brentwood', 'Dawson', 'Cherrywood',

  // Business/building types (10)
  'Plaza', 'Center', 'Tower', 'Building', 'Office',
  'Apartments', 'Condos', 'Ranch', 'Estates', 'Village',

  // Additional streets (10)
  'Cameron', 'Metric', 'Dessau', 'Lamar Blvd', 'IH 35',
  'Loop 360', 'Wells Branch', 'McNeil', 'Howard', 'Jollyville',
];

async function queueBatch() {
  logger.info('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—');
  logger.info('â•‘   QUEUEING 100 NEW SCRAPING JOBS                       â•‘');
  logger.info('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');

  logger.info(`Total search terms: ${SEARCH_TERMS.length}\n`);

  let queued = 0;
  let failed = 0;

  for (const searchTerm of SEARCH_TERMS) {
    try {
      const job = await scraperQueue.add(
        'scrape-properties',
        {
          searchTerm,
          userId: 'batch-100',
          scheduled: true,
        },
        {
          attempts: 3,
          backoff: {
            type: 'exponential',
            delay: 2000,
          },
          removeOnComplete: 100,
          removeOnFail: 50,
        }
      );

      queued++;
      logger.info(`  âœ“ [${queued}/${SEARCH_TERMS.length}] ${searchTerm} (Job ${job.id})`);

      // Small delay to avoid overwhelming Redis
      await new Promise(resolve => setTimeout(resolve, 100));

    } catch (error) {
      failed++;
      logger.error({ err: error }, `  âœ— ${searchTerm}:`);
    }
  }

  logger.info(`\nâœ… Successfully queued ${queued} jobs`);
  if (failed > 0) {
    logger.warn(`âš ï¸  Failed to queue ${failed} jobs`);
  }

  // Get queue stats
  const [waiting, active] = await Promise.all([
    scraperQueue.getWaitingCount(),
    scraperQueue.getActiveCount(),
  ]);

  logger.info(`\nCurrent queue status:`);
  logger.info(`  â³ Waiting: ${waiting}`);
  logger.info(`  ğŸ”„ Active: ${active}`);
  logger.info(`\nEstimated completion time: ~${Math.ceil((waiting + active) * 30 / 60)} minutes`);

  process.exit(0);
}

queueBatch().catch((error) => {
  logger.error({ err: error }, 'Fatal error:');
  process.exit(1);
});
</file>

<file path="batch-scrape-comprehensive.ts">
import { scraperQueue } from '../queues/scraper.queue';
import { prisma } from '../lib/prisma';
import winston from 'winston';

const logger = winston.createLogger({
  level: 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.simple()
  ),
  transports: [
    new winston.transports.Console(),
  ],
});

// ALL Travis County ZIP codes
const ALL_ZIP_CODES = [
  '78701', '78702', '78703', '78704', '78705', '78712', '78719',
  '78721', '78722', '78723', '78724', '78725', '78726', '78727',
  '78728', '78729', '78730', '78731', '78732', '78733', '78734',
  '78735', '78736', '78737', '78738', '78739', '78741', '78742',
  '78744', '78745', '78746', '78747', '78748', '78749', '78750',
  '78751', '78752', '78753', '78754', '78756', '78757', '78758',
  '78759', '78760', '78761', '78762', '78763', '78764', '78765',
  '78766', '78767', '78768', '78769', '78772', '78773', '78774',
  '78778', '78779', '78780', '78781', '78783', '78799',
];

// Travis County cities
const CITIES = [
  'Austin',
  'Round Rock',
  'Pflugerville',
  'Cedar Park',
  'Leander',
  'Georgetown',
  'Manor',
  'Lakeway',
  'Bee Cave',
  'West Lake Hills',
  'Rollingwood',
  'Sunset Valley',
  'Jonestown',
  'Creedmoor',
  'Elgin',
  'Hutto',
  'San Marcos',
];

// Common last name patterns for broad coverage
const ALPHABET_PATTERNS = [
  'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',
  'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'
];

// Two-letter combinations for more coverage (common name prefixes)
const COMMON_NAME_PREFIXES = [
  'Mc', 'Mac', 'Van', 'De', 'La', 'Le', 'St', 'San', 'Del',
  'Al', 'El', 'Di', 'Da', 'Du', 'Mc', 'O\'',
];

// Common street names in Austin/Travis County
const COMMON_STREETS = [
  'Main', 'Oak', 'Park', 'Cedar', 'Elm', 'Lake', 'Hill', 'River',
  'Congress', 'Lamar', 'Guadalupe', 'Burnet', 'Airport', 'Oltorf',
  'Anderson', 'Bee Cave', 'Slaughter', 'William Cannon',
  'Research', 'Parmer', 'Braker', 'Rundberg', 'North Loop',
];

// Numeric patterns (property IDs often searchable)
const NUMERIC_PATTERNS = [
  '1', '2', '3', '4', '5', '6', '7', '8', '9', '0',
  '10', '100', '1000', '2000', '3000', '5000',
];

interface ComprehensiveBatchConfig {
  includeZipCodes: boolean;
  includeCities: boolean;
  includeAlphabet: boolean;
  includeNamePrefixes: boolean;
  includeStreets: boolean;
  includeNumeric: boolean;
  batchSize: number;
  delayBetweenBatches: number;
}

class ComprehensiveBatchScraper {
  private config: ComprehensiveBatchConfig;
  private stats = {
    totalQueued: 0,
    totalCompleted: 0,
    totalFailed: 0,
    startTime: Date.now(),
  };

  constructor(config: Partial<ComprehensiveBatchConfig> = {}) {
    this.config = {
      includeZipCodes: false, // ZIP codes don't work on TCAD website
      includeCities: true,
      includeAlphabet: true,
      includeNamePrefixes: true,
      includeStreets: true,
      includeNumeric: true,
      batchSize: 20,
      delayBetweenBatches: 3000,
      ...config,
    };
  }

  private getSearchTerms(): string[] {
    const terms: string[] = [];

    if (this.config.includeZipCodes) {
      terms.push(...ALL_ZIP_CODES);
      logger.info(`âœ“ Added ${ALL_ZIP_CODES.length} ZIP codes`);
    }

    if (this.config.includeCities) {
      terms.push(...CITIES);
      logger.info(`âœ“ Added ${CITIES.length} cities`);
    }

    if (this.config.includeAlphabet) {
      terms.push(...ALPHABET_PATTERNS);
      logger.info(`âœ“ Added ${ALPHABET_PATTERNS.length} alphabet patterns`);
    }

    if (this.config.includeNamePrefixes) {
      terms.push(...COMMON_NAME_PREFIXES);
      logger.info(`âœ“ Added ${COMMON_NAME_PREFIXES.length} name prefixes`);
    }

    if (this.config.includeStreets) {
      terms.push(...COMMON_STREETS);
      logger.info(`âœ“ Added ${COMMON_STREETS.length} street names`);
    }

    if (this.config.includeNumeric) {
      terms.push(...NUMERIC_PATTERNS);
      logger.info(`âœ“ Added ${NUMERIC_PATTERNS.length} numeric patterns`);
    }

    // Filter out search terms with less than 4 characters (TCAD minimum)
    const filteredTerms = terms.filter(term => term.length >= 4);
    const removedCount = terms.length - filteredTerms.length;
    if (removedCount > 0) {
      logger.info(`âœ— Filtered out ${removedCount} terms shorter than 4 characters`);
    }

    return filteredTerms;
  }

  private async delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  async queueJobs(): Promise<void> {
    const searchTerms = this.getSearchTerms();

    logger.info('\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—');
    logger.info('â•‘   COMPREHENSIVE TCAD DATABASE SCRAPING                 â•‘');
    logger.info('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');

    logger.info(`Total search terms: ${searchTerms.length}`);
    logger.info(`Batch size: ${this.config.batchSize}`);
    logger.info(`Delay between batches: ${this.config.delayBetweenBatches}ms\n`);

    // Process in batches
    for (let i = 0; i < searchTerms.length; i += this.config.batchSize) {
      const batch = searchTerms.slice(i, i + this.config.batchSize);
      const batchNum = Math.floor(i / this.config.batchSize) + 1;
      const totalBatches = Math.ceil(searchTerms.length / this.config.batchSize);

      logger.info(`\nğŸ“¦ Batch ${batchNum}/${totalBatches} (${batch.length} terms)`);

      for (const searchTerm of batch) {
        try {
          const job = await scraperQueue.add(
            'scrape-properties',
            {
              searchTerm,
              userId: 'comprehensive-batch',
              scheduled: true,
            },
            {
              attempts: 3,
              backoff: {
                type: 'exponential',
                delay: 2000,
              },
              removeOnComplete: 100,
              removeOnFail: 50,
            }
          );

          this.stats.totalQueued++;
          logger.info(`  âœ“ ${searchTerm} (Job ${job.id})`);
        } catch (error) {
          logger.error({ err: error }, `  âœ— ${searchTerm}:`);
        }
      }

      // Delay between batches
      if (i + this.config.batchSize < searchTerms.length) {
        const remaining = searchTerms.length - (i + this.config.batchSize);
        logger.info(`\nâ³ Waiting ${this.config.delayBetweenBatches}ms... (${remaining} terms remaining)`);
        await this.delay(this.config.delayBetweenBatches);
      }
    }

    logger.info(`\nâœ… All ${this.stats.totalQueued} jobs queued successfully!`);
  }

  async monitorProgress(): Promise<void> {
    logger.info('\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—');
    logger.info('â•‘          MONITORING PROGRESS                           â•‘');
    logger.info('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');

    const checkInterval = 30000; // Check every 30 seconds

    while (true) {
      await this.delay(checkInterval);

      // Get queue stats
      const [waiting, active, completed, failed] = await Promise.all([
        scraperQueue.getWaitingCount(),
        scraperQueue.getActiveCount(),
        scraperQueue.getCompletedCount(),
        scraperQueue.getFailedCount(),
      ]);

      const elapsed = Math.floor((Date.now() - this.stats.startTime) / 1000);
      const minutes = Math.floor(elapsed / 60);
      const seconds = elapsed % 60;

      // Get database stats
      const totalProperties = await prisma.property.count();
      const recentProperties = await prisma.property.count({
        where: {
          scrapedAt: {
            gte: new Date(Date.now() - 300000), // Last 5 minutes
          },
        },
      });

      const progress = ((completed + failed) / this.stats.totalQueued) * 100;

      logger.info(`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Time: ${minutes}m ${seconds}s | Progress: ${progress.toFixed(1)}%
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Queue Status:
â”‚   â³ Waiting:   ${waiting.toString().padStart(6)}
â”‚   ğŸ”„ Active:    ${active.toString().padStart(6)}
â”‚   âœ… Completed: ${completed.toString().padStart(6)}
â”‚   âŒ Failed:    ${failed.toString().padStart(6)}
â”‚   ğŸ“Š Total:     ${this.stats.totalQueued.toString().padStart(6)}
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Database:
â”‚   ğŸ“ Total Properties:  ${totalProperties.toString().padStart(8)}
â”‚   ğŸ†• Last 5 min:        ${recentProperties.toString().padStart(8)}
â”‚   âš¡ Rate: ${(recentProperties / 5).toFixed(1)} properties/min
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      `);

      // Check if all jobs are done
      if (waiting === 0 && active === 0 && completed + failed >= this.stats.totalQueued) {
        logger.info('\nâœ… All jobs completed!');
        break;
      }
    }

    await this.printFinalReport();
  }

  async printFinalReport(): Promise<void> {
    const [totalProperties, uniqueCount, totalJobs, successfulJobs] = await Promise.all([
      prisma.property.count(),
      prisma.property.findMany({ select: { propertyId: true }, distinct: ['propertyId'] }),
      prisma.scrapeJob.count(),
      prisma.scrapeJob.count({ where: { status: 'completed' } }),
    ]);

    const elapsed = Math.floor((Date.now() - this.stats.startTime) / 1000);
    const hours = Math.floor(elapsed / 3600);
    const minutes = Math.floor((elapsed % 3600) / 60);
    const seconds = elapsed % 60;

    const completed = await scraperQueue.getCompletedCount();
    const failed = await scraperQueue.getFailedCount();

    logger.info(`
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         COMPREHENSIVE SCRAPING FINAL REPORT               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â±ï¸  Time Elapsed: ${hours}h ${minutes}m ${seconds}s

ğŸ“Š Jobs:
   â€¢ Total Queued:    ${this.stats.totalQueued}
   â€¢ Completed:       ${completed}
   â€¢ Failed:          ${failed}
   â€¢ Success Rate:    ${((completed / this.stats.totalQueued) * 100).toFixed(2)}%

ğŸ’¾ Database:
   â€¢ Total Properties:     ${totalProperties}
   â€¢ Unique Properties:    ${uniqueCount.length}
   â€¢ Duplicate Entries:    ${totalProperties - uniqueCount.length}
   â€¢ Total Scrape Jobs:    ${totalJobs}
   â€¢ Successful Jobs:      ${successfulJobs}
   â€¢ Overall Success Rate: ${((successfulJobs / totalJobs) * 100).toFixed(2)}%

âš¡ Performance:
   â€¢ Avg Time per Job:      ${(elapsed / completed).toFixed(2)}s
   â€¢ Properties per Hour:   ${((totalProperties / elapsed) * 3600).toFixed(0)}
   â€¢ Jobs per Hour:         ${((completed / elapsed) * 3600).toFixed(0)}

Coverage Strategy:
   âœ“ All ZIP codes (${ALL_ZIP_CODES.length})
   âœ“ Cities (${CITIES.length})
   âœ“ Alphabet patterns (${ALPHABET_PATTERNS.length})
   âœ“ Name prefixes (${COMMON_NAME_PREFIXES.length})
   âœ“ Street names (${COMMON_STREETS.length})
   âœ“ Numeric patterns (${NUMERIC_PATTERNS.length})

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ‰ COMPREHENSIVE SCRAPING COMPLETED SUCCESSFULLY! ğŸ‰     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    `);
  }

  async run(): Promise<void> {
    try {
      await this.queueJobs();
      await this.monitorProgress();
      process.exit(0);
    } catch (error) {
      logger.error({ err: error }, 'âŒ Fatal error:');
      process.exit(1);
    }
  }
}

// Run the scraper
const scraper = new ComprehensiveBatchScraper({
  includeZipCodes: false, // ZIP codes don't work on TCAD website
  includeCities: true,
  includeAlphabet: false, // Single letters likely won't return meaningful results
  includeNamePrefixes: false, // Short name prefixes filtered out by 4-char minimum
  includeStreets: true, // Streets work well!
  includeNumeric: false, // Numbers likely won't return useful results
  batchSize: 10, // Reduce batch size to avoid overwhelming the server
  delayBetweenBatches: 3000, // Increase delay to be more respectful
});

scraper.run();
</file>

<file path="batch-scrape.ts">
import { scraperQueue } from '../queues/scraper.queue';
import { prisma } from '../lib/prisma';
import winston from 'winston';

const logger = winston.createLogger({
  level: 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.json()
  ),
  transports: [
    new winston.transports.Console({
      format: winston.format.simple(),
    }),
  ],
});

// Travis County cities and major areas
const CITIES = [
  'Austin',
  'Round Rock',
  'Pflugerville',
  'Cedar Park',
  'Leander',
  'Georgetown',
  'Manor',
  'Lakeway',
  'Bee Cave',
  'West Lake Hills',
  'Rollingwood',
  'Sunset Valley',
  'Jonestown',
  'Creedmoor',
  'Elgin',
  'Hutto',
  'San Marcos',
];

// Travis County ZIP codes (major ones)
const ZIP_CODES = [
  '78701', '78702', '78703', '78704', '78705', '78712', '78719',
  '78721', '78722', '78723', '78724', '78725', '78726', '78727',
  '78728', '78729', '78730', '78731', '78732', '78733', '78734',
  '78735', '78736', '78737', '78738', '78739', '78741', '78742',
  '78744', '78745', '78746', '78747', '78748', '78749', '78750',
  '78751', '78752', '78753', '78754', '78756', '78757', '78758',
  '78759', '78760', '78761', '78762', '78763', '78764', '78765',
  '78766', '78767', '78768', '78769', '78772', '78773', '78774',
  '78778', '78779', '78780', '78781', '78783', '78799',
];

// Property types commonly found in TCAD
const PROPERTY_TYPES = [
  'A', // Single Family Residential
  'B', // Multi-Family Residential
  'C', // Vacant Lots/Land
  'D', // Rural Real (Land)
  'E', // Farm & Ranch
  'F', // Commercial Real
  'G', // Oil, Gas, Minerals
  'H', // Industrial Real
  'J', // Water Systems
  'L', // Miscellaneous
  'M', // Mobile Homes
  'N', // Intangible Personal Property
  'O', // Residential Inventory
  'P', // Non-Residential Inventory
  'S', // Special Inventory
  'X', // Totally Exempt Property
];

// Street name prefixes for comprehensive coverage
const STREET_PREFIXES = [
  'North', 'South', 'East', 'West',
  'N', 'S', 'E', 'W',
];

// Common street suffixes
const STREET_SUFFIXES = [
  'Street', 'St', 'Avenue', 'Ave', 'Road', 'Rd', 'Drive', 'Dr',
  'Lane', 'Ln', 'Court', 'Ct', 'Circle', 'Cir', 'Boulevard', 'Blvd',
  'Way', 'Trail', 'Path', 'Place', 'Pl',
];

interface BatchConfig {
  batchSize: number;
  delayBetweenBatches: number; // milliseconds
  maxConcurrentJobs: number;
  searchStrategy: 'cities' | 'zipcodes' | 'types' | 'comprehensive' | 'custom';
  customSearchTerms?: string[];
}

class BatchScraper {
  private config: BatchConfig;
  private jobIds: string[] = [];
  private stats = {
    totalQueued: 0,
    totalCompleted: 0,
    totalFailed: 0,
    startTime: Date.now(),
  };

  constructor(config: Partial<BatchConfig> = {}) {
    this.config = {
      batchSize: 10,
      delayBetweenBatches: 5000,
      maxConcurrentJobs: 3,
      searchStrategy: 'comprehensive',
      ...config,
    };
  }

  private getSearchTerms(): string[] {
    let terms: string[];
    switch (this.config.searchStrategy) {
      case 'cities':
        terms = CITIES;
        break;
      case 'zipcodes':
        terms = ZIP_CODES;
        break;
      case 'types':
        terms = PROPERTY_TYPES;
        break;
      case 'custom':
        terms = this.config.customSearchTerms || [];
        break;
      case 'comprehensive':
      default:
        // Combine multiple strategies for maximum coverage
        terms = [
          ...CITIES,
          ...ZIP_CODES.slice(0, 20), // Use first 20 ZIP codes
          ...PROPERTY_TYPES,
        ];
    }

    // Filter out search terms with less than 4 characters (TCAD minimum)
    return terms.filter(term => term.length >= 4);
  }

  private async delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  async queueJobs(): Promise<void> {
    const searchTerms = this.getSearchTerms();
    logger.info(`Starting batch scrape with ${searchTerms.length} search terms using strategy: ${this.config.searchStrategy}`);
    logger.info(`Batch size: ${this.config.batchSize}, Delay: ${this.config.delayBetweenBatches}ms`);

    // Process in batches
    for (let i = 0; i < searchTerms.length; i += this.config.batchSize) {
      const batch = searchTerms.slice(i, i + this.config.batchSize);
      logger.info(`\nQueuing batch ${Math.floor(i / this.config.batchSize) + 1}/${Math.ceil(searchTerms.length / this.config.batchSize)}`);

      for (const searchTerm of batch) {
        try {
          const job = await scraperQueue.add(
            'scrape-properties',
            {
              searchTerm,
              userId: 'batch-scraper',
              scheduled: true,
            },
            {
              attempts: 3,
              backoff: {
                type: 'exponential',
                delay: 2000,
              },
              removeOnComplete: 100,
              removeOnFail: 50,
            }
          );

          this.jobIds.push(job.id.toString());
          this.stats.totalQueued++;
          logger.info(`  âœ“ Queued: ${searchTerm} (Job ID: ${job.id})`);
        } catch (error) {
          logger.error({ err: error }, `  âœ— Failed to queue: ${searchTerm}`);
        }
      }

      // Delay between batches to avoid overwhelming the system
      if (i + this.config.batchSize < searchTerms.length) {
        logger.info(`Waiting ${this.config.delayBetweenBatches}ms before next batch...`);
        await this.delay(this.config.delayBetweenBatches);
      }
    }

    logger.info(`\nâœ“ All jobs queued! Total: ${this.stats.totalQueued}`);
  }

  async monitorProgress(): Promise<void> {
    logger.info('\n=== Monitoring Job Progress ===\n');

    const checkInterval = 10000; // Check every 10 seconds
    let lastCheck = Date.now();

    while (this.stats.totalCompleted + this.stats.totalFailed < this.stats.totalQueued) {
      await this.delay(checkInterval);

      // Get job counts from queue
      const [waiting, active, completed, failed] = await Promise.all([
        scraperQueue.getWaitingCount(),
        scraperQueue.getActiveCount(),
        scraperQueue.getCompletedCount(),
        scraperQueue.getFailedCount(),
      ]);

      const elapsed = Math.floor((Date.now() - this.stats.startTime) / 1000);
      const minutes = Math.floor(elapsed / 60);
      const seconds = elapsed % 60;

      logger.info(`
Status Update (${minutes}m ${seconds}s elapsed):
  Waiting: ${waiting}
  Active: ${active}
  Completed: ${completed}
  Failed: ${failed}
  Total: ${this.stats.totalQueued}
      `);

      // Check database stats
      const dbStats = await this.getDatabaseStats();
      logger.info(`
Database Stats:
  Total Properties: ${dbStats.totalProperties}
  Unique Properties: ${dbStats.uniqueProperties}
  Scrape Jobs: ${dbStats.totalJobs}
  Success Rate: ${dbStats.successRate}%
      `);

      // Update our stats
      this.stats.totalCompleted = completed;
      this.stats.totalFailed = failed;

      if (waiting === 0 && active === 0) {
        logger.info('\nâœ“ All jobs completed!');
        break;
      }
    }

    await this.printFinalReport();
  }

  async getDatabaseStats() {
    const [totalProperties, uniqueCount, totalJobs, successfulJobs] = await Promise.all([
      prisma.property.count(),
      prisma.property.findMany({ select: { propertyId: true }, distinct: ['propertyId'] }),
      prisma.scrapeJob.count(),
      prisma.scrapeJob.count({ where: { status: 'completed' } }),
    ]);

    return {
      totalProperties,
      uniqueProperties: uniqueCount.length,
      totalJobs,
      successfulJobs,
      successRate: totalJobs > 0 ? ((successfulJobs / totalJobs) * 100).toFixed(2) : '0',
    };
  }

  async printFinalReport(): Promise<void> {
    const dbStats = await this.getDatabaseStats();
    const elapsed = Math.floor((Date.now() - this.stats.startTime) / 1000);
    const minutes = Math.floor(elapsed / 60);
    const seconds = elapsed % 60;

    logger.info(`
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           BATCH SCRAPING FINAL REPORT                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Time Elapsed: ${minutes} minutes ${seconds} seconds

Jobs:
  â€¢ Total Queued: ${this.stats.totalQueued}
  â€¢ Completed: ${this.stats.totalCompleted}
  â€¢ Failed: ${this.stats.totalFailed}
  â€¢ Success Rate: ${((this.stats.totalCompleted / this.stats.totalQueued) * 100).toFixed(2)}%

Database:
  â€¢ Total Properties: ${dbStats.totalProperties}
  â€¢ Unique Properties: ${dbStats.uniqueProperties}
  â€¢ Deduplication Rate: ${(((dbStats.totalProperties - dbStats.uniqueProperties) / dbStats.totalProperties) * 100).toFixed(2)}%
  â€¢ Total Scrape Jobs: ${dbStats.totalJobs}
  â€¢ Overall Success Rate: ${dbStats.successRate}%

Performance:
  â€¢ Avg Time per Job: ${(elapsed / this.stats.totalCompleted).toFixed(2)}s
  â€¢ Properties per Minute: ${((dbStats.totalProperties / elapsed) * 60).toFixed(2)}

Strategy Used: ${this.config.searchStrategy}
    `);
  }

  async run(): Promise<void> {
    try {
      logger.info('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—');
      logger.info('â•‘        TCAD BATCH SCRAPER - Starting...                â•‘');
      logger.info('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');

      // Queue all jobs
      await this.queueJobs();

      // Monitor progress
      await this.monitorProgress();

      logger.info('\nâœ“ Batch scraping completed successfully!');
      process.exit(0);
    } catch (error) {
      logger.error({ err: error }, 'Fatal error during batch scraping:');
      process.exit(1);
    }
  }
}

// CLI interface
const args = process.argv.slice(2);
const strategy = (args[0] as BatchConfig['searchStrategy']) || 'comprehensive';
const batchSize = parseInt(args[1]) || 10;
const delay = parseInt(args[2]) || 5000;

const scraper = new BatchScraper({
  searchStrategy: strategy,
  batchSize,
  delayBetweenBatches: delay,
  maxConcurrentJobs: 3,
});

scraper.run();
</file>

<file path="check-column-ids.ts">
import { chromium } from 'playwright';
import logger from '../lib/logger';

async function checkColumnIds() {
  logger.info('ğŸ” Checking actual column IDs...\n');

  const browser = await chromium.launch({
    headless: true,
    args: ['--no-sandbox', '--disable-setuid-sandbox'],
  });

  const context = await browser.newContext({
    userAgent: 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
    viewport: { width: 1920, height: 1080 },
  });

  const page = await context.newPage();

  try {
    await page.goto('https://travis.prodigycad.com/property-search', {
      waitUntil: 'networkidle',
      timeout: 30000,
    });

    await page.waitForFunction(() => {
      const root = document.getElementById('root');
      return root && root.children.length > 0;
    }, { timeout: 15000 });

    await page.waitForSelector('#searchInput', { timeout: 10000 });
    await page.type('#searchInput', 'dede', { delay: 100 });
    await page.waitForTimeout(500);
    await page.press('#searchInput', 'Enter');
    await page.waitForTimeout(7000);

    const columnInfo = await page.evaluate(() => {
      const firstRow = document.querySelector('.ag-row');
      if (!firstRow) return { error: 'No rows found' };

      const cells = firstRow.querySelectorAll('[role="gridcell"]');
      const cellsInfo = Array.from(cells).map(cell => ({
        colId: cell.getAttribute('col-id'),
        text: cell.textContent?.trim(),
        ariaColIndex: cell.getAttribute('aria-colindex'),
      }));

      return { cellsInfo };
    });

    logger.info('ğŸ“Š All Column IDs from first row:\n');
    if ('error' in columnInfo) {
      logger.error(columnInfo.error);
    } else {
      columnInfo.cellsInfo.forEach((cell, i) => {
        logger.info(`Column ${i + 1}:`);
        logger.info(`  col-id: ${cell.colId}`);
        logger.info(`  text: ${cell.text || '(empty)'}`);
        logger.info(`  aria-colindex: ${cell.ariaColIndex}\n`);
      });
    }

  } catch (error: any) {
    logger.error(`âŒ Error: ${error.message}`);
  } finally {
    await context.close();
    await browser.close();
  }
}

checkColumnIds();
</file>

<file path="check-grove-job.ts">
import { scraperQueue } from '../queues/scraper.queue';

async function checkGroveJob() {
  const waiting = await scraperQueue.getWaiting();
  const active = await scraperQueue.getActive();
  const completed = await scraperQueue.getCompleted();

  console.log('Queue Status:');
  console.log('- Waiting jobs:', waiting.length);
  console.log('- Active jobs:', active.length);
  console.log('- Completed jobs (recent):', completed.length);

  // Find Grove job
  const groveJobs = [...waiting, ...active, ...completed].filter((job: any) =>
    job.data?.searchTerm === 'Grove'
  );

  console.log('\nGrove job(s):', groveJobs.length);
  if (groveJobs.length > 0) {
    const job = groveJobs[0];
    const state = await job.getState();
    console.log('Grove job details:', JSON.stringify({
      id: job.id,
      state,
      data: job.data,
      finishedOn: job.finishedOn,
      processedOn: job.processedOn,
      returnvalue: job.returnvalue
    }, null, 2));
  } else {
    console.log('No Grove job found in queue');
  }

  process.exit(0);
}

checkGroveJob().catch((err) => {
  console.error('Error:', err);
  process.exit(1);
});
</file>

<file path="check-queue-status.ts">
#!/usr/bin/env node
/**
 * Check Queue Status
 * Displays current status of all jobs in the scraper queue
 */

import { scraperQueue } from '../queues/scraper.queue';
import logger from '../lib/logger';

async function checkQueueStatus() {
  try {
    logger.info('ğŸ“Š Checking Scraper Queue Status...\n');

    // Get job counts by status
    const [
      waiting,
      active,
      completed,
      failed,
      delayed,
      isPaused
    ] = await Promise.all([
      scraperQueue.getWaiting(),
      scraperQueue.getActive(),
      scraperQueue.getCompleted(),
      scraperQueue.getFailed(),
      scraperQueue.getDelayed(),
      scraperQueue.isPaused()
    ]);

    // Get counts
    const counts = {
      waiting: waiting.length,
      active: active.length,
      completed: completed.length,
      failed: failed.length,
      delayed: delayed.length,
      paused: isPaused ? 1 : 0
    };

    logger.info('='.repeat(60));
    logger.info('ğŸ“ˆ QUEUE SUMMARY');
    logger.info('='.repeat(60));
    logger.info(`â³ Waiting:   ${counts.waiting}`);
    logger.info(`âš¡ Active:    ${counts.active}`);
    logger.info(`âœ… Completed: ${counts.completed}`);
    logger.info(`âŒ Failed:    ${counts.failed}`);
    logger.info(`â¸ï¸  Delayed:   ${counts.delayed}`);
    logger.info(`â¸ï¸  Paused:    ${counts.paused}`);
    logger.info('='.repeat(60));
    logger.info(`ğŸ“Š Total:     ${counts.waiting + counts.active + counts.completed + counts.failed + counts.delayed + counts.paused}`);
    logger.info('='.repeat(60));

    // Show active jobs
    if (active.length > 0) {
      logger.info('\nâš¡ ACTIVE JOBS:');
      for (const job of active.slice(0, 5)) {
        const data = job.data as any;
        logger.info(`  Job ${job.id}: "${data.searchTerm}" (Progress: ${job.progress}%)`);
      }
      if (active.length > 5) {
        logger.info(`  ... and ${active.length - 5} more`);
      }
    }

    // Show recent completed jobs
    if (completed.length > 0) {
      logger.info('\nâœ… RECENT COMPLETED JOBS (last 10):');
      for (const job of completed.slice(-10).reverse()) {
        const data = job.data as any;
        const returnValue = job.returnvalue as any;
        const propertiesCount = returnValue?.propertiesCount || 0;
        logger.info(`  Job ${job.id}: "${data.searchTerm}" â†’ ${propertiesCount} properties`);
      }
    }

    // Show recent failed jobs
    if (failed.length > 0) {
      logger.info('\nâŒ RECENT FAILED JOBS (last 5):');
      for (const job of failed.slice(-5).reverse()) {
        const data = job.data as any;
        const failedReason = job.failedReason || 'Unknown error';
        logger.info(`  Job ${job.id}: "${data.searchTerm}" - ${failedReason.substring(0, 80)}`);
      }
    }

    // Show next waiting jobs
    if (waiting.length > 0) {
      logger.info('\nâ³ NEXT WAITING JOBS (first 10):');
      for (const job of waiting.slice(0, 10)) {
        const data = job.data as any;
        const priority = job.opts.priority || 3;
        logger.info(`  Job ${job.id}: "${data.searchTerm}" (Priority: ${priority})`);
      }
      if (waiting.length > 10) {
        logger.info(`  ... and ${waiting.length - 10} more`);
      }
    }

    logger.info('');

    // Cleanup
    await scraperQueue.close();
    process.exit(0);
  } catch (error) {
    logger.error({ err: error }, 'âŒ Error checking queue status:');
    process.exit(1);
  }
}

checkQueueStatus();
</file>

<file path="continuous-batch-scraper.ts">
import { scraperQueue } from '../queues/scraper.queue';
import { prisma } from '../lib/prisma';
import winston from 'winston';
import { SearchTermDeduplicator } from '../lib/search-term-deduplicator';
import { searchTermOptimizer, SearchTermOptimizer } from '../services/search-term-optimizer';

const logger = winston.createLogger({
  level: 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.simple()
  ),
  transports: [
    new winston.transports.Console(),
    new winston.transports.File({ filename: 'logs/continuous-scraper.log' }),
  ],
});

const TARGET_PROPERTIES = 451339;
// Using API-based scraping (1000+ results per search). Optimized for high-yield search terms.
// Batch size reduced to enable more frequent optimizations (every 50 jobs = ~2-3 batches)
const BATCH_SIZE = 25; // Reduced from 75 to 25 for faster optimization cycles
const DELAY_BETWEEN_BATCHES = 30000; // 30 seconds
const CHECK_INTERVAL = 60000; // Check every minute

// Generate diverse search patterns
class SearchPatternGenerator {
  private usedTerms = new Set<string>();
  private deduplicator!: SearchTermDeduplicator; // Initialized in loadUsedTerms
  private dbTermsLoaded = false;
  private lastDbRefresh = 0;
  private readonly DB_REFRESH_INTERVAL = 60 * 60 * 1000; // Refresh every hour
  private optimizer: SearchTermOptimizer;
  private jobsProcessedSinceLastOptimization = 0;
  private readonly OPTIMIZATION_INTERVAL = 50; // Optimize after every 50 jobs

  // Common first names (top 200)
  private firstNames = [
    'James', 'Mary', 'John', 'Patricia', 'Robert', 'Jennifer', 'Michael', 'Linda',
    'William', 'Elizabeth', 'David', 'Barbara', 'Richard', 'Susan', 'Joseph', 'Jessica',
    'Thomas', 'Sarah', 'Charles', 'Karen', 'Christopher', 'Nancy', 'Daniel', 'Lisa',
    'Matthew', 'Betty', 'Anthony', 'Margaret', 'Mark', 'Sandra', 'Donald', 'Ashley',
    'Steven', 'Kimberly', 'Paul', 'Emily', 'Andrew', 'Donna', 'Joshua', 'Michelle',
    'Kenneth', 'Dorothy', 'Kevin', 'Carol', 'Brian', 'Amanda', 'George', 'Melissa',
    'Edward', 'Deborah', 'Ronald', 'Stephanie', 'Timothy', 'Rebecca', 'Jason', 'Sharon',
    'Jeffrey', 'Laura', 'Ryan', 'Cynthia', 'Jacob', 'Kathleen', 'Gary', 'Amy',
    'Nicholas', 'Shirley', 'Eric', 'Angela', 'Jonathan', 'Helen', 'Stephen', 'Anna',
    'Larry', 'Brenda', 'Justin', 'Pamela', 'Scott', 'Nicole', 'Brandon', 'Emma',
    'Benjamin', 'Samantha', 'Samuel', 'Katherine', 'Raymond', 'Christine', 'Gregory', 'Debra',
    'Frank', 'Rachel', 'Alexander', 'Catherine', 'Patrick', 'Carolyn', 'Raymond', 'Janet',
    'Jack', 'Ruth', 'Dennis', 'Maria', 'Jerry', 'Heather', 'Tyler', 'Diane',
    'Aaron', 'Virginia', 'Jose', 'Julie', 'Adam', 'Joyce', 'Henry', 'Victoria',
    'Nathan', 'Olivia', 'Douglas', 'Kelly', 'Zachary', 'Christina', 'Peter', 'Lauren',
    'Kyle', 'Joan', 'Walter', 'Evelyn', 'Ethan', 'Judith', 'Jeremy', 'Megan',
    'Harold', 'Cheryl', 'Keith', 'Andrea', 'Christian', 'Hannah', 'Roger', 'Jacqueline',
    'Noah', 'Martha', 'Gerald', 'Gloria', 'Carl', 'Teresa', 'Terry', 'Ann',
    'Sean', 'Sara', 'Austin', 'Madison', 'Arthur', 'Frances', 'Lawrence', 'Kathryn',
    'Jesse', 'Janice', 'Dylan', 'Jean', 'Bryan', 'Abigail', 'Joe', 'Sophia',
    'Jordan', 'Judy', 'Billy', 'Theresa', 'Bruce', 'Rose', 'Albert', 'Beverly',
    'Willie', 'Denise', 'Gabriel', 'Marilyn', 'Logan', 'Amber', 'Alan', 'Danielle',
    'Juan', 'Brittany', 'Wayne', 'Diana', 'Roy', 'Natalie', 'Ralph', 'Sophia',
    'Randy', 'Alexis', 'Eugene', 'Lori', 'Vincent', 'Kayla', 'Russell', 'Jane',
    'Louis', 'Grace', 'Philip', 'Judy', 'Bobby', 'Alice', 'Johnny', 'Julia',
  ];

  // Common last names (expanded to 500+)
  private lastNames = [
    'Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', 'Miller', 'Davis',
    'Rodriguez', 'Martinez', 'Hernandez', 'Lopez', 'Gonzalez', 'Wilson', 'Anderson', 'Thomas',
    'Taylor', 'Moore', 'Jackson', 'Martin', 'Lee', 'Perez', 'Thompson', 'White',
    'Harris', 'Sanchez', 'Clark', 'Ramirez', 'Lewis', 'Robinson', 'Walker', 'Young',
    'Allen', 'King', 'Wright', 'Scott', 'Torres', 'Nguyen', 'Hill', 'Flores',
    'Green', 'Adams', 'Nelson', 'Baker', 'Hall', 'Rivera', 'Campbell', 'Mitchell',
    'Carter', 'Roberts', 'Gomez', 'Phillips', 'Evans', 'Turner', 'Diaz', 'Parker',
    'Cruz', 'Edwards', 'Collins', 'Reyes', 'Stewart', 'Morris', 'Morales', 'Murphy',
    'Cook', 'Rogers', 'Gutierrez', 'Ortiz', 'Morgan', 'Cooper', 'Peterson', 'Bailey',
    'Reed', 'Kelly', 'Howard', 'Ramos', 'Kim', 'Cox', 'Ward', 'Richardson',
    'Watson', 'Brooks', 'Chavez', 'Wood', 'James', 'Bennett', 'Gray', 'Mendoza',
    'Ruiz', 'Hughes', 'Price', 'Alvarez', 'Castillo', 'Sanders', 'Patel', 'Myers',
    'Long', 'Ross', 'Foster', 'Jimenez', 'Powell', 'Jenkins', 'Perry', 'Russell',
    'Sullivan', 'Bell', 'Coleman', 'Butler', 'Henderson', 'Barnes', 'Gonzales', 'Fisher',
    'Vasquez', 'Simmons', 'Romero', 'Jordan', 'Patterson', 'Alexander', 'Hamilton', 'Graham',
    'Reynolds', 'Griffin', 'Wallace', 'Moreno', 'West', 'Cole', 'Hayes', 'Bryant',
    'Herrera', 'Gibson', 'Ellis', 'Tran', 'Medina', 'Aguilar', 'Stevens', 'Murray',
    'Ford', 'Castro', 'Marshall', 'Owens', 'Harrison', 'Fernandez', 'McDonald', 'Woods',
    'Washington', 'Kennedy', 'Wells', 'Vargas', 'Henry', 'Chen', 'Freeman', 'Webb',
    'Tucker', 'Guzman', 'Burns', 'Crawford', 'Olson', 'Simpson', 'Porter', 'Hunter',
    'Gordon', 'Mendez', 'Silva', 'Shaw', 'Snyder', 'Mason', 'Dixon', 'Munoz',
    'Hunt', 'Hicks', 'Holmes', 'Palmer', 'Wagner', 'Black', 'Robertson', 'Boyd',
    'Rose', 'Stone', 'Salazar', 'Fox', 'Warren', 'Mills', 'Meyer', 'Rice',
    'Schmidt', 'Garza', 'Daniels', 'Ferguson', 'Nichols', 'Stephens', 'Soto', 'Weaver',
    'Ryan', 'Gardner', 'Payne', 'Grant', 'Dunn', 'Kelley', 'Spencer', 'Hawkins',
    // Additional 300+ names
    'Lawson', 'Pierce', 'Hart', 'Elliott', 'Cunningham', 'Knight', 'Bradley', 'Carroll',
    'Hudson', 'Duncan', 'Armstrong', 'Berry', 'Andrews', 'Johnston', 'Ray', 'Lane',
    'Riley', 'Carpenter', 'Perkins', 'Williamson', 'Hanson', 'Austin', 'Newman', 'Oliver',
    'Howell', 'Dean', 'Wells', 'Fleming', 'French', 'Cannon', 'Barker', 'Watts',
    'McCoy', 'McLaughlin', 'Caldwell', 'Chandler', 'Lambert', 'Norton', 'Blake', 'Maxwell',
    'Carr', 'Walsh', 'Little', 'Park', 'Hodges', 'Haynes', 'Burgess', 'Benson',
    'Bishop', 'Todd', 'Norris', 'Fuller', 'Barber', 'Lamb', 'Parsons', 'Sutton',
    'Welch', 'Paul', 'Schwartz', 'Newman', 'Manning', 'Goodman', 'Watkins', 'Lyons',
    'Dawson', 'Powers', 'Figueroa', 'Nash', 'McKenzie', 'Booth', 'Shelton', 'Moran',
    'Rojas', 'Frank', 'Conner', 'Brock', 'Hogan', 'Brady', 'McCormick', 'Parks',
    'Floyd', 'Steele', 'Townsend', 'Valdez', 'Dennis', 'Hale', 'Delgado', 'Sutherland',
    'Buchanan', 'Marsh', 'Cummings', 'Patton', 'Rowe', 'Hampton', 'Lang', 'Gross',
    'Garner', 'Vincent', 'Doyle', 'Ramsey', 'Thornton', 'Wolfe', 'Glass', 'McCarthy',
    'Bowman', 'Luna', 'Norman', 'Pearson', 'Floyd', 'Mullins', 'Gregory', 'Schwartz',
    'Singleton', 'Wilkins', 'Schneider', 'Bowen', 'Hoffman', 'Logan', 'Cross', 'Moss',
    'Richards', 'Harmon', 'Brady', 'Rodgers', 'Duran', 'Hubbard', 'Bates', 'Reeves',
    'Klein', 'Frazier', 'Gibbs', 'Craig', 'Cochran', 'Chase', 'Moss', 'McKinney',
    'Bauer', 'Robbins', 'Curry', 'Sawyer', 'Powers', 'Jensen', 'Walters', 'Huff',
    'Aguilar', 'Glover', 'Browning', 'Carson', 'Mack', 'Clayton', 'Fritz', 'Hansen',
    'Schultz', 'Rich', 'Webster', 'Malone', 'Hammond', 'Flowers', 'Cobb', 'Moody',
    'Quinn', 'Randall', 'Brewer', 'Hutchinson', 'Holden', 'Wiley', 'Rowland', 'Mejia',
    'Sweeney', 'Dale', 'Frederick', 'Dalton', 'Logan', 'Sellers', 'Monroe', 'Hickman',
    'Gill', 'Cannon', 'Savage', 'Ballard', 'Joseph', 'Crosby', 'Drake', 'Vaughn',
    'Walls', 'Bolton', 'Chan', 'Stokes', 'Bentley', 'Skinner', 'Woodward', 'Brennan',
    'Hayden', 'Hancock', 'Huang', 'Pearce', 'Ingram', 'Reese', 'Lang', 'Spence',
    'Carey', 'Bird', 'Hess', 'Morse', 'Santiago', 'Leon', 'Krueger', 'Cochran',
    'Pratt', 'Valencia', 'Jarvis', 'Sharp', 'Oconnor', 'Levine', 'Flynn', 'Chang',
    'Yates', 'Nolan', 'Zuniga', 'Maddox', 'Whitehead', 'Gallagher', 'Michael', 'Cooke',
    'Sanford', 'Pitts', 'Haley', 'Hanna', 'Hatfield', 'Hoover', 'Decker', 'Davila',
    'Vega', 'Stafford', 'Cain', 'Dillon', 'Wiggins', 'Mathews', 'Krause', 'McMillan',
    'Kent', 'Holt', 'Shaffer', 'Dyer', 'Koch', 'Blackburn', 'Riddle', 'Shields',
    'Hendrix', 'Mahoney', 'Morrow', 'Collier', 'Stein', 'Best', 'Blanchard', 'Melton',
    'Maynard', 'Mercer', 'Osborne', 'Albert', 'Acosta', 'Petty', 'Winters', 'Trujillo',
    'Jennings', 'Conley', 'Prince', 'McGuire', 'Waller', 'Barr', 'Dickson', 'Stuart',
    'Potts', 'Valentine', 'Frost', 'Gentry', 'Hester', 'Cantrell', 'Ayers', 'Blevins',
    'Holman', 'Donovan', 'Bradshaw', 'English', 'Hahn', 'Aaron', 'Barton', 'Hendricks',
    'Church', 'Rosales', 'Howe', 'Everett', 'Gould', 'Harrington', 'Oneal', 'Bean',
    'Villanueva', 'Schroeder', 'Solomon', 'Summers', 'Dougherty', 'Livingston', 'Pace', 'Avila',
    'Knox', 'Dunlap', 'Saunders', 'Alvarado', 'Hayden', 'Greer', 'Roman', 'Buck',
    'Hines', 'Weeks', 'Witt', 'Navarro', 'Juarez', 'Cervantes', 'Carey', 'Garrett',
    'Lowe', 'Dodd', 'Duke', 'Pena', 'Costa', 'Galloway', 'Tate', 'Mayer',
    'Meyers', 'Schaefer', 'Noel', 'Kruger', 'Giles', 'Crosby', 'Sloan', 'Wyatt',
    'Johns', 'Ramsey', 'Ibarra', 'Escobar', 'Whitaker', 'Joyce', 'Burnett', 'Wall',
    'Barlow', 'Randolph', 'Atkinson', 'Horn', 'Clements', 'Floyd', 'Dodson', 'Lowery',
    'Ashley', 'Moon', 'Buchanan', 'Nava', 'Proctor', 'Pruitt', 'Phelps', 'Hinton',
  ];

  // Austin/Travis County street names (expanded to 150+)
  private streetNames = [
    'Main', 'Oak', 'Lamar', 'Congress', 'Guadalupe', 'Burnet', 'Airport', 'Oltorf',
    'Anderson', 'Cave', 'Slaughter', 'Cannon', 'Research', 'Parmer', 'Braker',
    'Rundberg', 'Loop', 'Lamar', 'Riverside', 'Anderson',
    'Congress', 'Red R', 'Rainey', 'Chavez', 'MLK', 'Dean',
    'Speedway', 'Duval', 'Shoal', 'Koenig', 'Far W', 'Research', 'Blvd',
     'First', 'East 7th', 'West 6th', 'Barton Springs', 'Westlake', 'Exposition',
      'Windsor', 'Enfield', 'Balcones', 'Spicewood', 'Capital of Texas', 'Cameron',
    'Metric', 'Dessau', 'Lamar Blvd', 'IH 35', 'Loop 360', 'Wells Branch',
    'McNeil', 'Howard', 'Jollyville', 'Mopac', 'Manchaca', 'Riverside',
        'Guadalupe', 'Rio Grande', 'Nueces', 'San Antonio', 'Lavaca', 'Colorado',
    'Brazos', 'San Jacinto', 'Trinity', 'Neches', 'Sabine', 'Blanco',
   'Manor', 'Martin Luther King', 'Airport', 'Pleasant Valley', 'Springdale',
    'Loyola', 'Berkman', 'Mueller', 'Cherrywood', 'Hancock',
    // Additional Austin streets
    'Burnet Road', 'South 1st', 'East 6th', 'West 5th', 'East 11th', 'West 12th',
    'Guadalupe', 'Street', 'Avenue', 'Lavaca', 'Street', 'Brazos', 'Boulevard',
   'Red River', 'Trinity', 'Neches', 'Sabine', 'Waller Street',
    'San Marcos', 'Cesar Chavez', 'East Cesar Chavez', 'Drive', 'Town Lake',
    'Manor', 'Airport', 'Koenig Lane', 'North Lamar', 'South Lamar Boulevard',
    'Mopac Expressway', 'Loop 1', 'Highway 183', 'Ben White', 'Highway 290',
    'FM 620', 'FM 2222', 'RM 2244', 'RM 620', 'Lakeline Boulevard',
    'Cedar Park', 'Anderson Lane', 'Steck Avenue', 'Spicewood Springs', 'Mesa Drive',
    'Hill', 'Boulevard', 'Lane', 'Burnet', 'Drive', 'Road', "East", "West", "Avenue", "Ave.",
    'Dittmar', 'Montopolis', 'South', 'North', 'Crossing', 'Fall',
    'Del Valle', 'Webberville', 'Creek', 'Johnny Morris', 'Cameron Road', 'Airport', 'Springdale', 'General',
    '4th S', '5th S', '2nd S', '3rd S', 'Square',
    'West Lynn', 'Park', 'Square', 'Place', 'San G',
  ];

  // Property types and building names (expanded)
  private propertyTypes = [
    'Apartments', 'Condos', 'Townhomes', 'Office', 'Retail', 'Plaza', 'Center',
    'Building', 'Tower', 'Park', 'Ranch', 'Estates', 'Village', 'Square',
    'Commons', 'Crossing', 'Landing', 'Pointe', 'Ridge', 'Creek', 'Hills',
        'Woods', 'Grove', 'Meadows', 'Terrace', 'Court', 'Place',
    'Lofts', 'Flats', 'Studios', 'Villas', 'Gardens', 'Heights', 'Trails',
    'Vista', 'Reserve', 'Springs', 'Oaks', 'Pines', 'Palms', 'Lake',
    'Ranch', 'Farm', 'Pecan', 'Walnut', 'River', 'Lake', 'Mount', 'Ridge',
  ];

  // Business/Company suffixes (optimized for high success rate)
  // ONLY legal entity types and proven real estate terms
  // Removed generic terms that cause zero-results: Ventures, Development, Developers,
  // Real Estate, Management, Equity, Assets, Portfolio (26% zero-result rate)
  private businessSuffixes = [
    'LLC',         // Legal entity - high reliability
    'Inc',         // Legal entity - high reliability
    'Corp',        // Legal entity - high reliability
    'Partner',     // Legal entity
    'Develop',     // Covers 'Developers'/'Development'/etc.
    'LTD',         // Legal entity - high reliability
    'Company',     // Legal entity - high reliability
    'Properties',  // Real estate specific - proven
    'Trust',       // Real estate specific - proven
    'Real',        // Real estate specific - proven
    'Holding',     // Investment specific - proven
    'Assoc',       // Association/Associates, etc.
  ];

  // Austin neighborhoods and subdivisions (expanded to 75+)
  private neighborhoods = [
    'Hyde',  'Park', 'Clark', 'ville', 'Bouldin',  'Creek', 'Travis', 'Heights', 'Zilker',
    'Allandale', 'Crestview', 'Rosedale', 'Loop', 'Mueller',
    'East Austin', 'South Congress', 'Barton', 'Tarrytown', 'West Lake',
    'Circle C', 'Ranch', 'Avery', 'Anderson Mill', 'Brushy Creek',
    'Wells Branch', 'Creek', 'Windsor Park', 'Cherrywood', 'Hancock',
    'Brentwood', 'Walnut', 'Gracywoods', 'Balcones', 'Great Hills',
    // Additional neighborhoods
    'Onion Creek', 'Barton Creek', 'Oak Hill', 'Sunset Valley', 'Rollingwood',
    'West Campus', 'East Cesar Chavez', 'Holly', 'Govalle', 'Riverside',
    'Montopolis', 'Pleasant Valley', 'Del Valle', 'Dove Springs', 'Southpark Meadows',
    'St. Edwards', 'St. Johns', 'North University', 'Wooten', 'Highland',
    'Heritage', 'Pemberton Heights', 'Old West Austin', 'Bryker Woods', 'Old Enfield',
    'Judges', 'Crest', 'Northwest', 'Estates', 'Ridgetop',
    'Spicewood', 'Bull', 'Mesa Park', 'Westover Hills', 'Rollingwood West', 'Lost Creek',
    'Senna', 'Ranch at Cypress Creek', 'Sendero Springs', 'Falconhead', 'Shady Hollow',
    'Eanes', 'Rob Roy', 'Courtyard', 'Sendera',
    'Belterra', 'Canyon', 'Maple Run', 'Common', 'Acres', 'Spring',
  ];

  // Common property descriptors
  private propertyDescriptors = [
    'Home', 'House', 'Property', 'Land', 'Lot', 'Parcel', 'Tract',
    'Residence', 'Unit', 'Suite', 'Space', 'Commercial', 'Residential',
    'Condo', 'Comm', 'Ste.'
  ];

  constructor() {
    this.optimizer = searchTermOptimizer;
  }

  // Load all previously used search terms from database to avoid duplicates
  async loadUsedTermsFromDatabase(forceRefresh = false): Promise<void> {
    const now = Date.now();

    // Check if we need to refresh
    if (!forceRefresh && this.dbTermsLoaded && (now - this.lastDbRefresh) < this.DB_REFRESH_INTERVAL) {
      return; // Already loaded and not time to refresh yet
    }

    const isRefresh = this.dbTermsLoaded;
    logger.info(isRefresh ? 'ğŸ”„ Refreshing search terms from database...' : 'ğŸ“š Loading previously used search terms from database...');

    try {
      const previousCount = this.usedTerms.size;

      // Get all unique search terms from scrape jobs
      const existingTerms = await prisma.scrapeJob.findMany({
        select: {
          searchTerm: true,
        },
        distinct: ['searchTerm'],
      });

      // Add all terms to the set (Set automatically handles duplicates)
      existingTerms.forEach(job => {
        this.usedTerms.add(job.searchTerm);
      });

      // Initialize or update the deduplicator with current terms
      if (!this.deduplicator) {
        this.deduplicator = new SearchTermDeduplicator(this.usedTerms);
      }

      const currentCount = this.usedTerms.size;
      const newTermsFound = currentCount - previousCount;

      if (isRefresh) {
        logger.info(`âœ… Refreshed: ${currentCount.toLocaleString()} total terms (${newTermsFound} new since last check)`);
      } else {
        logger.info(`âœ… Loaded ${currentCount.toLocaleString()} previously used search terms`);
        logger.info(`   Will avoid duplicates like: "Estate", "Family", "Trust", etc.`);
      }

      this.dbTermsLoaded = true;
      this.lastDbRefresh = now;
    } catch (error) {
      logger.error({ err: error }, 'âŒ Failed to load used terms from database:');
      throw error;
    }
  }

  private generateLastNameOnly(): string {
    return this.lastNames[Math.floor(Math.random() * this.lastNames.length)];
  }

  private generateStreetAddress(): string {
    const number = Math.floor(Math.random() * 9999) + 1;
    const street = this.streetNames[Math.floor(Math.random() * this.streetNames.length)];
    return `${number} ${street}`;
  }

  private generatePropertyType(): string {
    return this.propertyTypes[Math.floor(Math.random() * this.propertyTypes.length)];
  }

  private generateBusinessName(): string {
    const name = this.lastNames[Math.floor(Math.random() * this.lastNames.length)];
    const suffix = this.businessSuffixes[Math.floor(Math.random() * this.businessSuffixes.length)];
    return `${name} ${suffix}`;
  }

  private generateStreetNameOnly(): string {
    return this.streetNames[Math.floor(Math.random() * this.streetNames.length)];
  }

  private generateFourLetterWord(): string {
    const words = ['Park', 'Lake', 'Hill', 'Wood', 'Glen', 'Dale', 'View', 'Rock', 'Pine', 'Sage'];
    return words[Math.floor(Math.random() * words.length)];
  }

  // NEW: Generate neighborhood name
  private generateNeighborhood(): string {
    return this.neighborhoods[Math.floor(Math.random() * this.neighborhoods.length)];
  }

  // NEW: Generate property type with descriptor
  private generatePropertyWithDescriptor(): string {
    const type = this.propertyTypes[Math.floor(Math.random() * this.propertyTypes.length)];
    const descriptor = this.propertyDescriptors[Math.floor(Math.random() * this.propertyDescriptors.length)];
    return Math.random() > 0.5 ? `${type} ${descriptor}` : type;
  }

  // NEW: Generate partial street address (just number + street, more common)
  private generatePartialAddress(): string {
    const number = Math.floor(Math.random() * 9999) + 1;
    const street = this.streetNames[Math.floor(Math.random() * this.streetNames.length)];
    const words = street.split(' ');
    // Sometimes use just first word of street name for broader matches
    return Math.random() > 0.3 ? `${number} ${street}` : `${number} ${words[0]}`;
  }

  // OPTIMIZED: Generate first names only (HIGH YIELD - avg 426+ properties)
  private generateFirstNameOnly(): string {
    return this.firstNames[Math.floor(Math.random() * this.firstNames.length)];
  }

  // OPTIMIZED: Generate common street suffixes (VERY HIGH YIELD - avg 637+ properties)
  private generateStreetSuffix(): string {
    const suffixes = ['Avenue', 'Boulevard', 'Court', 'Drive', 'Lane', 'Circle',
                      'Place', 'Way', 'Trail', 'Path', 'Bend', 'Loop', 'Terrace',
                      'Parkway', 'Ridge', 'Hill', 'Manor'];
    return suffixes[Math.floor(Math.random() * suffixes.length)];
  }

  // OPTIMIZED: Generate 4-letter geographic terms (HIGH YIELD - avg 637+ properties)
  private generateGeographicTerm(): string {
    const terms = ['Hill', 'Lake', 'Cave', 'Park', 'Glen', 'Dale', 'Ford',
                   'Cove', 'Rock', 'Wood', 'Farm', 'Mill', 'Pond', 'Peak'];
    return terms[Math.floor(Math.random() * terms.length)];
  }

  // OPTIMIZED: Generate Hispanic/Asian surnames (HIGH YIELD - avg 2000+ properties each)
  private generateHispanicAsianSurname(): string {
    const names = ['Garcia', 'Hernandez', 'Lopez', 'Gonzalez', 'Perez', 'Sanchez',
                   'Rivera', 'Torres', 'Ramirez', 'Flores', 'Gomez', 'Cruz',
                   'Lee', 'Chen', 'Wang', 'Kim', 'Patel', 'Singh', 'Chang', 'Nguyen'];
    return names[Math.floor(Math.random() * names.length)];
  }

  /**
   * Optimize search strategy based on actual performance data every 50 jobs
   * This analyzes completed jobs and suggests high-performing terms
   */
  private async optimizeStrategy(): Promise<string[]> {
    logger.info('\nğŸ”§ Optimizing search strategy based on performance data...');

    try {
      // Get performance stats
      const stats = await this.optimizer.getPerformanceStats();
      logger.info(`ğŸ“Š Analyzed ${stats.totalSearchTerms} unique search terms`);
      logger.info(`   Avg efficiency: ${stats.avgEfficiency.toFixed(2)}`);
      logger.info(`   Avg success rate: ${(stats.avgSuccessRate * 100).toFixed(1)}%`);
      logger.info(`   Avg results per search: ${stats.avgResultsPerSearch.toFixed(1)}`);

      // Get top performers
      if (stats.topPerformers.length > 0) {
        logger.info(`\nğŸ† Top 5 performing search terms:`);
        stats.topPerformers.slice(0, 5).forEach((term, i) => {
          logger.info(`   ${i + 1}. "${term.searchTerm}" - ${term.avgResultsPerSearch.toFixed(0)} avg results, ${(term.successRate * 100).toFixed(0)}% success`);
        });
      }

      // Get optimized terms for next batch
      const optimizedTerms = await this.optimizer.getOptimizedTerms({
        minEfficiency: 5.0,
        minSuccessRate: 0.5,
        maxTermsToReturn: 30,
        excludeRecentlyUsed: true,
        recentDays: 1, // Only exclude terms used in last 24 hours
      });

      // Get suggested new terms based on successful patterns
      const suggestedTerms = await this.optimizer.suggestNewTerms(20);

      logger.info(`âœ¨ Generated ${optimizedTerms.length} high-efficiency terms to prioritize`);
      logger.info(`ğŸ’¡ Suggested ${suggestedTerms.length} new terms based on successful patterns\n`);

      // Reset the counter
      this.jobsProcessedSinceLastOptimization = 0;

      // Combine optimized and suggested terms
      return [...optimizedTerms, ...suggestedTerms];
    } catch (error) {
      logger.error({ err: error }, 'âŒ Failed to optimize strategy:');
      return [];
    }
  }

    async getNextBatch(batchSize: number): Promise<string[]> {
    // Load or refresh database terms (automatic hourly refresh)
    await this.loadUsedTermsFromDatabase();

    // Check if we should optimize strategy every 50 jobs
    this.jobsProcessedSinceLastOptimization++;
    let optimizedTerms: string[] = [];

    if (this.jobsProcessedSinceLastOptimization >= this.OPTIMIZATION_INTERVAL) {
      optimizedTerms = await this.optimizeStrategy();
    }

    const batch: string[] = [];

    // Weighted strategies - OPTIMIZED based on actual performance (286K+ properties analyzed):
    // Real-world results from database analysis:
    //   - Street Suffixes: 637.7 avg properties per term (4-char words) - BEST!
    //   - Common Names: 474.6 avg (6-char), 467.7 avg (5-char) - EXCELLENT
    //   - First Names: 426.4 avg properties - EXCELLENT (James, John, Robert, etc)
    //   - Hispanic/Asian Names: 2000-2700 avg each (Garcia, Rodriguez, Lee, Kim, etc)
    //   - Geographic Terms: 2000-6000 each (Hill, Lake, Cave, etc)
    //   - Business Entities: Only 2.8 avg - VERY POOR
    //
    // Strategy: Focus 85% on 4-6 character single words (proven winners)
    const strategies = [
      { fn: () => this.generateStreetSuffix(), weight: 50 },           // INCREASED! 1000+ avg - Boulevard, Drive, Lane untried
      { fn: () => this.generateFirstNameOnly(), weight: 35 },          // INCREASED! 1132 avg last hour - John: 13,393!
      { fn: () => this.generateLastNameOnly(), weight: 30 },           // REDUCED - Most high-yield names exhausted
      { fn: () => this.generateGeographicTerm(), weight: 25 },         // GREAT! Rock: 4,615, Mill: 3,778
      { fn: () => this.generateNeighborhood(), weight: 20 },           // Good for area coverage
      { fn: () => this.generateHispanicAsianSurname(), weight: 15 },   // 2000+ avg - Garcia, Lee, Kim, etc
      { fn: () => this.generatePropertyType(), weight: 10 },           // REDUCED - Moderate yield
      { fn: () => this.generateStreetNameOnly(), weight: 5 },          // REDUCED - Many covered
      { fn: () => this.generateBusinessName(), weight: 0 },            // ELIMINATED! 13% success, wasted 83% of last hour
      // REMOVED inefficient strategies:
      // - generatePropertyWithDescriptor() - 26% zero-result rate
      // - generateTwoLetterCombo() - 73.9% failure rate
      // - generateThreeLetterCombo() - 73.9% failure rate
      // - generateFourLetterWord() - 73.9% failure rate (now covered by generateGeographicTerm)
      // - generateFullName() - only 4.4 avg props, 26% zero-result rate
      // - generateStreetAddress() - 44.8% zero-result rate
      // - generateCompoundName() - causes JSON parse errors
    ];

    // Create weighted array
    const weightedStrategies: (() => string)[] = [];
    strategies.forEach(s => {
      for (let i = 0; i < s.weight; i++) {
        weightedStrategies.push(s.fn);
      }
    });

    let attempts = 0;
    const maxAttempts = batchSize * 10;

    // Reset deduplicator stats for this batch
    this.deduplicator.resetStats();

    // First, prioritize optimized terms (if available)
    if (optimizedTerms.length > 0) {
      logger.info(`ğŸ¯ Prioritizing ${optimizedTerms.length} high-performing terms in this batch`);
      for (const term of optimizedTerms) {
        if (batch.length >= batchSize) break;

        // Use the deduplicator to check if we should skip this term
        if (!this.deduplicator.shouldSkipTerm(term)) {
          this.deduplicator.markTermAsUsed(term);
          this.usedTerms.add(term);
          batch.push(term);
        }
      }
    }

    // Fill remaining slots with random strategy-generated terms
    while (batch.length < batchSize && attempts < maxAttempts) {
      attempts++;
      const strategy = weightedStrategies[Math.floor(Math.random() * weightedStrategies.length)];
      const term = strategy();

      // Use the deduplicator to check if we should skip this term
      if (!this.deduplicator.shouldSkipTerm(term)) {
        // Term is unique enough - add it to the batch
        this.deduplicator.markTermAsUsed(term);
        this.usedTerms.add(term); // Also update local set for backwards compatibility
        batch.push(term);
      }
    }

    // Log deduplication statistics
    const stats = this.deduplicator.getStats();
    if (stats.exactDuplicates > 0) {
      logger.info(`   âš ï¸  Skipped ${stats.exactDuplicates} exact duplicates`);
    }
    if (stats.tooCommonTerms > 0) {
      logger.info(`   â±ï¸  Skipped ${stats.tooCommonTerms} too-common terms (cause API timeouts)`);
    }
    if (stats.businessSupersets > 0) {
      logger.info(`   ğŸ¢ Skipped ${stats.businessSupersets} business entity supersets`);
    }
    if (stats.twoWordSupersets > 0) {
      logger.info(`   ğŸ“ Skipped ${stats.twoWordSupersets} two-word supersets`);
    }
    if (stats.multiWordSupersets > 0) {
      logger.info(`   ğŸ“š Skipped ${stats.multiWordSupersets} multi-word supersets`);
    }

    return batch;
  }
}


class ContinuousBatchScraper {
  private generator = new SearchPatternGenerator();
  private stats = {
    totalQueued: 0,
    batchesProcessed: 0,
    startTime: Date.now(),
    startingPropertyCount: 0,
  };
  private running = true;

  async run() {
    logger.info('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—');
    logger.info('â•‘   CONTINUOUS BATCH SCRAPER - VOLUME OPTIMIZED         â•‘');
    logger.info('â•‘   Target: 400,000 | Focus: High-Yield Search Terms    â•‘');
    logger.info('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');

    // Clear pending jobs from queue to start fresh with optimized strategy
    logger.info('ğŸ§¹ Clearing pending jobs from queue...');
    const pendingCount = await scraperQueue.getWaitingCount();
    if (pendingCount > 0) {
      await scraperQueue.clean(0, 'wait'); // Remove all waiting jobs
      logger.info(`âœ“ Cleared ${pendingCount} pending jobs\n`);
    } else {
      logger.info('âœ“ No pending jobs to clear\n');
    }

    // Get starting property count
    this.stats.startingPropertyCount = await prisma.property.count();
    logger.info(`Starting property count: ${this.stats.startingPropertyCount.toLocaleString()}`);
    logger.info(`Target: ${TARGET_PROPERTIES.toLocaleString()}`);
    logger.info(`Remaining: ${(TARGET_PROPERTIES - this.stats.startingPropertyCount).toLocaleString()}\n`);

    // Handle graceful shutdown
    process.on('SIGINT', () => this.stop());
    process.on('SIGTERM', () => this.stop());

    // Start monitoring in background
    this.startMonitoring();

    // Main loop
    while (this.running) {
      const currentCount = await prisma.property.count();

      if (currentCount >= TARGET_PROPERTIES) {
        logger.info(`\nğŸ‰ TARGET REACHED! Current count: ${currentCount.toLocaleString()}`);
        break;
      }

      // Check queue status
      const [waiting, active] = await Promise.all([
        scraperQueue.getWaitingCount(),
        scraperQueue.getActiveCount(),
      ]);

      // Queue threshold reduced from 500 to 100 because API method returns 50x more results
      // With old method: 500 searches * 20 results = 10,000 potential properties
      // With new method: 100 searches * 1000 results = 100,000 potential properties
      if (waiting + active < 100) {
        await this.queueBatch();
      } else {
        logger.info(`Queue full (${waiting} waiting, ${active} active). Waiting...`);
      }

      // Wait before next batch
      await this.delay(DELAY_BETWEEN_BATCHES);
    }

    await this.printFinalReport();
    process.exit(0);
  }

  private async queueBatch() {
    const searchTerms = await this.generator.getNextBatch(BATCH_SIZE);
    this.stats.batchesProcessed++;

    logger.info(`\nğŸ“¦ Batch #${this.stats.batchesProcessed} (${searchTerms.length} terms)`);

    for (const searchTerm of searchTerms) {
      try {
        await scraperQueue.add(
          'scrape-properties',
          {
            searchTerm,
            userId: 'continuous-batch',
            scheduled: true,
          },
          {
            attempts: 3,
            backoff: {
              type: 'exponential',
              delay: 2000,
            },
            removeOnComplete: 100,
            removeOnFail: 50,
          }
        );

        this.stats.totalQueued++;
      } catch (error) {
        logger.error({ err: error }, `Failed to queue ${searchTerm}:`);
      }
    }

    logger.info(`âœ“ Queued ${searchTerms.length} jobs (Total: ${this.stats.totalQueued})`);
  }

  private startMonitoring() {
    setInterval(async () => {
      try {
        const [currentCount, waiting, active, completed, failed] = await Promise.all([
          prisma.property.count(),
          scraperQueue.getWaitingCount(),
          scraperQueue.getActiveCount(),
          scraperQueue.getCompletedCount(),
          scraperQueue.getFailedCount(),
        ]);

        const newProperties = currentCount - this.stats.startingPropertyCount;
        const progress = (currentCount / TARGET_PROPERTIES) * 100;
        const elapsed = Math.floor((Date.now() - this.stats.startTime) / 1000);
        const hours = Math.floor(elapsed / 3600);
        const minutes = Math.floor((elapsed % 3600) / 60);
        const rate = newProperties / (elapsed / 60); // properties per minute

        logger.info(`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Runtime: ${hours}h ${minutes}m | Progress: ${progress.toFixed(2)}%
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Database:
â”‚   ğŸ“Š Current:     ${currentCount.toLocaleString().padStart(10)}
â”‚   ğŸ†• New:         ${newProperties.toLocaleString().padStart(10)}
â”‚   ğŸ¯ Target:      ${TARGET_PROPERTIES.toLocaleString().padStart(10)}
â”‚   ğŸ“ˆ Remaining:   ${(TARGET_PROPERTIES - currentCount).toLocaleString().padStart(10)}
â”‚   âš¡ Rate:        ${rate.toFixed(1)} props/min
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Queue:
â”‚   â³ Waiting:     ${waiting.toString().padStart(6)}
â”‚   ğŸ”„ Active:      ${active.toString().padStart(6)}
â”‚   âœ… Completed:   ${completed.toString().padStart(6)}
â”‚   âŒ Failed:      ${failed.toString().padStart(6)}
â”‚   ğŸ“¦ Batches:     ${this.stats.batchesProcessed.toString().padStart(6)}
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        `);

        // Estimate time to completion
        if (rate > 0) {
          const remaining = TARGET_PROPERTIES - currentCount;
          const minutesRemaining = remaining / rate;
          const hoursRemaining = minutesRemaining / 60;
          logger.info(`â±ï¸  Estimated time to target: ${hoursRemaining.toFixed(1)} hours`);
        }
      } catch (error) {
        logger.error({ err: error }, 'Monitoring error:');
      }
    }, CHECK_INTERVAL);
  }

  private async printFinalReport() {
    const finalCount = await prisma.property.count();
    const elapsed = Math.floor((Date.now() - this.stats.startTime) / 1000);
    const hours = Math.floor(elapsed / 3600);
    const minutes = Math.floor((elapsed % 3600) / 60);

    logger.info(`
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         CONTINUOUS SCRAPER FINAL REPORT                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â±ï¸  Total Runtime: ${hours}h ${minutes}m

ğŸ“Š Properties:
   â€¢ Starting:     ${this.stats.startingPropertyCount.toLocaleString()}
   â€¢ Final:        ${finalCount.toLocaleString()}
   â€¢ Added:        ${(finalCount - this.stats.startingPropertyCount).toLocaleString()}
   â€¢ Target:       ${TARGET_PROPERTIES.toLocaleString()}

ğŸ“¦ Jobs:
   â€¢ Total Queued: ${this.stats.totalQueued.toLocaleString()}
   â€¢ Batches:      ${this.stats.batchesProcessed}

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ‰ SCRAPING SESSION COMPLETED! ğŸ‰                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    `);
  }

  private stop() {
    logger.info('\nğŸ›‘ Stopping continuous scraper...');
    this.running = false;
  }

  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}

// Run the continuous scraper
const scraper = new ContinuousBatchScraper();
scraper.run().catch((error) => {
  logger.error({ err: error }, 'Fatal error:');
  process.exit(1);
});
</file>

<file path="debug-token-refresh.ts">
#!/usr/bin/env node
/**
 * Debug Token Refresh
 * Tests the token refresh service and prints detailed diagnostics
 */

import { tokenRefreshService } from '../services/token-refresh.service';
import logger from '../lib/logger';

async function debugTokenRefresh() {
  logger.info('='.repeat(60));
  logger.info('DEBUG: Token Refresh Service');
  logger.info('='.repeat(60));

  // Check initial state
  logger.info('\n1. Initial State:');
  logger.info('   currentToken:', tokenRefreshService.getCurrentToken());

  const initialStats = tokenRefreshService.getStats();
  logger.info('   Stats:', JSON.stringify(initialStats, null, 2));

  // Try to refresh token
  logger.info('\n2. Calling refreshToken()...');
  const startTime = Date.now();

  try {
    const token = await tokenRefreshService.refreshToken();
    const duration = Date.now() - startTime;

    logger.info(`\n3. refreshToken() returned after ${duration}ms:`);
    logger.info('   Type:', typeof token);
    logger.info('   Value:', token);
    logger.info('   Length:', token ? token.length : 'N/A');
    logger.info('   First 50 chars:', token ? token.substring(0, 50) : 'N/A');

  } catch (error) {
    logger.error('\n3. refreshToken() threw error:', error);
  }

  // Check state after refresh
  logger.info('\n4. State After Refresh:');
  logger.info('   currentToken:', tokenRefreshService.getCurrentToken());

  const afterStats = tokenRefreshService.getStats();
  logger.info('   Stats:', JSON.stringify(afterStats, null, 2));

  // Test getCurrentToken multiple times
  logger.info('\n5. Multiple getCurrentToken() calls:');
  for (let i = 0; i < 3; i++) {
    const token = tokenRefreshService.getCurrentToken();
    logger.info(`   Call ${i + 1}:`, token ? token.substring(0, 50) : 'null');
  }

  // Cleanup
  logger.info('\n6. Cleaning up...');
  await tokenRefreshService.cleanup();

  logger.info('\n' + '='.repeat(60));
  logger.info('DEBUG: Complete');
  logger.info('='.repeat(60));
}

debugTokenRefresh()
  .then(() => process.exit(0))
  .catch((error) => {
    logger.error('Script failed:', error);
    process.exit(1);
  });
</file>

<file path="enqueue-commercial-batch.ts">
#!/usr/bin/env node
/**
 * Enqueue Commercial Property Searches
 * Queues commercial property-related search terms
 */

import { scraperQueue } from '../queues/scraper.queue';
import logger from '../lib/logger';
import { config } from '../config';

const COMMERCIAL_TERMS = [
  'Shopping',
  'Retail',
  'Office',
  'Warehouse',
  'Industrial',
  'Commercial',
  'Business',
  'Store',
  'Mall',
  'Building',
];

async function enqueueCommercialBatch() {
  logger.info('ğŸ¢ Starting Commercial Batch Enqueue');
  logger.info(`Auto-refresh token enabled: ${config.scraper.autoRefreshToken}`);
  logger.info(`Using Doppler: ${config.doppler.enabled ? 'Yes' : 'No'}`);

  try {
    let successCount = 0;
    let failCount = 0;

    for (const term of COMMERCIAL_TERMS) {
      try {
        const job = await scraperQueue.add('scrape-properties', {
          searchTerm: term,
          userId: 'commercial-batch-enqueue',
          scheduled: true,
        }, {
          attempts: 3,
          backoff: {
            type: 'exponential',
            delay: 2000,
          },
          priority: 2, // Higher priority for commercial
          removeOnComplete: 100,
          removeOnFail: 50,
        });

        successCount++;
        logger.info(`âœ… [${successCount}/${COMMERCIAL_TERMS.length}] Queued: "${term}" (Job ID: ${job.id})`);
      } catch (error) {
        failCount++;
        logger.error({ err: error }, `âŒ Failed to queue "${term}":`);
      }
    }

    logger.info(`\nğŸ“Š Summary: ${successCount} queued, ${failCount} failed`);
    logger.info('âœ¨ Commercial batch enqueue completed!');
  } catch (error) {
    logger.error({ err: error }, 'âŒ Fatal error:');
    process.exit(1);
  }
}

enqueueCommercialBatch()
  .then(() => process.exit(0))
  .catch((error) => {
    logger.error({ err: error }, 'âŒ Script failed:');
    process.exit(1);
  });
</file>

<file path="enqueue-construction-batch.ts">
#!/usr/bin/env node
/**
 * Enqueue Construction & Building Searches
 * Queues construction and building-related search terms
 */

import { scraperQueue } from '../queues/scraper.queue';
import logger from '../lib/logger';
import { config } from '../config';

const CONSTRUCTION_TERMS = [
  'Construction',
  'Builders',
  'Builder',
  'Contractor',
  'Contracting',
  'Homes',
  'Home',
  'Custom Homes',
  'Housing',
  'Residential Builders',
];

async function enqueueConstructionBatch() {
  logger.info('ğŸ—ï¸  Starting Construction Batch Enqueue');
  logger.info(`Auto-refresh token enabled: ${config.scraper.autoRefreshToken}`);

  try {
    let successCount = 0;
    let failCount = 0;

    for (const term of CONSTRUCTION_TERMS) {
      try {
        const job = await scraperQueue.add('scrape-properties', {
          searchTerm: term,
          userId: 'construction-batch-enqueue',
          scheduled: true,
        }, {
          attempts: 3,
          backoff: {
            type: 'exponential',
            delay: 2000,
          },
          priority: 3,
          removeOnComplete: 100,
          removeOnFail: 50,
        });

        successCount++;
        logger.info(`âœ… [${successCount}/${CONSTRUCTION_TERMS.length}] Queued: "${term}" (Job ID: ${job.id})`);
      } catch (error) {
        failCount++;
        logger.error({ err: error }, `âŒ Failed to queue "${term}":`);
      }
    }

    logger.info(`\nğŸ“Š Summary: ${successCount} queued, ${failCount} failed`);
    logger.info('âœ¨ Construction batch enqueue completed!');
  } catch (error) {
    logger.error({ err: error }, 'âŒ Fatal error:');
    process.exit(1);
  }
}

enqueueConstructionBatch()
  .then(() => process.exit(0))
  .catch((error) => {
    logger.error({ err: error }, 'âŒ Script failed:');
    process.exit(1);
  });
</file>

<file path="enqueue-corporation-batch.ts">
#!/usr/bin/env node
/**
 * Enqueue Corporation Property Searches
 * Queues corporation-related search terms
 */

import { scraperQueue } from '../queues/scraper.queue';
import logger from '../lib/logger';
import { config } from '../config';

const CORPORATION_TERMS = [
  'Corp',
  'Corp.',
  'Corporation',
  'Incorporated',
  'Inc',
  'Inc.',
  'Company',
  'Co.',
  'Enterprise',
  'Enterprises',
];

async function enqueueCorporationBatch() {
  logger.info('ğŸ›ï¸  Starting Corporation Batch Enqueue');
  logger.info(`Auto-refresh token enabled: ${config.scraper.autoRefreshToken}`);

  try {
    let successCount = 0;
    let failCount = 0;

    for (const term of CORPORATION_TERMS) {
      try {
        const job = await scraperQueue.add('scrape-properties', {
          searchTerm: term,
          userId: 'corporation-batch-enqueue',
          scheduled: true,
        }, {
          attempts: 3,
          backoff: {
            type: 'exponential',
            delay: 2000,
          },
          priority: 2,
          removeOnComplete: 100,
          removeOnFail: 50,
        });

        successCount++;
        logger.info(`âœ… [${successCount}/${CORPORATION_TERMS.length}] Queued: "${term}" (Job ID: ${job.id})`);
      } catch (error) {
        failCount++;
        logger.error({ err: error }, `âŒ Failed to queue "${term}":`);
      }
    }

    logger.info(`\nğŸ“Š Summary: ${successCount} queued, ${failCount} failed`);
    logger.info('âœ¨ Corporation batch enqueue completed!');
  } catch (error) {
    logger.error({ err: error }, 'âŒ Fatal error:');
    process.exit(1);
  }
}

enqueueCorporationBatch()
  .then(() => process.exit(0))
  .catch((error) => {
    logger.error({ err: error }, 'âŒ Script failed:');
    process.exit(1);
  });
</file>

<file path="enqueue-foundation-batch.ts">
#!/usr/bin/env node
/**
 * Enqueue Foundation & Nonprofit Searches
 * Queues foundation, nonprofit, and charitable organization search terms
 */

import { scraperQueue } from '../queues/scraper.queue';
import logger from '../lib/logger';
import { config } from '../config';

const FOUNDATION_TERMS = [
  'Foundation',
  'Charitable',
  'Charity',
  'Nonprofit',
  'Non-Profit',
  'Organization',
  'Institute',
  'Society',
  'Association',
  'Endowment',
];

async function enqueueFoundationBatch() {
  logger.info('ğŸ—ï¸  Starting Foundation Batch Enqueue');
  logger.info(`Auto-refresh token enabled: ${config.scraper.autoRefreshToken}`);

  try {
    let successCount = 0;
    let failCount = 0;

    for (const term of FOUNDATION_TERMS) {
      try {
        const job = await scraperQueue.add('scrape-properties', {
          searchTerm: term,
          userId: 'foundation-batch-enqueue',
          scheduled: true,
        }, {
          attempts: 3,
          backoff: {
            type: 'exponential',
            delay: 2000,
          },
          priority: 3,
          removeOnComplete: 100,
          removeOnFail: 50,
        });

        successCount++;
        logger.info(`âœ… [${successCount}/${FOUNDATION_TERMS.length}] Queued: "${term}" (Job ID: ${job.id})`);
      } catch (error) {
        failCount++;
        logger.error({ err: error }, `âŒ Failed to queue "${term}":`);
      }
    }

    logger.info(`\nğŸ“Š Summary: ${successCount} queued, ${failCount} failed`);
    logger.info('âœ¨ Foundation batch enqueue completed!');
  } catch (error) {
    logger.error({ err: error }, 'âŒ Fatal error:');
    process.exit(1);
  }
}

enqueueFoundationBatch()
  .then(() => process.exit(0))
  .catch((error) => {
    logger.error({ err: error }, 'âŒ Script failed:');
    process.exit(1);
  });
</file>

<file path="enqueue-grove.ts">
import { scraperQueue } from '../queues/scraper.queue';

async function enqueueGrove() {
  console.log('Enqueueing job for search term: Grove');
  await scraperQueue.add('scrape', { searchTerm: 'Grove' });
  console.log('Job enqueued successfully');
  process.exit(0);
}

enqueueGrove().catch((err) => {
  console.error('Error enqueueing job:', err);
  process.exit(1);
});
</file>

<file path="enqueue-high-priority.ts">
import { scraperQueue } from '../queues/scraper.queue';

// High-value terms identified from last hour analysis
const HIGH_PRIORITY_TERMS = [
  'Boulevard',  // Expected: 7,000+ (Avenue = 25,483)
  'Drive',      // Expected: 5,000+
  'Lane',       // Expected: 5,000+
  'Way',        // Expected: 3,000+
  'Terrace',    // Expected: 2,000+
  'Michelle',   // Expected: 2,000+ (top 30 US name)
];

async function enqueueHighPriority() {
  console.log('Enqueueing high-priority search terms from analysis...\n');

  for (const term of HIGH_PRIORITY_TERMS) {
    try {
      await scraperQueue.add(
        'scrape-properties',
        { searchTerm: term },
        { priority: 1 } // Highest priority
      );
      console.log(`âœ“ Enqueued: ${term}`);
    } catch (error) {
      console.error(`âœ— Failed to enqueue ${term}:`, error);
    }
  }

  console.log(`\nâœ“ Successfully enqueued ${HIGH_PRIORITY_TERMS.length} high-priority terms`);
  console.log('Expected total: 24,000+ properties from these 6 terms alone!');
  process.exit(0);
}

enqueueHighPriority().catch((err) => {
  console.error('Error:', err);
  process.exit(1);
});
</file>

<file path="enqueue-high-value-batch.ts">
#!/usr/bin/env node
/**
 * Enqueue High-Value Batch Searches (40 queries)
 *
 * This script:
 * 1. Refreshes the TCAD API token first
 * 2. Enqueues 40 high-value search terms across multiple categories
 *
 * Categories included:
 * - Trust & Estate (highest yield ~70+ properties)
 * - Investment & Holdings (high yield)
 * - Corporate entities (LLC, Corp, etc.)
 * - Commercial properties
 * - Property types
 */

import { scraperQueue } from '../queues/scraper.queue';
import logger from '../lib/logger';
import { config } from '../config';
import { tokenRefreshService } from '../services/token-refresh.service';

// 40 High-Value Search Terms
const HIGH_VALUE_TERMS = [
  // Trust & Estate (10 terms - ~70+ properties each)
  'Trust',
  'Trustee',
  'Estate',
  'Family Trust',
  'Revocable Trust',
  'Irrevocable Trust',
  'Living Trust',
  'Testamentary',
  'Fiduciary',
  'Beneficiary',

  // Investment & Holdings (10 terms - high yield)
  'Investments',
  'Holdings',
  'Capital',
  'Fund',
  'Equity',
  'Ventures',
  'Asset',
  'Portfolio',
  'Management',
  'Partners',

  // Corporate Entities (10 terms - high volume)
  'LLC',
  'Limited',
  'Corporation',
  'Corp',
  'Inc',
  'Partnership',
  'LP',
  'LLP',
  'Company',
  'Group',

  // Commercial & Property Types (10 terms)
  'Commercial',
  'Residential',
  'Industrial',
  'Office',
  'Retail',
  'Warehouse',
  'Shopping',
  'Apartment',
  'Condo',
  'Development',
];

async function enqueueHighValueBatch() {
  logger.info('ğŸ’ Starting High-Value Batch Enqueue (40 queries)');
  logger.info(`Auto-refresh token enabled: ${config.scraper.autoRefreshToken}`);

  try {
    // Step 1: Refresh token first
    logger.info('ğŸ”„ Step 1/2: Refreshing TCAD API token...');
    const startRefresh = Date.now();

    const token = await tokenRefreshService.refreshToken();
    const refreshDuration = Date.now() - startRefresh;

    if (token) {
      logger.info(`âœ… Token refreshed successfully in ${refreshDuration}ms`);
      logger.info(`   Token preview: ${token.substring(0, 30)}...`);
    } else {
      logger.warn('âš ï¸  Token refresh returned null - continuing with existing token');
    }

    // Step 2: Enqueue all queries
    logger.info('\nğŸ“‹ Step 2/2: Enqueueing high-value queries...');
    logger.info(`Total queries to enqueue: ${HIGH_VALUE_TERMS.length}`);

    let successCount = 0;
    let failCount = 0;
    const startEnqueue = Date.now();

    for (let i = 0; i < HIGH_VALUE_TERMS.length; i++) {
      const term = HIGH_VALUE_TERMS[i];

      try {
        // Determine priority based on category
        let priority = 3; // Default
        if (i < 10) {
          priority = 1; // Trust & Estate - highest priority
        } else if (i < 20) {
          priority = 1; // Investment - high priority
        } else if (i < 30) {
          priority = 2; // Corporate - medium-high priority
        }

        const job = await scraperQueue.add('scrape-properties', {
          searchTerm: term,
          userId: 'high-value-batch-enqueue',
          scheduled: true,
        }, {
          attempts: 3,
          backoff: {
            type: 'exponential',
            delay: 2000,
          },
          priority,
          removeOnComplete: 100,
          removeOnFail: 50,
        });

        successCount++;

        // Determine category for logging
        let category = '';
        if (i < 10) category = 'Trust/Estate';
        else if (i < 20) category = 'Investment';
        else if (i < 30) category = 'Corporate';
        else category = 'Commercial';

        logger.info(
          `âœ… [${successCount}/${HIGH_VALUE_TERMS.length}] ` +
          `Queued: "${term}" (${category}, Priority: ${priority}, Job ID: ${job.id})`
        );

        // Small delay to avoid overwhelming the queue
        if (i < HIGH_VALUE_TERMS.length - 1) {
          await new Promise(resolve => setTimeout(resolve, 100));
        }
      } catch (error) {
        failCount++;
        logger.error({ err: error }, `âŒ Failed to queue "${term}"`);
      }
    }

    const enqueueDuration = Date.now() - startEnqueue;

    // Summary
    logger.info('\n' + '='.repeat(60));
    logger.info('ğŸ“Š BATCH ENQUEUE SUMMARY');
    logger.info('='.repeat(60));
    logger.info(`âœ… Successfully queued: ${successCount}/${HIGH_VALUE_TERMS.length}`);
    logger.info(`âŒ Failed: ${failCount}`);
    logger.info(`â±ï¸  Enqueue duration: ${enqueueDuration}ms`);
    logger.info(`â±ï¸  Total duration (with token refresh): ${refreshDuration + enqueueDuration}ms`);

    if (successCount > 0) {
      logger.info(`\nğŸ“ˆ Estimated minimum properties: ${successCount * 50}`);
      logger.info(`ğŸ“ˆ Estimated maximum properties: ${successCount * 100}`);
      logger.info(`   (Trust/Estate terms typically yield 70-100+ each)`);
    }

    logger.info('\nâœ¨ High-value batch enqueue completed!');
    logger.info('Monitor progress at: http://localhost:3001/admin/queues');
    logger.info('='.repeat(60));

  } catch (error) {
    logger.error({ err: error }, 'âŒ Fatal error');
    process.exit(1);
  }
}

// Run the script
enqueueHighValueBatch()
  .then(() => process.exit(0))
  .catch((error) => {
    logger.error({ err: error }, 'âŒ Script failed');
    process.exit(1);
  });
</file>

<file path="enqueue-investment-batch.ts">
#!/usr/bin/env node
/**
 * Enqueue Investment Property Searches
 * Queues investment and management search terms
 */

import { scraperQueue } from '../queues/scraper.queue';
import logger from '../lib/logger';
import { config } from '../config';

const INVESTMENT_TERMS = [
  'Investments',
  'Holdings',
  'Capital',
  'Fund',
  'Equity',
  'Ventures',
  'Asset',
  'Portfolio',
  'Management',
  'Manage',
];

async function enqueueInvestmentBatch() {
  logger.info('ğŸ’° Starting Investment Batch Enqueue');
  logger.info(`Auto-refresh token enabled: ${config.scraper.autoRefreshToken}`);

  try {
    let successCount = 0;
    let failCount = 0;

    for (const term of INVESTMENT_TERMS) {
      try {
        const job = await scraperQueue.add('scrape-properties', {
          searchTerm: term,
          userId: 'investment-batch-enqueue',
          scheduled: true,
        }, {
          attempts: 3,
          backoff: {
            type: 'exponential',
            delay: 2000,
          },
          priority: 1, // High priority - likely high yield
          removeOnComplete: 100,
          removeOnFail: 50,
        });

        successCount++;
        logger.info(`âœ… [${successCount}/${INVESTMENT_TERMS.length}] Queued: "${term}" (Job ID: ${job.id})`);
      } catch (error) {
        failCount++;
        logger.error({ err: error }, `âŒ Failed to queue "${term}":`);
      }
    }

    logger.info(`\nğŸ“Š Summary: ${successCount} queued, ${failCount} failed`);
    logger.info('âœ¨ Investment batch enqueue completed!');
  } catch (error) {
    logger.error({ err: error }, 'âŒ Fatal error:');
    process.exit(1);
  }
}

enqueueInvestmentBatch()
  .then(() => process.exit(0))
  .catch((error) => {
    logger.error({ err: error }, 'âŒ Script failed:');
    process.exit(1);
  });
</file>

<file path="enqueue-llc-batch.ts">
#!/usr/bin/env node
/**
 * Enqueue LLC Property Searches
 * Queues LLC and limited company search terms
 */

import { scraperQueue } from '../queues/scraper.queue';
import logger from '../lib/logger';
import { config } from '../config';

const LLC_TERMS = [
  'LLC',
  'LLC.',
  'L.L.C.',
  'Limited Liability',
  'Limited',
  'LMTD',
  'Limit',
  'L L C',
  'LTD',
  'Co LLC',
];

async function enqueueLLCBatch() {
  logger.info('ğŸ­ Starting LLC Batch Enqueue');
  logger.info(`Auto-refresh token enabled: ${config.scraper.autoRefreshToken}`);

  try {
    let successCount = 0;
    let failCount = 0;

    for (const term of LLC_TERMS) {
      try {
        const job = await scraperQueue.add('scrape-properties', {
          searchTerm: term,
          userId: 'llc-batch-enqueue',
          scheduled: true,
        }, {
          attempts: 3,
          backoff: {
            type: 'exponential',
            delay: 2000,
          },
          priority: 2,
          removeOnComplete: 100,
          removeOnFail: 50,
        });

        successCount++;
        logger.info(`âœ… [${successCount}/${LLC_TERMS.length}] Queued: "${term}" (Job ID: ${job.id})`);
      } catch (error) {
        failCount++;
        logger.error({ err: error }, `âŒ Failed to queue "${term}":`);
      }
    }

    logger.info(`\nğŸ“Š Summary: ${successCount} queued, ${failCount} failed`);
    logger.info('âœ¨ LLC batch enqueue completed!');
  } catch (error) {
    logger.error({ err: error }, 'âŒ Fatal error:');
    process.exit(1);
  }
}

enqueueLLCBatch()
  .then(() => process.exit(0))
  .catch((error) => {
    logger.error({ err: error }, 'âŒ Script failed:');
    process.exit(1);
  });
</file>

<file path="enqueue-partnership-batch.ts">
#!/usr/bin/env node
/**
 * Enqueue Partnership Property Searches
 * Queues partnership and association search terms
 */

import { scraperQueue } from '../queues/scraper.queue';
import logger from '../lib/logger';
import { config } from '../config';

const PARTNERSHIP_TERMS = [
  'Partnership',
  'Partners',
  'Part',
  'LP',
  'LLP',
  'Association',
  'Associates',
  'Assoc',
  'Assoc.',
  'Joint Venture',
];

async function enqueuePartnershipBatch() {
  logger.info('ğŸ¤ Starting Partnership Batch Enqueue');
  logger.info(`Auto-refresh token enabled: ${config.scraper.autoRefreshToken}`);

  try {
    let successCount = 0;
    let failCount = 0;

    for (const term of PARTNERSHIP_TERMS) {
      try {
        const job = await scraperQueue.add('scrape-properties', {
          searchTerm: term,
          userId: 'partnership-batch-enqueue',
          scheduled: true,
        }, {
          attempts: 3,
          backoff: {
            type: 'exponential',
            delay: 2000,
          },
          priority: 3,
          removeOnComplete: 100,
          removeOnFail: 50,
        });

        successCount++;
        logger.info(`âœ… [${successCount}/${PARTNERSHIP_TERMS.length}] Queued: "${term}" (Job ID: ${job.id})`);
      } catch (error) {
        failCount++;
        logger.error({ err: error }, `âŒ Failed to queue "${term}":`);
      }
    }

    logger.info(`\nğŸ“Š Summary: ${successCount} queued, ${failCount} failed`);
    logger.info('âœ¨ Partnership batch enqueue completed!');
  } catch (error) {
    logger.error({ err: error }, 'âŒ Fatal error:');
    process.exit(1);
  }
}

enqueuePartnershipBatch()
  .then(() => process.exit(0))
  .catch((error) => {
    logger.error({ err: error }, 'âŒ Script failed:');
    process.exit(1);
  });
</file>

<file path="enqueue-priority-terms.ts">
import { scraperQueue } from '../queues/scraper.queue';

const PRIORITY_TERMS = ['Lake', 'River', 'Pecan', 'Maple', 'Oak', 'Mount', 'Limited'];

async function enqueuePriorityTerms() {
  console.log('Enqueueing priority search terms...');

  for (const term of PRIORITY_TERMS) {
    try {
      await scraperQueue.add(
        'scrape',
        { searchTerm: term },
        { priority: 1 } // Higher priority (lower number = higher priority in Bull)
      );
      console.log(`âœ“ Enqueued: ${term}`);
    } catch (error) {
      console.error(`âœ— Failed to enqueue ${term}:`, error);
    }
  }

  console.log(`\nâœ“ Successfully enqueued ${PRIORITY_TERMS.length} priority terms`);
  console.log('These jobs will be processed before other waiting jobs');
  process.exit(0);
}

enqueuePriorityTerms().catch((err) => {
  console.error('Error:', err);
  process.exit(1);
});
</file>

<file path="enqueue-property-type-batch.ts">
#!/usr/bin/env node
/**
 * Enqueue Property Type Searches
 * Queues property type and real estate search terms
 */

import { scraperQueue } from '../queues/scraper.queue';
import logger from '../lib/logger';
import { config } from '../config';

const PROPERTY_TYPE_TERMS = [
  'Properties',
  'Property',
  'Real Estate',
  'Realty',
  'Land',
  'Acres',
  'Development',
  'Developers',
  'Plaza',
  'Center',
];

async function enqueuePropertyTypeBatch() {
  logger.info('ğŸ˜ï¸  Starting Property Type Batch Enqueue');
  logger.info(`Auto-refresh token enabled: ${config.scraper.autoRefreshToken}`);

  try {
    let successCount = 0;
    let failCount = 0;

    for (const term of PROPERTY_TYPE_TERMS) {
      try {
        const job = await scraperQueue.add('scrape-properties', {
          searchTerm: term,
          userId: 'property-type-batch-enqueue',
          scheduled: true,
        }, {
          attempts: 3,
          backoff: {
            type: 'exponential',
            delay: 2000,
          },
          priority: 2,
          removeOnComplete: 100,
          removeOnFail: 50,
        });

        successCount++;
        logger.info(`âœ… [${successCount}/${PROPERTY_TYPE_TERMS.length}] Queued: "${term}" (Job ID: ${job.id})`);
      } catch (error) {
        failCount++;
        logger.error({ err: error }, `âŒ Failed to queue "${term}":`);
      }
    }

    logger.info(`\nğŸ“Š Summary: ${successCount} queued, ${failCount} failed`);
    logger.info('âœ¨ Property type batch enqueue completed!');
  } catch (error) {
    logger.error({ err: error }, 'âŒ Fatal error:');
    process.exit(1);
  }
}

enqueuePropertyTypeBatch()
  .then(() => process.exit(0))
  .catch((error) => {
    logger.error({ err: error }, 'âŒ Script failed:');
    process.exit(1);
  });
</file>

<file path="enqueue-residential-batch.ts">
#!/usr/bin/env node
/**
 * Enqueue Residential Property Searches
 * Queues common residential property search terms
 */

import { scraperQueue } from '../queues/scraper.queue';
import logger from '../lib/logger';
import { config } from '../config';

const RESIDENTIAL_TERMS = [
  'Smith',
  'Johnson',
  'Williams',
  'Brown',
  'Jones',
  'Miller',
  'Davis',
  'Garcia',
  'Rodriguez',
  'Wilson',
];

async function enqueueResidentialBatch() {
  logger.info('ğŸ  Starting Residential Batch Enqueue');
  logger.info(`Auto-refresh token enabled: ${config.scraper.autoRefreshToken}`);
  logger.info(`Token refresh interval: ${config.scraper.tokenRefreshInterval}ms`);

  try {
    let successCount = 0;
    let failCount = 0;

    for (const term of RESIDENTIAL_TERMS) {
      try {
        const job = await scraperQueue.add('scrape-properties', {
          searchTerm: term,
          userId: 'residential-batch-enqueue',
          scheduled: true,
        }, {
          attempts: 3,
          backoff: {
            type: 'exponential',
            delay: 2000,
          },
          removeOnComplete: 100,
          removeOnFail: 50,
        });

        successCount++;
        logger.info(`âœ… [${successCount}/${RESIDENTIAL_TERMS.length}] Queued: "${term}" (Job ID: ${job.id})`);
      } catch (error) {
        failCount++;
        logger.error({ err: error }, `âŒ Failed to queue "${term}":`);
      }
    }

    logger.info(`\nğŸ“Š Summary: ${successCount} queued, ${failCount} failed`);
    logger.info('âœ¨ Residential batch enqueue completed!');
  } catch (error) {
    logger.error({ err: error }, 'âŒ Fatal error:');
    process.exit(1);
  }
}

enqueueResidentialBatch()
  .then(() => process.exit(0))
  .catch((error) => {
    logger.error({ err: error }, 'âŒ Script failed:');
    process.exit(1);
  });
</file>

<file path="enqueue-test-batch-20.ts">
#!/usr/bin/env node
/**
 * Enqueue Test Batch (20 queries)
 * Tests the fixed token refresh with high-value search terms
 */

import { scraperQueue } from '../queues/scraper.queue';
import logger from '../lib/logger';
import { config } from '../config';

// 20 High-Value Search Terms (including repeats from previous batch)
const TEST_TERMS = [
  // Trust & Estate terms (repeat some from before)
  'Trust',
  'Family Trust',
  'Estate',
  'Living Trust',
  'Trustee',

  // Real estate related
  'Real Estate',
  'Real',
  'Family',
  'Property',
  'Land',

  // Investment terms
  'Investment',
  'Holdings',
  'Capital',
  'Partners',
  'Fund',

  // Entity types
  'LLC',
  'Limited',
  'Partnership',
  'Corporation',
  'Company',
];

async function enqueueTestBatch() {
  logger.info('ğŸ§ª Starting Test Batch Enqueue (20 queries with fixed token refresh)');
  logger.info(`Auto-refresh token enabled: ${config.scraper.autoRefreshToken}`);

  try {
    let successCount = 0;
    let failCount = 0;
    const startEnqueue = Date.now();
    const jobIds: number[] = [];

    for (let i = 0; i < TEST_TERMS.length; i++) {
      const term = TEST_TERMS[i];

      try {
        const job = await scraperQueue.add('scrape-properties', {
          searchTerm: term,
          userId: 'test-batch-20',
          scheduled: true,
        }, {
          attempts: 3,
          backoff: {
            type: 'exponential',
            delay: 2000,
          },
          priority: 1, // High priority
          removeOnComplete: 100,
          removeOnFail: 50,
        });

        successCount++;
        jobIds.push(Number(job.id));

        logger.info(
          `âœ… [${successCount}/${TEST_TERMS.length}] ` +
          `Queued: "${term}" (Job ID: ${job.id})`
        );

        // Small delay to avoid overwhelming the queue
        if (i < TEST_TERMS.length - 1) {
          await new Promise(resolve => setTimeout(resolve, 100));
        }
      } catch (error) {
        failCount++;
        logger.error({ err: error }, `âŒ Failed to queue "${term}"`);
      }
    }

    const enqueueDuration = Date.now() - startEnqueue;

    // Summary
    logger.info('\n' + '='.repeat(60));
    logger.info('ğŸ“Š TEST BATCH ENQUEUE SUMMARY');
    logger.info('='.repeat(60));
    logger.info(`âœ… Successfully queued: ${successCount}/${TEST_TERMS.length}`);
    logger.info(`âŒ Failed: ${failCount}`);
    logger.info(`â±ï¸  Enqueue duration: ${enqueueDuration}ms`);
    logger.info(`ğŸ“‹ Job IDs: ${jobIds.join(', ')}`);
    logger.info('\nâœ¨ Test batch enqueue completed!');
    logger.info('Monitor progress at: http://hobbes.taildb60fa.ts.net:3001/admin/queues');
    logger.info('='.repeat(60));

  } catch (error) {
    logger.error({ err: error }, 'âŒ Fatal error');
    process.exit(1);
  }
}

// Run the script
enqueueTestBatch()
  .then(() => process.exit(0))
  .catch((error) => {
    logger.error({ err: error }, 'âŒ Script failed');
    process.exit(1);
  });
</file>

<file path="enqueue-trust-batch.ts">
#!/usr/bin/env node
/**
 * Enqueue Trust & Estate Searches
 * Queues trust and estate-related search terms (high-yield searches)
 */

import { scraperQueue } from '../queues/scraper.queue';
import logger from '../lib/logger';
import { config } from '../config';

const TRUST_TERMS = [
  'Trust',
  'Trustee',
  'Estate',
  'Family Trust',
  'Revocable Trust',
  'Irrevocable Trust',
  'Living Trust',
  'Testamentary',
  'Fiduciary',
  'Beneficiary',
];

async function enqueueTrustBatch() {
  logger.info('ğŸ“œ Starting Trust & Estate Batch Enqueue');
  logger.info(`Auto-refresh token enabled: ${config.scraper.autoRefreshToken}`);
  logger.info(`Expected high yield: ~70+ properties per term`);

  try {
    let successCount = 0;
    let failCount = 0;

    for (const term of TRUST_TERMS) {
      try {
        const job = await scraperQueue.add('scrape-properties', {
          searchTerm: term,
          userId: 'trust-batch-enqueue',
          scheduled: true,
        }, {
          attempts: 3,
          backoff: {
            type: 'exponential',
            delay: 2000,
          },
          priority: 1, // Highest priority - best yield
          removeOnComplete: 100,
          removeOnFail: 50,
        });

        successCount++;
        logger.info(`âœ… [${successCount}/${TRUST_TERMS.length}] Queued: "${term}" (Job ID: ${job.id})`);
      } catch (error) {
        failCount++;
        logger.error({ err: error }, `âŒ Failed to queue "${term}":`);
      }
    }

    logger.info(`\nğŸ“Š Summary: ${successCount} queued, ${failCount} failed`);
    logger.info(`ğŸ“ˆ Estimated total properties: ${successCount * 70} (if all succeed)`);
    logger.info('âœ¨ Trust batch enqueue completed!');
  } catch (error) {
    logger.error({ err: error }, 'âŒ Fatal error:');
    process.exit(1);
  }
}

enqueueTrustBatch()
  .then(() => process.exit(0))
  .catch((error) => {
    logger.error({ err: error }, 'âŒ Script failed:');
    process.exit(1);
  });
</file>

<file path="enqueue-ultra-high-priority.ts">
import { scraperQueue } from '../queues/scraper.queue';

// Ultra-high-value terms based on complete database analysis
// These are PROVEN patterns that haven't been tried yet
const ULTRA_HIGH_PRIORITY = [
  // Street terms (expected 5,000-10,000 each based on Avenue/Court performance)
  'Street',     // Expected: 10,000+ (like Avenue)
  'Drive',      // Expected: 5,000+
  'Lane',       // Expected: 5,000+
  'Road',       // Expected: 5,000+

  // Female names (expected 1,500-3,000 each)
  'Amy',        // Top 50 US name - Expected: 2,000+
  'Cynthia',    // Top 50 US name - Expected: 1,500+

  // Geographic terms (expected 2,000-4,000 each based on River/Rock)
  'Brook',      // Expected: 3,000+
  'Meadow',     // Expected: 2,500+
  'Valley',     // Expected: 3,000+
  'Point',      // Expected: 2,000+
];

async function enqueueUltraHighPriority() {
  console.log('ğŸš€ Enqueueing ULTRA-high-priority terms...\n');
  console.log('Expected yield: 40,000-60,000 properties from these 10 terms!\n');

  for (const term of ULTRA_HIGH_PRIORITY) {
    try {
      await scraperQueue.add(
        'scrape-properties',
        { searchTerm: term },
        { priority: -100 }  // Ultra-high priority
      );
      console.log(`âœ“ Enqueued: ${term}`);
    } catch (error) {
      console.error(`âœ— Failed to enqueue ${term}:`, error);
    }
  }

  console.log(`\nâœ… Successfully enqueued ${ULTRA_HIGH_PRIORITY.length} ultra-high-priority terms`);
  process.exit(0);
}

enqueueUltraHighPriority().catch((err) => {
  console.error('Error:', err);
  process.exit(1);
});
</file>

<file path="migrate-to-logger.ts">
#!/usr/bin/env tsx
/**
 * Migration Helper: Replace console.log with Pino logger
 *
 * This script helps migrate console.log statements to the Pino logger
 * Usage: tsx migrate-to-logger.ts <file-path>
 */

import * as fs from 'fs';
import * as path from 'path';

function migrateFile(filePath: string): void {
  const content = fs.readFileSync(filePath, 'utf-8');
  const lines = content.split('\n');

  let hasImport = false;
  let importLineIndex = -1;
  let lastImportIndex = -1;

  // Check if logger is already imported
  for (let i = 0; i < lines.length; i++) {
    const line = lines[i];
    if (line.includes("import") && line.includes("logger") && line.includes("../lib/logger")) {
      hasImport = true;
      importLineIndex = i;
      break;
    }
    if (line.startsWith('import ') && !line.includes('type')) {
      lastImportIndex = i;
    }
  }

  // Replace console.log patterns with logger equivalents
  let modified = content;

  // Track different console methods and their logger equivalents
  const replacements: { pattern: RegExp; replacement: string }[] = [
    { pattern: /console\.error\(/g, replacement: 'logger.error(' },
    { pattern: /console\.warn\(/g, replacement: 'logger.warn(' },
    { pattern: /console\.info\(/g, replacement: 'logger.info(' },
    { pattern: /console\.debug\(/g, replacement: 'logger.debug(' },
    { pattern: /console\.log\(/g, replacement: 'logger.info(' },
  ];

  replacements.forEach(({ pattern, replacement }) => {
    modified = modified.replace(pattern, replacement);
  });

  // Add import if not present
  if (!hasImport && modified !== content) {
    const lines = modified.split('\n');
    const insertIndex = lastImportIndex >= 0 ? lastImportIndex + 1 : 0;

    // Calculate relative path
    const fileDir = path.dirname(filePath);
    const loggerPath = path.join(__dirname, '../lib/logger');
    const relativePath = path.relative(fileDir, loggerPath).replace(/\\/g, '/');
    const importPath = relativePath.startsWith('.') ? relativePath : `./${relativePath}`;

    lines.splice(insertIndex, 0, `import logger from '${importPath}';`);
    modified = lines.join('\n');
  }

  // Write back if changed
  if (modified !== content) {
    fs.writeFileSync(filePath, modified, 'utf-8');
    console.log(`âœ… Migrated: ${filePath}`);
  } else {
    console.log(`â­ï¸  No changes: ${filePath}`);
  }
}

// Get file path from command line
const filePath = process.argv[2];

if (!filePath) {
  console.error('Usage: tsx migrate-to-logger.ts <file-path>');
  process.exit(1);
}

if (!fs.existsSync(filePath)) {
  console.error(`File not found: ${filePath}`);
  process.exit(1);
}

try {
  migrateFile(filePath);
} catch (error) {
  console.error('Migration failed:', error);
  process.exit(1);
}
</file>

<file path="queue-entity-searches-fresh.ts">
import { scraperQueue } from '../queues/scraper.queue';
import logger from '../lib/logger';

/**
 * Queue 50 high-yield entity term searches with fresh TCAD token
 * First clears failed jobs, then queues new searches
 */

const ENTITY_TERMS = [
  // Trust/Estate terms (highest yield)
  'Trust',
  'Estate',
  'Family',
  'Revocable',
  'Irrevocable',

  // Business entities - LLC variations
  'LLC.',
  'LLC',
  'L.L.C',
  'L.L.C.',
  'Limited',
  'Limit',
  'LMTD',

  // Business entities - Corporation
  'Corp',
  'Corp.',
  'Corporation',
  'Inc.',
  'Inc',
  'Incorporated',

  // Partnership terms
  'Part',
  'Partnership',
  'Partners',
  'Assoc',
  'Association',
  'Associates',

  // Property/Real Estate terms
  'Real',
  'Realty',
  'Properties',
  'Property',
  'Park',
  'Parc',
  'Plaza',
  'Center',

  // Management/Investment terms
  'Manage',
  'Management',
  'Investments',
  'Holdings',
  'Group',
  'Ventures',

  // Other entity terms
  'Home',
  'Homes',
  'Company',
  'Foundation',
  'Fund',
  'Capital',
  'Development',
  'Builders',
  'Construction',
];

async function clearAndQueueSearches() {
  logger.info('ğŸ”„ Clearing Failed Jobs and Queuing Fresh Entity Searches\n');
  logger.info('=' .repeat(80) + '\n');

  try {
    // Clean up failed jobs
    logger.info('ğŸ§¹ Cleaning up failed jobs...');
    const failedJobs = await scraperQueue.getFailed(0, 100);
    logger.info(`Found ${failedJobs.length} failed jobs`);

    let removedCount = 0;
    for (const job of failedJobs) {
      try {
        await job.remove();
        removedCount++;
      } catch (error) {
        logger.error(`Failed to remove job ${job.id}:`, error instanceof Error ? error.message : 'Unknown error');
      }
    }
    logger.info(`âœ… Removed ${removedCount} failed jobs\n`);

    // Take first 50 entity terms
    const searchTerms = ENTITY_TERMS.slice(0, 50);

    logger.info(`Queuing ${searchTerms.length} high-yield entity term searches...\n`);

    const jobs = [];
    let queuedCount = 0;
    let failedCount = 0;

    for (const searchTerm of searchTerms) {
      try {
        const job = await scraperQueue.add('scrape-properties', {
          searchTerm,
          userId: 'entity-batch-scraper-fresh',
          scheduled: true,
        }, {
          attempts: 3,
          backoff: {
            type: 'exponential',
            delay: 2000,
          },
          removeOnComplete: 100,
          removeOnFail: 50,
        });

        jobs.push(job);
        queuedCount++;
        logger.info(`âœ… [${queuedCount}/${searchTerms.length}] Queued: "${searchTerm}" (Job ID: ${job.id})`);

      } catch (error) {
        failedCount++;
        logger.error(`âŒ Failed to queue "${searchTerm}":`, error instanceof Error ? error.message : 'Unknown error');
      }
    }

    logger.info('\n' + 'â”€'.repeat(80));
    logger.info('QUEUE SUMMARY');
    logger.info('â”€'.repeat(80) + '\n');
    logger.info(`âœ… Successfully queued: ${queuedCount} jobs`);
    logger.info(`âŒ Failed to queue: ${failedCount} jobs`);
    logger.info(`ğŸ“Š Total jobs added: ${queuedCount}`);

    if (queuedCount > 0) {
      logger.info('\n' + '='.repeat(80));
      logger.info('MONITORING');
      logger.info('='.repeat(80) + '\n');
      logger.info('ğŸ¯ Bull Board Dashboard: http://localhost:3001/admin/queues');
      logger.info('   Monitor job progress, view completed/failed jobs, and queue stats\n');

      logger.info('ğŸ“ˆ Expected Results:');
      logger.info(`   - Entity terms average: ~70 properties/search`);
      logger.info(`   - Estimated total properties: ${queuedCount * 70} (if all succeed)`);
      logger.info(`   - Processing time: ~${Math.ceil(queuedCount / 2 * 15 / 60)} hours (2 concurrent workers)\n`);

      logger.info('âš ï¸  Note: Token expires in 5 minutes!');
      logger.info('   Run refresh-tcad-token.sh every 4 minutes to keep scraping active\n');
    }

    logger.info('âœ¨ Entity term searches queued successfully!\n');

  } catch (error) {
    logger.error('âŒ Fatal error:', error);
    process.exit(1);
  }
}

// Run the script
clearAndQueueSearches()
  .then(() => {
    logger.info('âœ… Script completed. Jobs are now processing...');
    process.exit(0);
  })
  .catch((error) => {
    logger.error('âŒ Script failed:', error);
    process.exit(1);
  });
</file>

<file path="queue-entity-searches.ts">
import { scraperQueue } from '../queues/scraper.queue';
import logger from '../lib/logger';

/**
 * Queue 50 high-yield entity term searches based on optimal search strategy
 *
 * Priority: Entity terms perform best (~70+ properties/search)
 * These terms target trusts, LLCs, partnerships, and corporations
 */

const ENTITY_TERMS = [
  // Trust/Estate terms (highest yield)
  'Trust',
  'Estate',
  'Family',
  'Revocable',
  'Irrevocable',

  // Business entities - LLC variations
  'LLC.',
  'LLC',
  'L.L.C',
  'L.L.C.',
  'Limited',
  'Limit',
  'LMTD',

  // Business entities - Corporation
  'Corp',
  'Corp.',
  'Corporation',
  'Inc.',
  'Inc',
  'Incorporated',

  // Partnership terms
  'Part',
  'Partnership',
  'Partners',
  'Assoc',
  'Association',
  'Associates',

  // Property/Real Estate terms
  'Real',
  'Realty',
  'Properties',
  'Property',
  'Park',
  'Parc',
  'Plaza',
  'Center',

  // Management/Investment terms
  'Manage',
  'Management',
  'Investments',
  'Holdings',
  'Group',
  'Ventures',

  // Other entity terms
  'Home',
  'Homes',
  'Company',
  'Foundation',
  'Fund',
  'Capital',
  'Development',
  'Builders',
  'Construction',
];

async function queueEntitySearches() {
  logger.info('ğŸ”„ Queuing Entity Term Searches for TCAD Scraper\n');
  logger.info('=' .repeat(80) + '\n');

  try {
    // Take first 50 entity terms
    const searchTerms = ENTITY_TERMS.slice(0, 50);

    logger.info(`Queuing ${searchTerms.length} high-yield entity term searches...\n`);

    const jobs = [];
    let queuedCount = 0;
    let failedCount = 0;

    for (const searchTerm of searchTerms) {
      try {
        const job = await scraperQueue.add('scrape-properties', {
          searchTerm,
          userId: 'entity-batch-scraper',
          scheduled: true,
        }, {
          attempts: 3,
          backoff: {
            type: 'exponential',
            delay: 2000,
          },
          removeOnComplete: 100,
          removeOnFail: 50,
        });

        jobs.push(job);
        queuedCount++;
        logger.info(`âœ… [${queuedCount}/${searchTerms.length}] Queued: "${searchTerm}" (Job ID: ${job.id})`);

      } catch (error) {
        failedCount++;
        logger.error(`âŒ Failed to queue "${searchTerm}":`, error instanceof Error ? error.message : 'Unknown error');
      }
    }

    logger.info('\n' + 'â”€'.repeat(80));
    logger.info('QUEUE SUMMARY');
    logger.info('â”€'.repeat(80) + '\n');
    logger.info(`âœ… Successfully queued: ${queuedCount} jobs`);
    logger.info(`âŒ Failed to queue: ${failedCount} jobs`);
    logger.info(`ğŸ“Š Total jobs added: ${queuedCount}`);

    if (queuedCount > 0) {
      logger.info('\n' + '='.repeat(80));
      logger.info('MONITORING');
      logger.info('='.repeat(80) + '\n');
      logger.info('ğŸ¯ Bull Board Dashboard: http://localhost:3001/admin/queues');
      logger.info('   Monitor job progress, view completed/failed jobs, and queue stats\n');

      logger.info('ğŸ“ˆ Expected Results:');
      logger.info(`   - Entity terms average: ~70 properties/search`);
      logger.info(`   - Estimated total properties: ${queuedCount * 70} (if all succeed)`);
      logger.info(`   - Processing time: ~${Math.ceil(queuedCount / 2 * 15 / 60)} hours (2 concurrent workers)\n`);
    }

    logger.info('âœ¨ Entity term searches queued successfully!\n');

  } catch (error) {
    logger.error('âŒ Fatal error:', error);
    process.exit(1);
  }
}

// Run the script
queueEntitySearches()
  .then(() => {
    logger.info('âœ… Script completed. Jobs are now processing...');
    process.exit(0);
  })
  .catch((error) => {
    logger.error('âŒ Script failed:', error);
    process.exit(1);
  });
</file>

<file path="README_ENHANCED.md">
# scripts

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "SoftwareSourceCode",
  "name": "scripts",
  "description": "Directory containing 30 code files with 6 classes and 29 functions",
  "programmingLanguage": [
    {
      "@type": "ComputerLanguage",
      "name": "Typescript"
    }
  ],
  "featureList": [
    "6 class definitions",
    "29 function definitions"
  ]
}
</script>

## Overview

This directory contains 30 code file(s) with extracted schemas.

## Files and Schemas

### `batch-scrape-100.ts` (typescript)

**Functions:**
- `async queueBatch()` - Line 49

### `batch-scrape-comprehensive.ts` (typescript)

**Classes:**
- `ComprehensiveBatchScraper` - Line 86
- `ComprehensiveBatchConfig` - Line 75

### `batch-scrape.ts` (typescript)

**Classes:**
- `BatchScraper` - Line 92
- `BatchConfig` - Line 84

### `check-column-ids.ts` (typescript)

**Functions:**
- `async checkColumnIds()` - Line 3

### `check-grove-job.ts` (typescript)

**Functions:**
- `async checkGroveJob()` - Line 2

### `check-queue-status.ts` (typescript)

**Functions:**
- `async checkQueueStatus()` - Line 9

### `continuous-batch-scraper.ts` (typescript)

**Classes:**
- `SearchPatternGenerator` - Line 26
- `ContinuousBatchScraper` - Line 522

### `debug-token-refresh.ts` (typescript)

**Functions:**
- `async debugTokenRefresh()` - Line 9

### `enqueue-commercial-batch.ts` (typescript)

**Functions:**
- `async enqueueCommercialBatch()` - Line 23

### `enqueue-construction-batch.ts` (typescript)

**Functions:**
- `async enqueueConstructionBatch()` - Line 23

### `enqueue-corporation-batch.ts` (typescript)

**Functions:**
- `async enqueueCorporationBatch()` - Line 23

### `enqueue-foundation-batch.ts` (typescript)

**Functions:**
- `async enqueueFoundationBatch()` - Line 23

### `enqueue-grove.ts` (typescript)

**Functions:**
- `async enqueueGrove()` - Line 2

### `enqueue-high-priority.ts` (typescript)

**Functions:**
- `async enqueueHighPriority()` - Line 12

### `enqueue-high-value-batch.ts` (typescript)

**Functions:**
- `async enqueueHighValueBatch()` - Line 72

### `enqueue-investment-batch.ts` (typescript)

**Functions:**
- `async enqueueInvestmentBatch()` - Line 23

### `enqueue-llc-batch.ts` (typescript)

**Functions:**
- `async enqueueLLCBatch()` - Line 23

### `enqueue-partnership-batch.ts` (typescript)

**Functions:**
- `async enqueuePartnershipBatch()` - Line 23

### `enqueue-priority-terms.ts` (typescript)

**Functions:**
- `async enqueuePriorityTerms()` - Line 4

### `enqueue-property-type-batch.ts` (typescript)

**Functions:**
- `async enqueuePropertyTypeBatch()` - Line 23

### `enqueue-residential-batch.ts` (typescript)

**Functions:**
- `async enqueueResidentialBatch()` - Line 23

### `enqueue-test-batch-20.ts` (typescript)

**Functions:**
- `async enqueueTestBatch()` - Line 41

### `enqueue-trust-batch.ts` (typescript)

**Functions:**
- `async enqueueTrustBatch()` - Line 23

### `enqueue-ultra-high-priority.ts` (typescript)

**Functions:**
- `async enqueueUltraHighPriority()` - Line 22

### `queue-entity-searches-fresh.ts` (typescript)

**Functions:**
- `async clearAndQueueSearches()` - Line 71

### `queue-entity-searches.ts` (typescript)

**Functions:**
- `async queueEntitySearches()` - Line 73

### `test-api-token-config.ts` (typescript)

**Functions:**
- `async testTokenUsage()` - Line 47
- `simulateQueueJob()` - Line 85
- `async runAllTests()` - Line 110

### `test-queue-job-flow.ts` (typescript)

**Functions:**
- `async simulateQueueJobProcessing()` - Line 15

### `test-single-job.ts` (typescript)

**Functions:**
- `async testSingleJob()` - Line 9

### `test-token-refresh.ts` (typescript)

**Functions:**
- `async testTokenRefresh()` - Line 18

---
*Generated by Enhanced Schema Generator with schema.org markup*
</file>

<file path="test-api-token-config.ts">
#!/usr/bin/env ts-node

/**
 * Test script to verify TCAD_API_KEY configuration
 *
 * This script tests:
 * 1. Config loading for TCAD_API_KEY
 * 2. Scraper initialization with token
 * 3. Token usage in API calls (without actually making requests)
 */

import { config } from '../config';
import { TCADScraper } from '../lib/tcad-scraper';
import logger from '../lib/logger';

logger.info('=== TCAD API Token Configuration Test ===\n');

// Test 1: Check if TCAD_API_KEY is loaded in config
logger.info('Test 1: Config Loading');
logger.info('----------------------');
if (config.scraper.tcadApiKey) {
  logger.info('âœ… TCAD_API_KEY is configured');
  logger.info(`   Token preview: ${config.scraper.tcadApiKey.substring(0, 20)}...`);
} else {
  logger.info('âŒ TCAD_API_KEY is NOT configured');
  logger.info('   Scraper will fall back to browser-based token capture');
}
logger.info('');

// Test 2: Check full configuration
logger.info('Test 2: Full Scraper Configuration');
logger.info('-----------------------------------');
logger.info(`Headless: ${config.scraper.headless}`);
logger.info(`Timeout: ${config.scraper.timeout}ms`);
logger.info(`Retry Attempts: ${config.scraper.retryAttempts}`);
logger.info(`Retry Delay: ${config.scraper.retryDelay}ms`);
logger.info(`TCAD API Token: ${config.scraper.tcadApiKey ? 'âœ… Set' : 'âŒ Not Set'}`);
logger.info('');

// Test 3: Initialize scraper and check token usage
logger.info('Test 3: Scraper Initialization');
logger.info('-------------------------------');

const scraper = new TCADScraper();

// We can't directly access private fields, but we can test the flow
// by checking if the scraper would use the token
async function testTokenUsage() {
  try {
    logger.info('Initializing browser...');
    await scraper.initialize();
    logger.info('âœ… Browser initialized successfully');

    // Note: We won't actually run a scrape to avoid hitting the API
    // But we can verify the configuration is correct
    logger.info('');
    logger.info('Configuration Status:');
    logger.info('--------------------');

    if (config.scraper.tcadApiKey) {
      logger.info('âœ… When scraping runs, it will:');
      logger.info('   1. Use pre-fetched TCAD_API_KEY from environment');
      logger.info('   2. Skip browser-based token capture');
      logger.info('   3. Make direct API calls');
      logger.info('   4. Faster and more efficient');
    } else {
      logger.info('âš ï¸  When scraping runs, it will:');
      logger.info('   1. Load the TCAD search page');
      logger.info('   2. Perform a test search');
      logger.info('   3. Capture auth token from network requests');
      logger.info('   4. Then make API calls (slower)');
    }

  } catch (error) {
    logger.error('âŒ Error during test:', error);
  } finally {
    await scraper.cleanup();
    logger.info('\nâœ… Cleanup complete');
  }
}

// Test 4: Simulate what would happen in a queue job
logger.info('Test 4: Queue Job Simulation');
logger.info('-----------------------------');

function simulateQueueJob() {
  logger.info('When a scrape job is added to the queue:');
  logger.info('');
  logger.info('1. Queue worker creates new TCADScraper instance');
  logger.info('2. Calls scraper.initialize()');
  logger.info('3. Calls scraper.scrapePropertiesViaAPI(searchTerm)');
  logger.info('');
  logger.info('Inside scrapePropertiesViaAPI:');
  logger.info('   - Line 128: let authToken = appConfig.scraper.tcadApiKey || null;');

  if (config.scraper.tcadApiKey) {
    logger.info('   - âœ… authToken is set from config');
    logger.info('   - âœ… Logs: "Using pre-fetched TCAD_API_KEY from environment"');
    logger.info('   - âœ… Skips browser token capture (lines 133-166)');
    logger.info('   - âœ… Proceeds directly to API calls (line 170+)');
  } else {
    logger.info('   - âš ï¸  authToken is null');
    logger.info('   - âš ï¸  Logs: "No TCAD_API_KEY found, capturing token from browser..."');
    logger.info('   - âš ï¸  Loads page and captures token (lines 133-166)');
    logger.info('   - âš ï¸  Then proceeds to API calls (slower)');
  }
  logger.info('');
}

// Run all tests
async function runAllTests() {
  simulateQueueJob();

  logger.info('=== Running Browser Initialization Test ===\n');
  await testTokenUsage();

  logger.info('\n=== Test Complete ===\n');

  // Summary
  logger.info('Summary:');
  logger.info('--------');
  if (config.scraper.tcadApiKey) {
    logger.info('âœ… PASS: API token is configured');
    logger.info('âœ… PASS: Scraper will use fast API mode');
    logger.info('');
    logger.info('Next steps:');
    logger.info('  1. Run a test scrape: npm run test:scrape');
    logger.info('  2. Check logs for "Using pre-fetched TCAD_API_KEY from environment"');
  } else {
    logger.info('âš ï¸  WARNING: API token is NOT configured');
    logger.info('âš ï¸  WARNING: Scraper will use fallback browser mode');
    logger.info('');
    logger.info('To enable fast API mode:');
    logger.info('  1. Get token from https://travis.prodigycad.com (see docs/TCAD_API_TOKEN_SETUP.md)');
    logger.info('  2. Add to .env: TCAD_API_KEY=your_token_here');
    logger.info('  3. Restart server: pm2 restart ecosystem.config.js');
    logger.info('  4. Re-run this test: npm run test:token-config');
  }

  process.exit(0);
}

runAllTests().catch((error) => {
  logger.error('Test failed:', error);
  process.exit(1);
});
</file>

<file path="test-queue-job-flow.ts">
#!/usr/bin/env ts-node

/**
 * Comprehensive test that simulates the full queue job flow
 * Shows exactly what happens when a scrape job is created
 */

import { config } from '../config';
import { TCADScraper } from '../lib/tcad-scraper';
import logger from '../lib/logger';

logger.info('=== Queue Job Flow Simulation ===\n');
logger.info('This simulates what happens when you add a scrape job to the queue.\n');

// Simulate the queue worker processing a job
async function simulateQueueJobProcessing() {
  const searchTerm = 'TEST_SEARCH_TERM';
  const jobId = 'test-job-123';

  logger.info('Step 1: Queue worker receives job');
  logger.info('----------------------------------');
  logger.info(`Job ID: ${jobId}`);
  logger.info(`Search Term: ${searchTerm}`);
  logger.info(`Status: pending â†’ processing\n`);

  logger.info('Step 2: Create TCADScraper instance');
  logger.info('------------------------------------');
  logger.info('Code: const scraper = new TCADScraper({ headless: true });');

  const scraper = new TCADScraper({
    headless: config.env.isProduction ? true : config.scraper.headless,
  });

  logger.info('âœ… Scraper instance created\n');

  logger.info('Step 3: Initialize browser');
  logger.info('--------------------------');
  logger.info('Code: await scraper.initialize();');

  try {
    await scraper.initialize();
    logger.info('âœ… Browser initialized\n');

    logger.info('Step 4: Call scrapePropertiesViaAPI');
    logger.info('------------------------------------');
    logger.info(`Code: await scraper.scrapePropertiesViaAPI('${searchTerm}');`);
    logger.info('');

    logger.info('Inside scrapePropertiesViaAPI (src/lib/tcad-scraper.ts:106):');
    logger.info('  Line 128: let authToken = appConfig.scraper.tcadApiKey || null;');
    logger.info('');

    if (config.scraper.tcadApiKey) {
      logger.info('  âœ… authToken = appConfig.scraper.tcadApiKey');
      logger.info(`  âœ… Token value: ${config.scraper.tcadApiKey.substring(0, 20)}...`);
      logger.info('  âœ… Condition: if (authToken) â†’ TRUE');
      logger.info('');
      logger.info('  Line 131: logger.info("Using pre-fetched TCAD_API_KEY from environment");');
      logger.info('  âœ… Skips lines 133-166 (browser token capture)');
      logger.info('  âœ… Jumps to line 170 (API calls)');
      logger.info('');
      logger.info('  Flow:');
      logger.info('    1. Inject __tcad_search function into page (lines 170-291)');
      logger.info('    2. Call function with pre-fetched token (line 294)');
      logger.info('    3. Function makes API calls to prod-container.trueprodigyapi.com');
      logger.info('    4. Returns property data');
      logger.info('    5. Transform to PropertyData format (lines 299-309)');
      logger.info('');
      logger.info('  âš¡ Performance: FAST (no page load, direct API)');
    } else {
      logger.info('  âš ï¸  authToken = null');
      logger.info('  âš ï¸  Condition: if (authToken) â†’ FALSE');
      logger.info('');
      logger.info('  Line 133: logger.info("No TCAD_API_KEY found, capturing token from browser...");');
      logger.info('  âš ï¸  Executes lines 135-166 (browser token capture):');
      logger.info('');
      logger.info('    Lines 142-145: Navigate to https://travis.prodigycad.com/property-search');
      logger.info('    Lines 149-152: Wait for React app to load');
      logger.info('    Lines 155-159: Perform test search to trigger API request');
      logger.info('    Lines 136-140: Capture Authorization header from request');
      logger.info('');
      logger.info('  Then continues to line 170 (API calls) with captured token');
      logger.info('');
      logger.info('  ğŸŒ Performance: SLOW (full page load + test search + token capture)');
    }

    logger.info('');
    logger.info('Step 5: Save to database');
    logger.info('------------------------');
    logger.info('Code: await prisma.property.upsert(...)');
    logger.info('âœ… Properties saved to database');
    logger.info('');

    logger.info('Step 6: Update job status');
    logger.info('-------------------------');
    logger.info('Code: await prisma.scrapeJob.update({ status: "completed" })');
    logger.info('âœ… Job marked as completed\n');

  } catch (error) {
    logger.error('âŒ Error during simulation:', error);
  } finally {
    logger.info('Step 7: Cleanup');
    logger.info('---------------');
    logger.info('Code: await scraper.cleanup();');
    await scraper.cleanup();
    logger.info('âœ… Browser closed\n');
  }

  // Summary
  logger.info('=== Summary ===\n');

  if (config.scraper.tcadApiKey) {
    logger.info('âœ… Current Configuration: OPTIMAL');
    logger.info('');
    logger.info('Your scrape jobs will:');
    logger.info('  â€¢ Use pre-fetched API token');
    logger.info('  â€¢ Skip browser-based token capture');
    logger.info('  â€¢ Complete faster');
    logger.info('  â€¢ Use fewer resources');
    logger.info('');
    logger.info('Execution Path:');
    logger.info('  Line 128: Get token from config âœ…');
    logger.info('  Line 131: Log "Using pre-fetched..." âœ…');
    logger.info('  Lines 133-166: SKIPPED â­ï¸');
    logger.info('  Line 170+: Direct API calls âœ…');
    logger.info('');
    logger.info('Next Steps:');
    logger.info('  1. Replace test token with real token from https://travis.prodigycad.com');
    logger.info('  2. Restart server: pm2 restart ecosystem.config.js');
    logger.info('  3. Run actual scrape job and monitor logs');
  } else {
    logger.info('âš ï¸  Current Configuration: FALLBACK MODE');
    logger.info('');
    logger.info('Your scrape jobs will:');
    logger.info('  â€¢ Load full webpage');
    logger.info('  â€¢ Perform test search');
    logger.info('  â€¢ Capture token from browser');
    logger.info('  â€¢ Then make API calls');
    logger.info('  â€¢ Take longer to complete');
    logger.info('');
    logger.info('Execution Path:');
    logger.info('  Line 128: authToken = null âš ï¸');
    logger.info('  Line 133: Log "No TCAD_API_KEY found..." âš ï¸');
    logger.info('  Lines 133-166: EXECUTED (browser capture) ğŸŒ');
    logger.info('  Line 170+: API calls with captured token âœ…');
    logger.info('');
    logger.info('To Enable Fast Mode:');
    logger.info('  1. Get token from https://travis.prodigycad.com (see docs/TCAD_API_TOKEN_SETUP.md)');
    logger.info('  2. Add to .env: TCAD_API_KEY=your_token_here');
    logger.info('  3. Restart server: pm2 restart ecosystem.config.js');
    logger.info('  4. Re-run this test: npm run test:queue-flow');
  }
}

// Run simulation
simulateQueueJobProcessing().catch((error) => {
  logger.error('Simulation failed:', error);
  process.exit(1);
});
</file>

<file path="test-single-job.ts">
#!/usr/bin/env node
/**
 * Test Single Job
 * Enqueues a single test job and monitors its progress
 */

import { scraperQueue } from '../queues/scraper.queue';
import logger from '../lib/logger';

async function testSingleJob() {
  const searchTerm = 'Development';

  logger.info(`ğŸ“‹ Enqueueing test job for: "${searchTerm}"`);

  try {
    const job = await scraperQueue.add('scrape-properties', {
      searchTerm,
      userId: 'test-single-job',
      scheduled: false,
    }, {
      attempts: 3,
      backoff: {
        type: 'exponential',
        delay: 2000,
      },
      priority: 1,
    });

    logger.info(`âœ… Job enqueued: ID ${job.id}`);
    logger.info(`â³ Waiting for job to complete...`);

    // Wait for job to finish
    const result = await job.finished();

    logger.info(`âœ… Job completed successfully!`);
    logger.info(`   Properties found: ${result.count}`);
    logger.info(`   Duration: ${result.duration}ms`);

    if (result.properties && result.properties.length > 0) {
      logger.info(`   Sample property: ${JSON.stringify(result.properties[0], null, 2)}`);
    }

    process.exit(0);
  } catch (error) {
    logger.error({ err: error }, `âŒ Job failed:`);
    process.exit(1);
  }
}

testSingleJob();
</file>

<file path="test-token-refresh.ts">
#!/usr/bin/env ts-node

/**
 * Test script for TCAD Token Auto-Refresh Service
 *
 * This script tests:
 * 1. Manual token refresh
 * 2. Token retrieval
 * 3. Service statistics
 * 4. Auto-refresh scheduling
 */

import { tokenRefreshService } from '../services/token-refresh.service';
import { config } from '../config';
import logger from '../lib/logger';

logger.info('=== TCAD Token Auto-Refresh Service Test ===\n');

async function testTokenRefresh() {
  logger.info('Configuration:');
  logger.info('--------------');
  logger.info(`Auto-Refresh Enabled: ${config.scraper.autoRefreshToken}`);
  logger.info(`Refresh Interval: ${config.scraper.tokenRefreshInterval}ms (${config.scraper.tokenRefreshInterval / 60000} minutes)`);
  logger.info(`Cron Schedule: ${config.scraper.tokenRefreshCron || 'Not set (using interval)'}`);
  logger.info('');

  // Test 1: Check initial state
  logger.info('Test 1: Initial State');
  logger.info('---------------------');
  const initialStats = tokenRefreshService.getStats();
  logger.info(`Current Token: ${initialStats.currentToken || 'None'}`);
  logger.info(`Last Refresh: ${initialStats.lastRefreshTime || 'Never'}`);
  logger.info(`Refresh Count: ${initialStats.refreshCount}`);
  logger.info(`Failure Count: ${initialStats.failureCount}`);
  logger.info(`Is Running: ${initialStats.isRunning}`);
  logger.info('');

  // Test 2: Manual token refresh
  logger.info('Test 2: Manual Token Refresh');
  logger.info('-----------------------------');
  logger.info('â³ Refreshing token (this may take 5-10 seconds)...');
  logger.info('');

  const startTime = Date.now();
  const token = await tokenRefreshService.refreshToken();
  const duration = Date.now() - startTime;

  logger.info('');
  if (token) {
    logger.info(`âœ… Token refreshed successfully in ${duration}ms`);
    logger.info(`Token preview: ${token.substring(0, 50)}...`);
  } else {
    logger.info(`âŒ Token refresh failed`);
  }
  logger.info('');

  // Test 3: Check stats after refresh
  logger.info('Test 3: Statistics After Refresh');
  logger.info('---------------------------------');
  const statsAfterRefresh = tokenRefreshService.getStats();
  logger.info(`Current Token: ${statsAfterRefresh.currentToken || 'None'}`);
  logger.info(`Last Refresh: ${statsAfterRefresh.lastRefreshTime}`);
  logger.info(`Refresh Count: ${statsAfterRefresh.refreshCount}`);
  logger.info(`Failure Count: ${statsAfterRefresh.failureCount}`);
  logger.info('');

  // Test 4: Health check
  logger.info('Test 4: Health Check');
  logger.info('--------------------');
  const health = tokenRefreshService.getHealth();
  logger.info(`Healthy: ${health.healthy ? 'âœ…' : 'âŒ'}`);
  logger.info(`Has Token: ${health.hasToken ? 'âœ…' : 'âŒ'}`);
  logger.info(`Time Since Last Refresh: ${health.timeSinceLastRefresh ? `${health.timeSinceLastRefresh}ms` : 'N/A'}`);
  logger.info(`Failure Rate: ${health.failureRate}`);
  logger.info(`Auto-Refresh Running: ${health.isAutoRefreshRunning ? 'âœ…' : 'âŒ'}`);
  logger.info('');

  // Test 5: Demo auto-refresh (run for 30 seconds)
  if (config.scraper.autoRefreshToken) {
    logger.info('Test 5: Auto-Refresh Demo');
    logger.info('-------------------------');
    logger.info('â³ Starting auto-refresh service for 30 seconds...');
    logger.info('   (In production, this runs continuously)');
    logger.info('');

    // Start auto-refresh with a short interval for demo (30 seconds)
    tokenRefreshService.startAutoRefreshInterval(30000); // 30 seconds for demo

    logger.info('Service started. Waiting for first scheduled refresh...');
    logger.info('(Press Ctrl+C to stop early)');
    logger.info('');

    // Wait 35 seconds to see at least one refresh
    await new Promise(resolve => setTimeout(resolve, 35000));

    // Stop auto-refresh
    tokenRefreshService.stopAutoRefresh();
    logger.info('');
    logger.info('Auto-refresh stopped.');
    logger.info('');

    // Show final stats
    const finalStats = tokenRefreshService.getStats();
    logger.info('Final Statistics:');
    logger.info(`  Total Refreshes: ${finalStats.refreshCount}`);
    logger.info(`  Total Failures: ${finalStats.failureCount}`);
    logger.info(`  Last Refresh: ${finalStats.lastRefreshTime}`);
    logger.info('');
  } else {
    logger.info('Test 5: Auto-Refresh Demo');
    logger.info('-------------------------');
    logger.info('âš ï¸  Auto-refresh is disabled in configuration');
    logger.info('   Set TCAD_AUTO_REFRESH_TOKEN=true to enable');
    logger.info('');
  }

  // Cleanup
  logger.info('Cleaning up...');
  await tokenRefreshService.cleanup();
  logger.info('âœ… Cleanup complete');
  logger.info('');

  // Summary
  logger.info('=== Summary ===');
  logger.info('');

  const finalHealth = tokenRefreshService.getHealth();
  if (finalHealth.healthy) {
    logger.info('âœ… Token refresh service is working correctly');
    logger.info('');
    logger.info('Production Usage:');
    logger.info('  1. Service starts automatically with server');
    logger.info(`  2. Refreshes token every ${config.scraper.tokenRefreshInterval / 60000} minutes`);
    logger.info('  3. Scraper uses refreshed token automatically');
    logger.info('  4. Check health: GET /health/token');
    logger.info('');
    logger.info('Next Steps:');
    logger.info('  â€¢ Start server: npm run dev');
    logger.info('  â€¢ Monitor logs for "Token refreshed successfully"');
    logger.info('  â€¢ Check health endpoint: curl http://localhost:3001/health/token');
  } else {
    logger.info('âš ï¸  Token refresh encountered issues');
    logger.info('');
    logger.info('Troubleshooting:');
    logger.info('  â€¢ Check browser executable path');
    logger.info('  â€¢ Verify TCAD website is accessible');
    logger.info('  â€¢ Review error logs above');
  }
}

// Run the test
testTokenRefresh()
  .then(() => {
    logger.info('');
    logger.info('Test complete!');
    process.exit(0);
  })
  .catch((error) => {
    logger.error('');
    logger.error('Test failed:', error);
    process.exit(1);
  });
</file>

<file path="worker.ts">
import { scraperQueue } from '../queues/scraper.queue';
import winston from 'winston';

const logger = winston.createLogger({
  level: 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.simple()
  ),
  transports: [
    new winston.transports.Console(),
  ],
});

logger.info('ğŸš€ TCAD Scraper Worker started');
logger.info(`   Redis: ${process.env.REDIS_HOST || 'localhost'}:${process.env.REDIS_PORT || '6379'}`);
logger.info(`   Database: ${process.env.DATABASE_URL}`);
logger.info('\nğŸ‘‚ Listening for jobs...\n');

// Graceful shutdown
process.on('SIGTERM', async () => {
  logger.info('\nğŸ›‘ Shutting down worker...');
  await scraperQueue.close();
  process.exit(0);
});

process.on('SIGINT', async () => {
  logger.info('\nğŸ›‘ Shutting down worker...');
  await scraperQueue.close();
  process.exit(0);
});
</file>

</files>
